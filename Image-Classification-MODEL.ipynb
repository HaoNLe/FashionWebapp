{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic 7 steps for building models in general are listed so:\n",
    "\n",
    "1. Load Dataset\n",
    "2. Make Dataset Iterable\n",
    "3. Create Model Class\n",
    "4. Instantiate Model Class\n",
    "5. Instantiate Loss Class\n",
    "6. Instantiate Optimizer Class\n",
    "7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  import sys\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.7/site-packages/ipykernel_launcher.py:20: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "STEP 1: LOAD DATASET\n",
    "'''\n",
    "# test_df = pd.read_csv('fashionmnist/fashion-mnist_test.csv')\n",
    "# test_df_labels = test_df['label']\n",
    "# test_pixels_df = test_df.drop('label', axis=1)\n",
    "all_df = pd.read_table('list_eval_partition.txt', delim_whitespace=True)\n",
    "validation_df = all_df[all_df['evaluation_status'].str.contains('val')]\n",
    "# np.random.shuffle(validation_df)\n",
    "validation_df = validation_df.drop(['evaluation_status'], axis=1)\n",
    "\n",
    "train_df = all_df[all_df['evaluation_status'].str.contains('train')]\n",
    "# np.random.shuffle(train_df)\n",
    "train_df = train_df.drop(['evaluation_status'], axis=1)\n",
    "\n",
    "test_df = all_df[all_df['evaluation_status'].str.contains('test')]\n",
    "# np.random.shuffle(test_df)\n",
    "test_df = test_df.drop(['evaluation_status'], axis=1)\n",
    "\n",
    "labels_df = pd.read_table('list_category_img.txt', delim_whitespace=True)\n",
    "labels_df\n",
    "\n",
    "def func(image_name):\n",
    "    category_label = labels_df[labels_df['image_name'].str.match(image_name)].iloc[0]['category_label']\n",
    "    create_new = np.zeros(50)\n",
    "    create_new[category_label - 1] = 1\n",
    "    return create_new\n",
    "\n",
    "# train_df = pd.read_csv('fashionmnist/fashion-mnist_train.csv')\n",
    "# train_pixels_df = train_df.drop('label', axis=1)\n",
    "# train_df_labels = train_df['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  2,  5,  8,  9, 10, 11, 14, 15, 16, 17, 18, 31]),\n",
       " array([ 4,  8,  5,  2,  1,  5,  1,  1,  3,  2, 13,  4,  1]))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.argmax(labelvalidation, axis=1), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  4,  5,  6,  9, 10, 11, 14, 15, 16, 17, 18, 19]),\n",
       " array([ 1, 26, 97,  3, 22,  2,  7,  6,  1,  2, 15, 13, 46,  8,  1]))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.argmax(labeltrain, axis=1), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "allimagesvalidation = []\n",
    "labelvalidation = []\n",
    "for index in range(50):\n",
    "#     create random number\n",
    "    x = np.random.choice(20000)\n",
    "    img_filepath = validation_df.iloc[x]['image_name']\n",
    "    im = Image.open(img_filepath)\n",
    "    labelvalidation.append(func(img_filepath))\n",
    "    imarr = np.uint8(np.asarray(im.convert('RGB').resize((224,224))))\n",
    "    imarr = (imarr - imarr.mean())/imarr.std()\n",
    "    allimagesvalidation.append(imarr)\n",
    "\n",
    "allimagestrain = []\n",
    "labeltrain = []\n",
    "for index in range(250):\n",
    "    x = np.random.choice(50000)\n",
    "    img_filepath = train_df.iloc[x]['image_name']\n",
    "    im = Image.open(img_filepath)\n",
    "    labeltrain.append(func(img_filepath))\n",
    "    imarr = np.uint8(np.asarray(im.convert('RGB').resize((224,224))))\n",
    "    imarr = (imarr - imarr.mean())/imarr.std()\n",
    "    allimagestrain.append(imarr)\n",
    "\n",
    "# allimagestest = []\n",
    "# labeltest = []\n",
    "# for index in range(10000):\n",
    "#     img_filepath = test_df.iloc[index]['image_name']\n",
    "#     im = Image.open(img_filepath)\n",
    "#     labeltest.append(func(img_filepath))\n",
    "#     imarr = np.uint8(np.asarray(im.convert('RGB').resize((224,224))))\n",
    "#     #imarr = np.round((imarr - imarr.mean())/imarr.std())\n",
    "#     allimagestest.append(imarr)\n",
    "# im = Image.open('414m1dOolTL._SX342_.jpg')\n",
    "# imarr = np.uint8(np.asarray(im.convert('RGB').resize((224,224))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# STEP 1.5: defining and instantiating Dataset subclass \n",
    "# '''\n",
    "\n",
    "# '''\n",
    "# This is our custom Dataset class. Remember from 1st meeting that we need this to pipeline our data into training our model.\n",
    "\n",
    "# The pipeline is important!!! At larger scale, machine learning can get bottlenecked at disk reads (in image classification for example)\n",
    "# so understanding the various stages is important. We don't have to worry about that kind of stuff now since we're just creating small\n",
    "# project models as opposed to complex production models.\n",
    "\n",
    "# NOTE: this is not the only way to create a dataset. An alternative is to simply pass in a dataframe that contains both pixel and label data.\n",
    "# Then we can index the label and pixel data inside of __getitem__ as opposed to separating labels and pixel data before hand like I did.\n",
    "# '''\n",
    "# class FashionDataset(Dataset):\n",
    "#     def __init__(self, dataframe, labels):\n",
    "#         self.labels = torch.LongTensor(labels)\n",
    "#         self.df = dataframe\n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "#         # I'm using .loc to access the row of the dataframe by index\n",
    "#         a = self.df.loc[index]\n",
    "# #         a = (a - np.mean(a))/np.std(a)\n",
    "#         img = torch.Tensor(a)\n",
    "#         label = self.labels[index]\n",
    "#         return img, label\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "    \n",
    "# '''\n",
    "# This class is for providing image data as (1, 28, 28) tensor as opposed to a (784) tensor. You\n",
    "# use these for conv2d layers which are powerful for image recognition!\n",
    "\n",
    "# NOTE: Please note that I normalized the data VERY INCORRECTLY. Here I am normalizing the data across \n",
    "# each sample individually which is not good. I should be normalizing across the ENTIRE training data set.\n",
    "\n",
    "# Also, when I create the test dataset I should normalize it based on the TRAINING set's mean and standard deviation.\n",
    "# Since the model is trained on the training data, we want to make sure that we transform the test data the same way we\n",
    "# transform the training data. Otherwise it's like training a model to do one job and then testing it by on another job.\n",
    "# '''\n",
    "# class Fashion2DDataset(Dataset):\n",
    "#     def __init__(self, dataframe, labels):\n",
    "#         self.labels = torch.LongTensor(labels)\n",
    "#         self.df = dataframe\n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "#         # I'm using .loc to access the row of the dataframe by index\n",
    "#         a = self.df.loc[index]\n",
    "#         a = (a - np.mean(a))/np.std(a)\n",
    "#         a = np.split(a, 28)\n",
    "#         a = np.array([a])\n",
    "#         img = torch.Tensor(a)\n",
    "        \n",
    "#         label = self.labels[index]\n",
    "#         return img, label\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "class ClothingDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index]\n",
    "        \n",
    "        trans = transforms.ToTensor()\n",
    "        img = trans(img).float()\n",
    "        \n",
    "        label = self.labels[index]\n",
    "        label = torch.LongTensor(label)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "# train_dataset = Fashion2DDataset(train_pixels_df, train_df_labels.values)\n",
    "# test_dataset = Fashion2DDataset(test_pixels_df, test_df_labels.values)\n",
    "\n",
    "# batch_size = 100\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "#                                            batch_size=batch_size, \n",
    "#                                            shuffle=True)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "#                                           batch_size=batch_size, \n",
    "#  shuffle=False)\n",
    "\n",
    "batch_size = 25\n",
    "\n",
    "train_dataset = ClothingDataset(allimagestrain, labeltrain)\n",
    "validation_dataset = ClothingDataset(allimagesvalidation, labelvalidation) \n",
    "test_dataset = ClothingDataset(allimagestest, labeltest)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size,                                             \n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "# class FeedforwardNeuralNetModel(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super(FeedforwardNeuralNetModel, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.fc1(x)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.fc2(out)\n",
    "#         return out\n",
    "    \n",
    "class ConvolutionalNeuralNetModel(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(ConvolutionalNeuralNetModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=56, stride=2, padding=2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(10)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        self.fc = nn.Linear(19360, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "#input_dim = 224*224\n",
    "#hidden_dim = 5000\n",
    "output_dim = 50 \n",
    "\n",
    "# model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "model = ConvolutionalNeuralNetModel(output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "\"\"\"\n",
    "Most of the time I use SGD. Feel free to use another optimizer if you wish.\n",
    "What hyperparameters would you use/set here?\n",
    "\"\"\"\n",
    "learning_rate = .1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1. Loss: 3.6125376224517822. Accuracy: 14\n",
      "Iteration: 2. Loss: 3.6951448917388916. Accuracy: 10\n",
      "Iteration: 3. Loss: 3.6666314601898193. Accuracy: 20\n",
      "Iteration: 4. Loss: 3.4955334663391113. Accuracy: 18\n",
      "Iteration: 5. Loss: 3.431241035461426. Accuracy: 14\n",
      "Iteration: 6. Loss: 3.2960643768310547. Accuracy: 18\n",
      "Iteration: 7. Loss: 2.844761610031128. Accuracy: 16\n",
      "Iteration: 8. Loss: 2.3488619327545166. Accuracy: 12\n",
      "Iteration: 9. Loss: 2.1518523693084717. Accuracy: 22\n",
      "Iteration: 10. Loss: 4.039227485656738. Accuracy: 14\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "losses = []\n",
    "accuracies = []\n",
    "for epoch in range(1):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "#         images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        labels = torch.max(labels,1)[1]\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 1 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in validation_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                # images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                labels = torch.max(labels,1)[1]\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy =  100 * correct / total\n",
    "            \n",
    "            accuracies.append(accuracy)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
