{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic 7 steps for building models in general are listed so:\n",
    "\n",
    "1. Load Dataset\n",
    "2. Make Dataset Iterable\n",
    "3. Create Model Class\n",
    "4. Instantiate Model Class\n",
    "5. Instantiate Loss Class\n",
    "6. Instantiate Optimizer Class\n",
    "7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as dsets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "all_df = pd.read_table('list_eval_partition.txt', delim_whitespace=True)\n",
    "labels_df = pd.read_table('list_category_img.txt', delim_whitespace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(image_name):\n",
    "    category_label = labels_df[labels_df['image_name'].str.match(image_name)].iloc[0]['category_label']\n",
    "    create_new = np.zeros(50)\n",
    "    create_new[category_label - 1] = 1\n",
    "    return create_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41    72158\n",
       "18    36887\n",
       "3     24557\n",
       "32    19666\n",
       "17    15429\n",
       "33    14773\n",
       "6     13311\n",
       "16    13123\n",
       "11    10467\n",
       "19    10078\n",
       "2      7495\n",
       "48     7408\n",
       "26     7076\n",
       "42     6153\n",
       "30     5013\n",
       "29     4416\n",
       "10     4048\n",
       "34     3048\n",
       "44     2294\n",
       "39     2120\n",
       "24     1669\n",
       "35     1106\n",
       "15      791\n",
       "12      748\n",
       "9       716\n",
       "13      676\n",
       "27      594\n",
       "22      527\n",
       "23      486\n",
       "36      386\n",
       "5       330\n",
       "7       324\n",
       "4       309\n",
       "1       160\n",
       "47      150\n",
       "20      146\n",
       "43      126\n",
       "14       97\n",
       "21       77\n",
       "46       70\n",
       "37       54\n",
       "25       49\n",
       "28       45\n",
       "31       32\n",
       "8        17\n",
       "40       17\n",
       "Name: category_label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = labels_df[\"category_label\"].value_counts()\n",
    "# x.index.sort_values()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_attributes = [38,45,49,50]\n",
    "low_count_attr = [1,4,5,7,8,14,20,21,22,23,27,13,9,12,15,25,28,31,36,46,37,40,43,46,47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>category_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000001.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000002.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000003.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000004.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000005.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000006.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000007.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000008.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000009.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000010.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000011.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000012.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000013.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000014.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000015.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000016.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000017.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000018.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000019.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000020.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000021.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000022.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000023.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000024.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000025.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000026.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000027.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000028.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000029.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000030.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289192</th>\n",
       "      <td>img/Paisley_Maxi_Cami_Dress/img_00000024.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289193</th>\n",
       "      <td>img/Paisley_Maxi_Cami_Dress/img_00000025.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289194</th>\n",
       "      <td>img/Paisley_Maxi_Cami_Dress/img_00000026.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289195</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000028.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289196</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000029.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289197</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000030.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289198</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000031.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289199</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000032.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289200</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000033.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289201</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000034.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289202</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000035.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289203</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000036.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289204</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000037.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289205</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000038.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289206</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000039.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289207</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000040.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289208</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000041.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289209</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000042.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289210</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000043.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289211</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000044.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289212</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000045.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289213</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000046.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289214</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000047.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289215</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000048.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289216</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000049.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289217</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000050.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289218</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000051.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289219</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000052.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289220</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000053.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289221</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000054.jpg</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>282295 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image_name  category_label\n",
       "0         img/Sheer_Pleated-Front_Blouse/img_00000001.jpg               3\n",
       "1         img/Sheer_Pleated-Front_Blouse/img_00000002.jpg               3\n",
       "2         img/Sheer_Pleated-Front_Blouse/img_00000003.jpg               3\n",
       "3         img/Sheer_Pleated-Front_Blouse/img_00000004.jpg               3\n",
       "4         img/Sheer_Pleated-Front_Blouse/img_00000005.jpg               3\n",
       "5         img/Sheer_Pleated-Front_Blouse/img_00000006.jpg               3\n",
       "6         img/Sheer_Pleated-Front_Blouse/img_00000007.jpg               3\n",
       "7         img/Sheer_Pleated-Front_Blouse/img_00000008.jpg               3\n",
       "8         img/Sheer_Pleated-Front_Blouse/img_00000009.jpg               3\n",
       "9         img/Sheer_Pleated-Front_Blouse/img_00000010.jpg               3\n",
       "10        img/Sheer_Pleated-Front_Blouse/img_00000011.jpg               3\n",
       "11        img/Sheer_Pleated-Front_Blouse/img_00000012.jpg               3\n",
       "12        img/Sheer_Pleated-Front_Blouse/img_00000013.jpg               3\n",
       "13        img/Sheer_Pleated-Front_Blouse/img_00000014.jpg               3\n",
       "14        img/Sheer_Pleated-Front_Blouse/img_00000015.jpg               3\n",
       "15        img/Sheer_Pleated-Front_Blouse/img_00000016.jpg               3\n",
       "16        img/Sheer_Pleated-Front_Blouse/img_00000017.jpg               3\n",
       "17        img/Sheer_Pleated-Front_Blouse/img_00000018.jpg               3\n",
       "18        img/Sheer_Pleated-Front_Blouse/img_00000019.jpg               3\n",
       "19        img/Sheer_Pleated-Front_Blouse/img_00000020.jpg               3\n",
       "20        img/Sheer_Pleated-Front_Blouse/img_00000021.jpg               3\n",
       "21        img/Sheer_Pleated-Front_Blouse/img_00000022.jpg               3\n",
       "22        img/Sheer_Pleated-Front_Blouse/img_00000023.jpg               3\n",
       "23        img/Sheer_Pleated-Front_Blouse/img_00000024.jpg               3\n",
       "24        img/Sheer_Pleated-Front_Blouse/img_00000025.jpg               3\n",
       "25        img/Sheer_Pleated-Front_Blouse/img_00000026.jpg               3\n",
       "26        img/Sheer_Pleated-Front_Blouse/img_00000027.jpg               3\n",
       "27        img/Sheer_Pleated-Front_Blouse/img_00000028.jpg               3\n",
       "28        img/Sheer_Pleated-Front_Blouse/img_00000029.jpg               3\n",
       "29        img/Sheer_Pleated-Front_Blouse/img_00000030.jpg               3\n",
       "...                                                   ...             ...\n",
       "289192       img/Paisley_Maxi_Cami_Dress/img_00000024.jpg              41\n",
       "289193       img/Paisley_Maxi_Cami_Dress/img_00000025.jpg              41\n",
       "289194       img/Paisley_Maxi_Cami_Dress/img_00000026.jpg              41\n",
       "289195  img/Paisley_Print_Babydoll_Dress/img_00000028.jpg              41\n",
       "289196  img/Paisley_Print_Babydoll_Dress/img_00000029.jpg              41\n",
       "289197  img/Paisley_Print_Babydoll_Dress/img_00000030.jpg              41\n",
       "289198  img/Paisley_Print_Babydoll_Dress/img_00000031.jpg              41\n",
       "289199  img/Paisley_Print_Babydoll_Dress/img_00000032.jpg              41\n",
       "289200  img/Paisley_Print_Babydoll_Dress/img_00000033.jpg              41\n",
       "289201  img/Paisley_Print_Babydoll_Dress/img_00000034.jpg              41\n",
       "289202  img/Paisley_Print_Babydoll_Dress/img_00000035.jpg              41\n",
       "289203  img/Paisley_Print_Babydoll_Dress/img_00000036.jpg              41\n",
       "289204  img/Paisley_Print_Babydoll_Dress/img_00000037.jpg              41\n",
       "289205  img/Paisley_Print_Babydoll_Dress/img_00000038.jpg              41\n",
       "289206  img/Paisley_Print_Babydoll_Dress/img_00000039.jpg              41\n",
       "289207  img/Paisley_Print_Babydoll_Dress/img_00000040.jpg              41\n",
       "289208  img/Paisley_Print_Babydoll_Dress/img_00000041.jpg              41\n",
       "289209  img/Paisley_Print_Babydoll_Dress/img_00000042.jpg              41\n",
       "289210  img/Paisley_Print_Babydoll_Dress/img_00000043.jpg              41\n",
       "289211  img/Paisley_Print_Babydoll_Dress/img_00000044.jpg              41\n",
       "289212  img/Paisley_Print_Babydoll_Dress/img_00000045.jpg              41\n",
       "289213  img/Paisley_Print_Babydoll_Dress/img_00000046.jpg              41\n",
       "289214  img/Paisley_Print_Babydoll_Dress/img_00000047.jpg              41\n",
       "289215  img/Paisley_Print_Babydoll_Dress/img_00000048.jpg              41\n",
       "289216  img/Paisley_Print_Babydoll_Dress/img_00000049.jpg              41\n",
       "289217  img/Paisley_Print_Babydoll_Dress/img_00000050.jpg              41\n",
       "289218  img/Paisley_Print_Babydoll_Dress/img_00000051.jpg              41\n",
       "289219  img/Paisley_Print_Babydoll_Dress/img_00000052.jpg              41\n",
       "289220  img/Paisley_Print_Babydoll_Dress/img_00000053.jpg              41\n",
       "289221  img/Paisley_Print_Babydoll_Dress/img_00000054.jpg              41\n",
       "\n",
       "[282295 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for low in low_count_attr:\n",
    "    labels_df = labels_df[labels_df['category_label'] != low]\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>category_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000001.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000002.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000003.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000004.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000005.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000006.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000007.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000008.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000009.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000010.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000011.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000012.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000013.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000014.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000015.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000016.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000017.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000018.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000019.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000020.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000021.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000022.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000023.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000024.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000025.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000026.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000027.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000028.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000029.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>img/Single-Button_Blazer/img_00000030.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139460</th>\n",
       "      <td>img/Boxy_Nautical_Stripe_Top/img_00000022.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139461</th>\n",
       "      <td>img/Boxy_Nautical_Stripe_Top/img_00000023.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139462</th>\n",
       "      <td>img/Boxy_Nautical_Stripe_Top/img_00000024.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139463</th>\n",
       "      <td>img/Boxy_Nautical_Stripe_Top/img_00000025.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139464</th>\n",
       "      <td>img/Boxy_Nautical_Stripe_Top/img_00000026.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139465</th>\n",
       "      <td>img/Boxy_Nautical_Stripe_Top/img_00000027.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139685</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000001.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139686</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000002.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139687</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000003.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139688</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000004.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139689</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000005.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139690</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000006.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139691</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000007.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139692</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000008.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139693</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000009.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139694</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000010.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139695</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000011.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139696</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000012.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139697</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000013.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139698</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000014.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139699</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000015.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139700</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000016.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139701</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000017.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139702</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000018.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139703</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000019.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139704</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000020.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139705</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000021.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139706</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000022.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139707</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000023.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139708</th>\n",
       "      <td>img/Abstract_Geo_Print_Top/img_00000024.jpg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151846 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           image_name  category_label\n",
       "750         img/Single-Button_Blazer/img_00000001.jpg               2\n",
       "751         img/Single-Button_Blazer/img_00000002.jpg               2\n",
       "752         img/Single-Button_Blazer/img_00000003.jpg               2\n",
       "753         img/Single-Button_Blazer/img_00000004.jpg               2\n",
       "754         img/Single-Button_Blazer/img_00000005.jpg               2\n",
       "755         img/Single-Button_Blazer/img_00000006.jpg               2\n",
       "756         img/Single-Button_Blazer/img_00000007.jpg               2\n",
       "757         img/Single-Button_Blazer/img_00000008.jpg               2\n",
       "758         img/Single-Button_Blazer/img_00000009.jpg               2\n",
       "759         img/Single-Button_Blazer/img_00000010.jpg               2\n",
       "760         img/Single-Button_Blazer/img_00000011.jpg               2\n",
       "761         img/Single-Button_Blazer/img_00000012.jpg               2\n",
       "762         img/Single-Button_Blazer/img_00000013.jpg               2\n",
       "763         img/Single-Button_Blazer/img_00000014.jpg               2\n",
       "764         img/Single-Button_Blazer/img_00000015.jpg               2\n",
       "765         img/Single-Button_Blazer/img_00000016.jpg               2\n",
       "766         img/Single-Button_Blazer/img_00000017.jpg               2\n",
       "767         img/Single-Button_Blazer/img_00000018.jpg               2\n",
       "768         img/Single-Button_Blazer/img_00000019.jpg               2\n",
       "769         img/Single-Button_Blazer/img_00000020.jpg               2\n",
       "770         img/Single-Button_Blazer/img_00000021.jpg               2\n",
       "771         img/Single-Button_Blazer/img_00000022.jpg               2\n",
       "772         img/Single-Button_Blazer/img_00000023.jpg               2\n",
       "773         img/Single-Button_Blazer/img_00000024.jpg               2\n",
       "774         img/Single-Button_Blazer/img_00000025.jpg               2\n",
       "775         img/Single-Button_Blazer/img_00000026.jpg               2\n",
       "776         img/Single-Button_Blazer/img_00000027.jpg               2\n",
       "777         img/Single-Button_Blazer/img_00000028.jpg               2\n",
       "778         img/Single-Button_Blazer/img_00000029.jpg               2\n",
       "779         img/Single-Button_Blazer/img_00000030.jpg               2\n",
       "...                                               ...             ...\n",
       "139460  img/Boxy_Nautical_Stripe_Top/img_00000022.jpg              19\n",
       "139461  img/Boxy_Nautical_Stripe_Top/img_00000023.jpg              19\n",
       "139462  img/Boxy_Nautical_Stripe_Top/img_00000024.jpg              19\n",
       "139463  img/Boxy_Nautical_Stripe_Top/img_00000025.jpg              19\n",
       "139464  img/Boxy_Nautical_Stripe_Top/img_00000026.jpg              19\n",
       "139465  img/Boxy_Nautical_Stripe_Top/img_00000027.jpg              19\n",
       "139685    img/Abstract_Geo_Print_Top/img_00000001.jpg              19\n",
       "139686    img/Abstract_Geo_Print_Top/img_00000002.jpg              19\n",
       "139687    img/Abstract_Geo_Print_Top/img_00000003.jpg              19\n",
       "139688    img/Abstract_Geo_Print_Top/img_00000004.jpg              19\n",
       "139689    img/Abstract_Geo_Print_Top/img_00000005.jpg              19\n",
       "139690    img/Abstract_Geo_Print_Top/img_00000006.jpg              19\n",
       "139691    img/Abstract_Geo_Print_Top/img_00000007.jpg              19\n",
       "139692    img/Abstract_Geo_Print_Top/img_00000008.jpg              19\n",
       "139693    img/Abstract_Geo_Print_Top/img_00000009.jpg              19\n",
       "139694    img/Abstract_Geo_Print_Top/img_00000010.jpg              19\n",
       "139695    img/Abstract_Geo_Print_Top/img_00000011.jpg              19\n",
       "139696    img/Abstract_Geo_Print_Top/img_00000012.jpg              19\n",
       "139697    img/Abstract_Geo_Print_Top/img_00000013.jpg              19\n",
       "139698    img/Abstract_Geo_Print_Top/img_00000014.jpg              19\n",
       "139699    img/Abstract_Geo_Print_Top/img_00000015.jpg              19\n",
       "139700    img/Abstract_Geo_Print_Top/img_00000016.jpg              19\n",
       "139701    img/Abstract_Geo_Print_Top/img_00000017.jpg              19\n",
       "139702    img/Abstract_Geo_Print_Top/img_00000018.jpg              19\n",
       "139703    img/Abstract_Geo_Print_Top/img_00000019.jpg              19\n",
       "139704    img/Abstract_Geo_Print_Top/img_00000020.jpg              19\n",
       "139705    img/Abstract_Geo_Print_Top/img_00000021.jpg              19\n",
       "139706    img/Abstract_Geo_Print_Top/img_00000022.jpg              19\n",
       "139707    img/Abstract_Geo_Print_Top/img_00000023.jpg              19\n",
       "139708    img/Abstract_Geo_Print_Top/img_00000024.jpg              19\n",
       "\n",
       "[151846 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower = [41,18,3,32,17,33,6,16,11,19]\n",
    "\n",
    "for elem in lower:  \n",
    "    jon = labels_df[labels_df['category_label'] == elem]\n",
    "    drop_diff = len(labels_df[labels_df['category_label'] == elem]) - 10000\n",
    "    # random_vector = np.random.choice(drop_diff + 10000, drop_diff)\n",
    "    # labels_df = labels_df.drop(labels_df.[random_vector])\n",
    "    # labels_df\n",
    "    df_dropped = jon.drop(jon.sample(n=drop_diff).index)\n",
    "    labels_df = labels_df[labels_df['category_label'] != elem]\n",
    "    labels_df = labels_df.append(df_dropped)\n",
    "labels_df\n",
    "\n",
    "# duplicate = [35,24,39,44,34,10,29,30,42,26,48,2]\n",
    "\n",
    "# for elem in duplicate:\n",
    "#     snow = labels_df[labels_df['category_label'] == elem]\n",
    "#     sample_diff = 10000 - len(snow)\n",
    "#     df_dropped = snow.sample(n=10000, replace=True)\n",
    "#     labels_df = labels_df[labels_df['category_label'] != elem]\n",
    "#     labels_df = labels_df.append(df_dropped)\n",
    "\n",
    "# indices = np.arange(len(labels_df))\n",
    "# labels_df.set_index(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = all_df[all_df.isin(labels_df)['image_name'] == True]\n",
    "# print(len(all_df))\n",
    "# all_df = labels_df[['image_name']].merge(all_df)\n",
    "# all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000001.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000004.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000008.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000009.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000011.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000014.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000016.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000018.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000021.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000024.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000026.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000032.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000036.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000041.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000046.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000048.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000049.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000055.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000062.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000064.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000067.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000069.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000075.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000077.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000079.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000086.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000092.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000094.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000097.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>img/Sheer_Pleated-Front_Blouse/img_00000098.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288994</th>\n",
       "      <td>img/Painted_Floral_Slip_Dress/img_00000029.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288999</th>\n",
       "      <td>img/Painted_Floral_Slip_Dress/img_00000034.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289022</th>\n",
       "      <td>img/Paisley_Cami_Maxi_Dress/img_00000021.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289025</th>\n",
       "      <td>img/Paisley_Cami_Trapeze_Dress/img_00000003.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289028</th>\n",
       "      <td>img/Paisley_Cami_Trapeze_Dress/img_00000006.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289034</th>\n",
       "      <td>img/Paisley_Cami_Trapeze_Dress/img_00000012.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289050</th>\n",
       "      <td>img/Paisley_Fit_&amp;_Flare_Dress/img_00000001.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289059</th>\n",
       "      <td>img/Paisley_Fit_&amp;_Flare_Dress/img_00000010.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289060</th>\n",
       "      <td>img/Paisley_Fit_&amp;_Flare_Dress/img_00000011.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289064</th>\n",
       "      <td>img/Paisley_Fit_&amp;_Flare_Dress/img_00000015.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289091</th>\n",
       "      <td>img/Paisley_Floral_A-Line_Dress/img_00000005.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289097</th>\n",
       "      <td>img/Paisley_Floral_A-Line_Dress/img_00000011.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289108</th>\n",
       "      <td>img/Paisley_Floral_A-Line_Dress/img_00000022.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289109</th>\n",
       "      <td>img/Paisley_Floral_A-Line_Dress/img_00000023.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289113</th>\n",
       "      <td>img/Paisley_Floral_A-Line_Dress/img_00000027.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289118</th>\n",
       "      <td>img/Paisley_Floral_A-Line_Dress/img_00000032.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289135</th>\n",
       "      <td>img/Paisley_Floral_A-Line_Dress/img_00000049.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289137</th>\n",
       "      <td>img/Paisley_High-Slit_Maxi_Dress/img_00000002.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289142</th>\n",
       "      <td>img/Paisley_High-Slit_Maxi_Dress/img_00000007.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289143</th>\n",
       "      <td>img/Paisley_High-Slit_Maxi_Dress/img_00000008.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289148</th>\n",
       "      <td>img/Paisley_High-Slit_Maxi_Dress/img_00000013.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289151</th>\n",
       "      <td>img/Paisley_High-Slit_Maxi_Dress/img_00000016.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289177</th>\n",
       "      <td>img/Paisley_Maxi_Cami_Dress/img_00000009.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289179</th>\n",
       "      <td>img/Paisley_Maxi_Cami_Dress/img_00000011.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289187</th>\n",
       "      <td>img/Paisley_Maxi_Cami_Dress/img_00000019.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289195</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000028.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289198</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000031.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289205</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000038.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289218</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000051.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289220</th>\n",
       "      <td>img/Paisley_Print_Babydoll_Dress/img_00000053.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109906 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image_name\n",
       "0         img/Sheer_Pleated-Front_Blouse/img_00000001.jpg\n",
       "3         img/Sheer_Pleated-Front_Blouse/img_00000004.jpg\n",
       "7         img/Sheer_Pleated-Front_Blouse/img_00000008.jpg\n",
       "8         img/Sheer_Pleated-Front_Blouse/img_00000009.jpg\n",
       "10        img/Sheer_Pleated-Front_Blouse/img_00000011.jpg\n",
       "13        img/Sheer_Pleated-Front_Blouse/img_00000014.jpg\n",
       "15        img/Sheer_Pleated-Front_Blouse/img_00000016.jpg\n",
       "17        img/Sheer_Pleated-Front_Blouse/img_00000018.jpg\n",
       "20        img/Sheer_Pleated-Front_Blouse/img_00000021.jpg\n",
       "23        img/Sheer_Pleated-Front_Blouse/img_00000024.jpg\n",
       "25        img/Sheer_Pleated-Front_Blouse/img_00000026.jpg\n",
       "31        img/Sheer_Pleated-Front_Blouse/img_00000032.jpg\n",
       "35        img/Sheer_Pleated-Front_Blouse/img_00000036.jpg\n",
       "40        img/Sheer_Pleated-Front_Blouse/img_00000041.jpg\n",
       "45        img/Sheer_Pleated-Front_Blouse/img_00000046.jpg\n",
       "47        img/Sheer_Pleated-Front_Blouse/img_00000048.jpg\n",
       "48        img/Sheer_Pleated-Front_Blouse/img_00000049.jpg\n",
       "54        img/Sheer_Pleated-Front_Blouse/img_00000055.jpg\n",
       "61        img/Sheer_Pleated-Front_Blouse/img_00000062.jpg\n",
       "63        img/Sheer_Pleated-Front_Blouse/img_00000064.jpg\n",
       "66        img/Sheer_Pleated-Front_Blouse/img_00000067.jpg\n",
       "68        img/Sheer_Pleated-Front_Blouse/img_00000069.jpg\n",
       "74        img/Sheer_Pleated-Front_Blouse/img_00000075.jpg\n",
       "76        img/Sheer_Pleated-Front_Blouse/img_00000077.jpg\n",
       "78        img/Sheer_Pleated-Front_Blouse/img_00000079.jpg\n",
       "85        img/Sheer_Pleated-Front_Blouse/img_00000086.jpg\n",
       "91        img/Sheer_Pleated-Front_Blouse/img_00000092.jpg\n",
       "93        img/Sheer_Pleated-Front_Blouse/img_00000094.jpg\n",
       "96        img/Sheer_Pleated-Front_Blouse/img_00000097.jpg\n",
       "97        img/Sheer_Pleated-Front_Blouse/img_00000098.jpg\n",
       "...                                                   ...\n",
       "288994     img/Painted_Floral_Slip_Dress/img_00000029.jpg\n",
       "288999     img/Painted_Floral_Slip_Dress/img_00000034.jpg\n",
       "289022       img/Paisley_Cami_Maxi_Dress/img_00000021.jpg\n",
       "289025    img/Paisley_Cami_Trapeze_Dress/img_00000003.jpg\n",
       "289028    img/Paisley_Cami_Trapeze_Dress/img_00000006.jpg\n",
       "289034    img/Paisley_Cami_Trapeze_Dress/img_00000012.jpg\n",
       "289050     img/Paisley_Fit_&_Flare_Dress/img_00000001.jpg\n",
       "289059     img/Paisley_Fit_&_Flare_Dress/img_00000010.jpg\n",
       "289060     img/Paisley_Fit_&_Flare_Dress/img_00000011.jpg\n",
       "289064     img/Paisley_Fit_&_Flare_Dress/img_00000015.jpg\n",
       "289091   img/Paisley_Floral_A-Line_Dress/img_00000005.jpg\n",
       "289097   img/Paisley_Floral_A-Line_Dress/img_00000011.jpg\n",
       "289108   img/Paisley_Floral_A-Line_Dress/img_00000022.jpg\n",
       "289109   img/Paisley_Floral_A-Line_Dress/img_00000023.jpg\n",
       "289113   img/Paisley_Floral_A-Line_Dress/img_00000027.jpg\n",
       "289118   img/Paisley_Floral_A-Line_Dress/img_00000032.jpg\n",
       "289135   img/Paisley_Floral_A-Line_Dress/img_00000049.jpg\n",
       "289137  img/Paisley_High-Slit_Maxi_Dress/img_00000002.jpg\n",
       "289142  img/Paisley_High-Slit_Maxi_Dress/img_00000007.jpg\n",
       "289143  img/Paisley_High-Slit_Maxi_Dress/img_00000008.jpg\n",
       "289148  img/Paisley_High-Slit_Maxi_Dress/img_00000013.jpg\n",
       "289151  img/Paisley_High-Slit_Maxi_Dress/img_00000016.jpg\n",
       "289177       img/Paisley_Maxi_Cami_Dress/img_00000009.jpg\n",
       "289179       img/Paisley_Maxi_Cami_Dress/img_00000011.jpg\n",
       "289187       img/Paisley_Maxi_Cami_Dress/img_00000019.jpg\n",
       "289195  img/Paisley_Print_Babydoll_Dress/img_00000028.jpg\n",
       "289198  img/Paisley_Print_Babydoll_Dress/img_00000031.jpg\n",
       "289205  img/Paisley_Print_Babydoll_Dress/img_00000038.jpg\n",
       "289218  img/Paisley_Print_Babydoll_Dress/img_00000051.jpg\n",
       "289220  img/Paisley_Print_Babydoll_Dress/img_00000053.jpg\n",
       "\n",
       "[109906 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "STEP 1: LOAD DATASET\n",
    "'''\n",
    "# test_df = pd.read_csv('fashionmnist/fashion-mnist_test.csv')\n",
    "# test_df_labels = test_df['label']\n",
    "# test_pixels_df = test_df.drop('label', axis=1)\n",
    "validation_df = all_df[all_df['evaluation_status'].str.contains('val')]\n",
    "# np.random.shuffle(validation_df)\n",
    "validation_df = validation_df.drop(['evaluation_status'], axis=1)\n",
    "\n",
    "train_df = all_df[all_df['evaluation_status'].str.contains('train')]\n",
    "# np.random.shuffle(train_df)\n",
    "train_df = train_df.drop(['evaluation_status'], axis=1)\n",
    "\n",
    "test_df = all_df[all_df['evaluation_status'].str.contains('test')]\n",
    "# np.random.shuffle(test_df)\n",
    "test_df = test_df.drop(['evaluation_status'], axis=1)\n",
    "\n",
    "# train_df = pd.read_csv('fashionmnist/fashion-mnist_train.csv')\n",
    "# train_pixels_df = train_df.drop('label', axis=1)\n",
    "# train_df_labels = train_df['label']\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #np.unique(np.argmax(labelvalidation, axis=1), return_counts=True)\n",
    "\n",
    "# validation_df.iloc[1334]['image_name']\n",
    "# func('img/Open-Back_Knit_Blouse/img_00000020.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(np.argmax(labeltrain, axis=1), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "allimagesvalidation = []\n",
    "labelvalidation = []\n",
    "# fiftyimagesArray = np.zeros(50)\n",
    "for index in range(50):\n",
    "#     create random number\n",
    "    x = np.random.choice(20969)\n",
    "    img_filepath = validation_df.iloc[x]['image_name']\n",
    "    im = Image.open(img_filepath)\n",
    "#     print(x)\n",
    "    labelvalidation.append(func(img_filepath))\n",
    "    imarr = np.uint8(np.asarray(im.convert('RGB').resize((224,224))))\n",
    "    imarr = (imarr - imarr.mean())/imarr.std()\n",
    "    allimagesvalidation.append(imarr)\n",
    "\n",
    "allimagestrain = []\n",
    "labeltrain = []\n",
    "for index in range(500):\n",
    "    x = np.random.choice(109826)\n",
    "    img_filepath = train_df.iloc[x]['image_name']\n",
    "    im = Image.open(img_filepath)\n",
    "    labeltrain.append(func(img_filepath))\n",
    "    imarr = np.uint8(np.asarray(im.convert('RGB').resize((224,224))))\n",
    "    imarr = (imarr - imarr.mean())/imarr.std()\n",
    "    allimagestrain.append(imarr)\n",
    "\n",
    "# allimagestest = []\n",
    "# labeltest = []\n",
    "# for index in range(10000):\n",
    "#     img_filepath = test_df.iloc[index]['image_name']\n",
    "#     im = Image.open(img_filepath)\n",
    "#     labeltest.append(func(img_filepath))\n",
    "#     imarr = np.uint8(np.asarray(im.convert('RGB').resize((224,224))))\n",
    "#     #imarr = np.round((imarr - imarr.mean())/imarr.std())\n",
    "#     allimagestest.append(imarr)\n",
    "# im = Image.open('414m1dOolTL._SX342_.jpg')\n",
    "# imarr = np.uint8(np.asarray(im.convert('RGB').resize((224,224))))\n",
    "\n",
    "# fiftyimagesArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# STEP 1.5: defining and instantiating Dataset subclass \n",
    "# '''\n",
    "\n",
    "# '''\n",
    "# This is our custom Dataset class. Remember from 1st meeting that we need this to pipeline our data into training our model.\n",
    "\n",
    "# The pipeline is important!!! At larger scale, machine learning can get bottlenecked at disk reads (in image classification for example)\n",
    "# so understanding the various stages is important. We don't have to worry about that kind of stuff now since we're just creating small\n",
    "# project models as opposed to complex production models.\n",
    "\n",
    "# NOTE: this is not the only way to create a dataset. An alternative is to simply pass in a dataframe that contains both pixel and label data.\n",
    "# Then we can index the label and pixel data inside of __getitem__ as opposed to separating labels and pixel data before hand like I did.\n",
    "# '''\n",
    "# class FashionDataset(Dataset):\n",
    "#     def __init__(self, dataframe, labels):\n",
    "#         self.labels = torch.LongTensor(labels)\n",
    "#         self.df = dataframe\n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "#         # I'm using .loc to access the row of the dataframe by index\n",
    "#         a = self.df.loc[index]\n",
    "# #         a = (a - np.mean(a))/np.std(a)\n",
    "#         img = torch.Tensor(a)\n",
    "#         label = self.labels[index]\n",
    "#         return img, label\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "    \n",
    "# '''\n",
    "# This class is for providing image data as (1, 28, 28) tensor as opposed to a (784) tensor. You\n",
    "# use these for conv2d layers which are powerful for image recognition!\n",
    "\n",
    "# NOTE: Please note that I normalized the data VERY INCORRECTLY. Here I am normalizing the data across \n",
    "# each sample individually which is not good. I should be normalizing across the ENTIRE training data set.\n",
    "\n",
    "# Also, when I create the test dataset I should normalize it based on the TRAINING set's mean and standard deviation.\n",
    "# Since the model is trained on the training data, we want to make sure that we transform the test data the same way we\n",
    "# transform the training data. Otherwise it's like training a model to do one job and then testing it by on another job.\n",
    "# '''\n",
    "# class Fashion2DDataset(Dataset):\n",
    "#     def __init__(self, dataframe, labels):\n",
    "#         self.labels = torch.LongTensor(labels)\n",
    "#         self.df = dataframe\n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "#         # I'm using .loc to access the row of the dataframe by index\n",
    "#         a = self.df.loc[index]\n",
    "#         a = (a - np.mean(a))/np.std(a)\n",
    "#         a = np.split(a, 28)\n",
    "#         a = np.array([a])\n",
    "#         img = torch.Tensor(a)\n",
    "        \n",
    "#         label = self.labels[index]\n",
    "#         return img, label\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "class ClothingDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index]\n",
    "        \n",
    "        trans = transforms.ToTensor()\n",
    "        img = trans(img).float()\n",
    "        \n",
    "        label = self.labels[index]\n",
    "        label = torch.LongTensor(label)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "# train_dataset = Fashion2DDataset(train_pixels_df, train_df_labels.values)\n",
    "# test_dataset = Fashion2DDataset(test_pixels_df, test_df_labels.values)\n",
    "\n",
    "# batch_size = 100\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "#                                            batch_size=batch_size, \n",
    "#                                            shuffle=True)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "#                                           batch_size=batch_size, \n",
    "#  shuffle=False)\n",
    "\n",
    "batch_size = 25\n",
    "\n",
    "train_dataset = ClothingDataset(allimagestrain, labeltrain)\n",
    "validation_dataset = ClothingDataset(allimagesvalidation, labelvalidation) \n",
    "# test_dataset = ClothingDataset(allimagestest, labeltest)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size,                                             \n",
    "                                           shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "#                                           batch_size=batch_size,\n",
    "#                                           shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "# class FeedforwardNeuralNetModel(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super(FeedforwardNeuralNetModel, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.fc1(x)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.fc2(out)\n",
    "#         return out\n",
    "    \n",
    "class ConvolutionalNeuralNetModel(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(ConvolutionalNeuralNetModel, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 10, kernel_size=56, stride=2, padding=2, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(10)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "#         self.fc = nn.Linear(19360, output_dim)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 25, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(25, 3, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(25, 3, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(9408, 9408)\n",
    "        self.fc2 = nn.Linear(9408, 9408)\n",
    "        self.fc3 = nn.Linear(out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "#input_dim = 224*224\n",
    "#hidden_dim = 5000\n",
    "output_dim = 50 \n",
    "\n",
    "# model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "#model = ConvolutionalNeuralNetModel(output_dim)\n",
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "\"\"\"\n",
    "Most of the time I use SGD. Feel free to use another optimizer if you wish.\n",
    "What hyperparameters would you use/set here?\n",
    "\"\"\"\n",
    "learning_rate = .1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1. Loss: 0.0012039184803143144. Accuracy: 18\n",
      "Iteration: 2. Loss: 0.00037971496931277215. Accuracy: 22\n",
      "Iteration: 3. Loss: 0.0004096984921488911. Accuracy: 20\n",
      "Iteration: 4. Loss: 0.0002816772321239114. Accuracy: 24\n",
      "Iteration: 5. Loss: 0.00034378052805550396. Accuracy: 20\n",
      "Iteration: 6. Loss: 0.0005784606910310686. Accuracy: 22\n",
      "Iteration: 7. Loss: 0.0006861114525236189. Accuracy: 20\n",
      "Iteration: 8. Loss: 0.0012207793770357966. Accuracy: 22\n",
      "Iteration: 9. Loss: 0.00040725708822719753. Accuracy: 22\n",
      "Iteration: 10. Loss: 0.0011867523426190019. Accuracy: 24\n",
      "Iteration: 11. Loss: 0.007437057327479124. Accuracy: 22\n",
      "Iteration: 12. Loss: 0.008965606801211834. Accuracy: 26\n",
      "Iteration: 13. Loss: 0.0004527282726485282. Accuracy: 20\n",
      "Iteration: 14. Loss: 0.0006258392240852118. Accuracy: 18\n",
      "Iteration: 15. Loss: 0.0018978881416842341. Accuracy: 22\n",
      "Iteration: 16. Loss: 0.0002571868826635182. Accuracy: 24\n",
      "Iteration: 17. Loss: 0.0005566406180150807. Accuracy: 14\n",
      "Iteration: 18. Loss: 0.000943679828196764. Accuracy: 20\n",
      "Iteration: 19. Loss: 0.0006233978201635182. Accuracy: 18\n",
      "Iteration: 20. Loss: 0.0005635833949781954. Accuracy: 18\n",
      "Iteration: 21. Loss: 0.0006510925013571978. Accuracy: 16\n",
      "Iteration: 22. Loss: 0.0007595825009047985. Accuracy: 20\n",
      "Iteration: 23. Loss: 0.0004223632859066129. Accuracy: 22\n",
      "Iteration: 24. Loss: 0.0011158752022311091. Accuracy: 24\n",
      "Iteration: 25. Loss: 0.00047897337935864925. Accuracy: 18\n",
      "Iteration: 26. Loss: 0.00042633057455532253. Accuracy: 18\n",
      "Iteration: 27. Loss: 0.0003623199590947479. Accuracy: 22\n",
      "Iteration: 28. Loss: 0.0004049682756885886. Accuracy: 18\n",
      "Iteration: 29. Loss: 0.00028015137650072575. Accuracy: 16\n",
      "Iteration: 30. Loss: 0.0001987457217182964. Accuracy: 18\n",
      "Iteration: 31. Loss: 0.0004094695905223489. Accuracy: 24\n",
      "Iteration: 32. Loss: 0.0009071350214071572. Accuracy: 20\n",
      "Iteration: 33. Loss: 0.0006076049758121371. Accuracy: 18\n",
      "Iteration: 34. Loss: 0.0009527587681077421. Accuracy: 20\n",
      "Iteration: 35. Loss: 0.0005067443707957864. Accuracy: 26\n",
      "Iteration: 36. Loss: 0.009511108510196209. Accuracy: 20\n",
      "Iteration: 37. Loss: 0.0005840301746502519. Accuracy: 20\n",
      "Iteration: 38. Loss: 0.0004936981131322682. Accuracy: 22\n",
      "Iteration: 39. Loss: 0.0003827667096629739. Accuracy: 18\n",
      "Iteration: 40. Loss: 0.0009157562162727118. Accuracy: 18\n",
      "Iteration: 41. Loss: 0.000799636822193861. Accuracy: 12\n",
      "Iteration: 42. Loss: 0.0003249359142500907. Accuracy: 20\n",
      "Iteration: 43. Loss: 0.0005036163493059576. Accuracy: 20\n",
      "Iteration: 44. Loss: 0.000990371685475111. Accuracy: 18\n",
      "Iteration: 45. Loss: 0.0007734679966233671. Accuracy: 18\n",
      "Iteration: 46. Loss: 0.0008266448858194053. Accuracy: 22\n",
      "Iteration: 47. Loss: 0.0003795623779296875. Accuracy: 18\n",
      "Iteration: 48. Loss: 0.0006677245837636292. Accuracy: 20\n",
      "Iteration: 49. Loss: 0.0003359985421411693. Accuracy: 20\n",
      "Iteration: 50. Loss: 0.0035731506068259478. Accuracy: 22\n",
      "Iteration: 51. Loss: 0.0040206145495176315. Accuracy: 18\n",
      "Iteration: 52. Loss: 0.0013445281656458974. Accuracy: 18\n",
      "Iteration: 53. Loss: 0.003411254845559597. Accuracy: 18\n",
      "Iteration: 54. Loss: 0.00046676635975018144. Accuracy: 18\n",
      "Iteration: 55. Loss: 0.0035561369732022285. Accuracy: 20\n",
      "Iteration: 56. Loss: 0.0007198333623819053. Accuracy: 22\n",
      "Iteration: 57. Loss: 0.0006665802211500704. Accuracy: 20\n",
      "Iteration: 58. Loss: 0.0006024932954460382. Accuracy: 20\n",
      "Iteration: 59. Loss: 0.001996154896914959. Accuracy: 18\n",
      "Iteration: 60. Loss: 0.0003749847528524697. Accuracy: 18\n",
      "Iteration: 61. Loss: 0.0007022857898846269. Accuracy: 18\n",
      "Iteration: 62. Loss: 0.0006420898716896772. Accuracy: 22\n",
      "Iteration: 63. Loss: 0.00033706665271893144. Accuracy: 18\n",
      "Iteration: 64. Loss: 0.0011486053699627519. Accuracy: 22\n",
      "Iteration: 65. Loss: 0.00031295776716433465. Accuracy: 22\n",
      "Iteration: 66. Loss: 0.0015877533005550504. Accuracy: 22\n",
      "Iteration: 67. Loss: 0.0006474304245784879. Accuracy: 14\n",
      "Iteration: 68. Loss: 0.00041290282388217747. Accuracy: 20\n",
      "Iteration: 69. Loss: 0.0002834320184774697. Accuracy: 20\n",
      "Iteration: 70. Loss: 0.0007196808001026511. Accuracy: 22\n",
      "Iteration: 71. Loss: 0.0012088012881577015. Accuracy: 20\n",
      "Iteration: 72. Loss: 0.0009590148692950606. Accuracy: 20\n",
      "Iteration: 73. Loss: 0.0003620910574682057. Accuracy: 20\n",
      "Iteration: 74. Loss: 0.00018196106248069555. Accuracy: 16\n",
      "Iteration: 75. Loss: 0.0004904174711555243. Accuracy: 20\n",
      "Iteration: 76. Loss: 0.0008117675897665322. Accuracy: 24\n",
      "Iteration: 77. Loss: 0.0005311584682203829. Accuracy: 22\n",
      "Iteration: 78. Loss: 0.001532974187284708. Accuracy: 16\n",
      "Iteration: 79. Loss: 0.0006732177571393549. Accuracy: 22\n",
      "Iteration: 80. Loss: 0.0005896759103052318. Accuracy: 20\n",
      "Iteration: 81. Loss: 0.0002902221749536693. Accuracy: 24\n",
      "Iteration: 82. Loss: 0.0011424255790188909. Accuracy: 20\n",
      "Iteration: 83. Loss: 0.0006475829868577421. Accuracy: 20\n",
      "Iteration: 84. Loss: 0.0007476806640625. Accuracy: 18\n",
      "Iteration: 85. Loss: 0.0010513305896893144. Accuracy: 22\n",
      "Iteration: 86. Loss: 0.00048049926408566535. Accuracy: 18\n",
      "Iteration: 87. Loss: 0.0002209472586400807. Accuracy: 20\n",
      "Iteration: 88. Loss: 0.004766769241541624. Accuracy: 18\n",
      "Iteration: 89. Loss: 0.0008525848388671875. Accuracy: 14\n",
      "Iteration: 90. Loss: 0.0006558990571647882. Accuracy: 24\n",
      "Iteration: 91. Loss: 0.001534423790872097. Accuracy: 20\n",
      "Iteration: 92. Loss: 0.00044158936361782253. Accuracy: 20\n",
      "Iteration: 93. Loss: 0.000520248431712389. Accuracy: 20\n",
      "Iteration: 94. Loss: 0.0009071350214071572. Accuracy: 18\n",
      "Iteration: 95. Loss: 0.0009559631580486894. Accuracy: 20\n",
      "Iteration: 96. Loss: 0.0003649902355391532. Accuracy: 20\n",
      "Iteration: 97. Loss: 0.0004039764462504536. Accuracy: 22\n",
      "Iteration: 98. Loss: 0.0009412384242750704. Accuracy: 22\n",
      "Iteration: 99. Loss: 0.0004565429699141532. Accuracy: 14\n",
      "Iteration: 100. Loss: 0.0008802032680250704. Accuracy: 22\n",
      "Iteration: 101. Loss: 0.0016387939685955644. Accuracy: 20\n",
      "Iteration: 102. Loss: 0.0003170013369526714. Accuracy: 18\n",
      "Iteration: 103. Loss: 0.0003822326543740928. Accuracy: 20\n",
      "Iteration: 104. Loss: 0.0005030822940170765. Accuracy: 22\n",
      "Iteration: 105. Loss: 0.003944549709558487. Accuracy: 22\n",
      "Iteration: 106. Loss: 0.0005055999499745667. Accuracy: 24\n",
      "Iteration: 107. Loss: 0.0008452606271021068. Accuracy: 20\n",
      "Iteration: 108. Loss: 0.0024436188396066427. Accuracy: 22\n",
      "Iteration: 109. Loss: 0.0001948547433130443. Accuracy: 22\n",
      "Iteration: 110. Loss: 0.00024154662969522178. Accuracy: 20\n",
      "Iteration: 111. Loss: 0.0003520965692587197. Accuracy: 20\n",
      "Iteration: 112. Loss: 0.001603088341653347. Accuracy: 18\n",
      "Iteration: 113. Loss: 0.0005052185151726007. Accuracy: 20\n",
      "Iteration: 114. Loss: 0.0003211212169844657. Accuracy: 16\n",
      "Iteration: 115. Loss: 0.0006160736083984375. Accuracy: 20\n",
      "Iteration: 116. Loss: 0.00042671203846111894. Accuracy: 20\n",
      "Iteration: 117. Loss: 0.0006494140834547579. Accuracy: 20\n",
      "Iteration: 118. Loss: 0.0006557464366778731. Accuracy: 22\n",
      "Iteration: 119. Loss: 0.0018293762113898993. Accuracy: 18\n",
      "Iteration: 120. Loss: 0.003411407582461834. Accuracy: 22\n",
      "Iteration: 121. Loss: 0.0006130218389444053. Accuracy: 18\n",
      "Iteration: 122. Loss: 0.00035949706216342747. Accuracy: 24\n",
      "Iteration: 123. Loss: 0.0002273559512104839. Accuracy: 18\n",
      "Iteration: 124. Loss: 0.0004863739013671875. Accuracy: 22\n",
      "Iteration: 125. Loss: 0.0002548980701249093. Accuracy: 22\n",
      "Iteration: 126. Loss: 0.0004856109735555947. Accuracy: 16\n",
      "Iteration: 127. Loss: 0.000608901958912611. Accuracy: 22\n",
      "Iteration: 128. Loss: 0.001938400324434042. Accuracy: 20\n",
      "Iteration: 129. Loss: 0.0005346679827198386. Accuracy: 20\n",
      "Iteration: 130. Loss: 0.00036827087751589715. Accuracy: 18\n",
      "Iteration: 131. Loss: 0.00046066284994594753. Accuracy: 18\n",
      "Iteration: 132. Loss: 0.00036834715865552425. Accuracy: 20\n",
      "Iteration: 133. Loss: 0.0004456329334061593. Accuracy: 20\n",
      "Iteration: 134. Loss: 0.0006435394170694053. Accuracy: 26\n",
      "Iteration: 135. Loss: 0.00039306640974245965. Accuracy: 22\n",
      "Iteration: 136. Loss: 0.00045494080404751003. Accuracy: 20\n",
      "Iteration: 137. Loss: 0.00029327391530387104. Accuracy: 16\n",
      "Iteration: 138. Loss: 0.0004991149762645364. Accuracy: 22\n",
      "Iteration: 139. Loss: 0.0007751464727334678. Accuracy: 24\n",
      "Iteration: 140. Loss: 0.00044158936361782253. Accuracy: 22\n",
      "Iteration: 141. Loss: 0.00015907287888694555. Accuracy: 16\n",
      "Iteration: 142. Loss: 0.0008073425269685686. Accuracy: 20\n",
      "Iteration: 143. Loss: 0.00038055420736782253. Accuracy: 20\n",
      "Iteration: 144. Loss: 0.000986556988209486. Accuracy: 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 145. Loss: 0.0005702209309674799. Accuracy: 20\n",
      "Iteration: 146. Loss: 0.0005661773611791432. Accuracy: 14\n",
      "Iteration: 147. Loss: 0.0004812622210010886. Accuracy: 22\n",
      "Iteration: 148. Loss: 0.0005778503254987299. Accuracy: 26\n",
      "Iteration: 149. Loss: 0.0009644317906349897. Accuracy: 22\n",
      "Iteration: 150. Loss: 0.0008400726364925504. Accuracy: 20\n",
      "Iteration: 151. Loss: 0.0006111908005550504. Accuracy: 20\n",
      "Iteration: 152. Loss: 0.0005121612339280546. Accuracy: 20\n",
      "Iteration: 153. Loss: 0.0005804443499073386. Accuracy: 22\n",
      "Iteration: 154. Loss: 0.00034332275390625. Accuracy: 16\n",
      "Iteration: 155. Loss: 0.0006031799130141735. Accuracy: 22\n",
      "Iteration: 156. Loss: 0.0007762908935546875. Accuracy: 18\n",
      "Iteration: 157. Loss: 0.0001653289800742641. Accuracy: 22\n",
      "Iteration: 158. Loss: 0.0003330230829305947. Accuracy: 22\n",
      "Iteration: 159. Loss: 0.0016596984351053834. Accuracy: 24\n",
      "Iteration: 160. Loss: 0.0005355834728106856. Accuracy: 20\n",
      "Iteration: 161. Loss: 0.0008206939673982561. Accuracy: 20\n",
      "Iteration: 162. Loss: 0.0004853057907894254. Accuracy: 26\n",
      "Iteration: 163. Loss: 0.0005516052478924394. Accuracy: 22\n",
      "Iteration: 164. Loss: 0.0003322601260151714. Accuracy: 16\n",
      "Iteration: 165. Loss: 0.0004954528994858265. Accuracy: 20\n",
      "Iteration: 166. Loss: 0.0003836822579614818. Accuracy: 20\n",
      "Iteration: 167. Loss: 0.0011927032610401511. Accuracy: 22\n",
      "Iteration: 168. Loss: 0.0002867889415938407. Accuracy: 24\n",
      "Iteration: 169. Loss: 0.0005451965262182057. Accuracy: 18\n",
      "Iteration: 170. Loss: 0.0002025604189839214. Accuracy: 18\n",
      "Iteration: 171. Loss: 0.00018104552873410285. Accuracy: 20\n",
      "Iteration: 172. Loss: 0.00042434691567905247. Accuracy: 24\n",
      "Iteration: 173. Loss: 0.0007627868908457458. Accuracy: 20\n",
      "Iteration: 174. Loss: 0.00023696899006608874. Accuracy: 22\n",
      "Iteration: 175. Loss: 0.001295547466725111. Accuracy: 22\n",
      "Iteration: 176. Loss: 0.0006433105445466936. Accuracy: 22\n",
      "Iteration: 177. Loss: 0.0003312682965770364. Accuracy: 20\n",
      "Iteration: 178. Loss: 0.0003653716994449496. Accuracy: 20\n",
      "Iteration: 179. Loss: 0.0002390289300819859. Accuracy: 22\n",
      "Iteration: 180. Loss: 0.00032135008950717747. Accuracy: 20\n",
      "Iteration: 181. Loss: 0.0005154418759047985. Accuracy: 16\n",
      "Iteration: 182. Loss: 0.0002349853457417339. Accuracy: 24\n",
      "Iteration: 183. Loss: 0.0005838012439198792. Accuracy: 22\n",
      "Iteration: 184. Loss: 0.0034945679362863302. Accuracy: 18\n",
      "Iteration: 185. Loss: 0.0006288146832957864. Accuracy: 20\n",
      "Iteration: 186. Loss: 0.0007668304606340826. Accuracy: 22\n",
      "Iteration: 187. Loss: 0.001075210515409708. Accuracy: 24\n",
      "Iteration: 188. Loss: 0.0007027435349300504. Accuracy: 18\n",
      "Iteration: 189. Loss: 0.0002925109874922782. Accuracy: 20\n",
      "Iteration: 190. Loss: 0.0002199554437538609. Accuracy: 24\n",
      "Iteration: 191. Loss: 0.0004336547863204032. Accuracy: 22\n",
      "Iteration: 192. Loss: 0.0004514312604442239. Accuracy: 20\n",
      "Iteration: 193. Loss: 0.001390762277878821. Accuracy: 22\n",
      "Iteration: 194. Loss: 0.00025535584427416325. Accuracy: 24\n",
      "Iteration: 195. Loss: 0.0006357574602589011. Accuracy: 24\n",
      "Iteration: 196. Loss: 0.000286102294921875. Accuracy: 20\n",
      "Iteration: 197. Loss: 0.0002880859246943146. Accuracy: 24\n",
      "Iteration: 198. Loss: 0.0016336822882294655. Accuracy: 20\n",
      "Iteration: 199. Loss: 0.0004993438487872481. Accuracy: 24\n",
      "Iteration: 200. Loss: 0.000278472900390625. Accuracy: 22\n",
      "Iteration: 201. Loss: 0.000512619037181139. Accuracy: 20\n",
      "Iteration: 202. Loss: 0.00032249451032839715. Accuracy: 22\n",
      "Iteration: 203. Loss: 0.0003376769891474396. Accuracy: 20\n",
      "Iteration: 204. Loss: 0.0006177520845085382. Accuracy: 18\n",
      "Iteration: 205. Loss: 0.000370025634765625. Accuracy: 20\n",
      "Iteration: 206. Loss: 0.0007167815929278731. Accuracy: 22\n",
      "Iteration: 207. Loss: 0.00038505555130541325. Accuracy: 20\n",
      "Iteration: 208. Loss: 0.000415802001953125. Accuracy: 18\n",
      "Iteration: 209. Loss: 0.00020996094099245965. Accuracy: 20\n",
      "Iteration: 210. Loss: 0.0007228088215924799. Accuracy: 24\n",
      "Iteration: 211. Loss: 0.0005181884625926614. Accuracy: 18\n",
      "Iteration: 212. Loss: 0.0005615997361019254. Accuracy: 24\n",
      "Iteration: 213. Loss: 0.0002575683465693146. Accuracy: 24\n",
      "Iteration: 214. Loss: 0.0003977966262027621. Accuracy: 22\n",
      "Iteration: 215. Loss: 0.0002978515694849193. Accuracy: 24\n",
      "Iteration: 216. Loss: 0.0003970336983911693. Accuracy: 18\n",
      "Iteration: 217. Loss: 0.0004922485095448792. Accuracy: 24\n",
      "Iteration: 218. Loss: 0.0004306793271098286. Accuracy: 20\n",
      "Iteration: 219. Loss: 0.00028274537180550396. Accuracy: 22\n",
      "Iteration: 220. Loss: 0.000675201416015625. Accuracy: 22\n",
      "Iteration: 221. Loss: 0.0006702422979287803. Accuracy: 18\n",
      "Iteration: 222. Loss: 0.0002828979631885886. Accuracy: 24\n",
      "Iteration: 223. Loss: 0.0006449890206567943. Accuracy: 24\n",
      "Iteration: 224. Loss: 0.00036193846608512104. Accuracy: 22\n",
      "Iteration: 225. Loss: 0.0012048339704051614. Accuracy: 22\n",
      "Iteration: 226. Loss: 0.00026985167642123997. Accuracy: 22\n",
      "Iteration: 227. Loss: 0.0004164123674854636. Accuracy: 18\n",
      "Iteration: 228. Loss: 0.0006953430129215121. Accuracy: 22\n",
      "Iteration: 229. Loss: 0.0002132415829692036. Accuracy: 20\n",
      "Iteration: 230. Loss: 0.0007057953043840826. Accuracy: 22\n",
      "Iteration: 231. Loss: 0.0005198669387027621. Accuracy: 20\n",
      "Iteration: 232. Loss: 0.0004618835519067943. Accuracy: 16\n",
      "Iteration: 233. Loss: 0.0007159423548728228. Accuracy: 18\n",
      "Iteration: 234. Loss: 0.00019409180094953626. Accuracy: 22\n",
      "Iteration: 235. Loss: 0.00019470215192995965. Accuracy: 20\n",
      "Iteration: 236. Loss: 0.0007700347923673689. Accuracy: 22\n",
      "Iteration: 237. Loss: 0.0011036682408303022. Accuracy: 20\n",
      "Iteration: 238. Loss: 0.00010086059774039313. Accuracy: 20\n",
      "Iteration: 239. Loss: 0.0004676818789448589. Accuracy: 22\n",
      "Iteration: 240. Loss: 0.001270828302949667. Accuracy: 18\n",
      "Iteration: 241. Loss: 0.00031951905111782253. Accuracy: 22\n",
      "Iteration: 242. Loss: 0.00037239075754769146. Accuracy: 22\n",
      "Iteration: 243. Loss: 0.00044105530832894146. Accuracy: 20\n",
      "Iteration: 244. Loss: 0.00025924682267941535. Accuracy: 26\n",
      "Iteration: 245. Loss: 0.0004035186721011996. Accuracy: 20\n",
      "Iteration: 246. Loss: 0.00029739379533566535. Accuracy: 24\n",
      "Iteration: 247. Loss: 0.0002101135323755443. Accuracy: 20\n",
      "Iteration: 248. Loss: 0.0009948730003088713. Accuracy: 24\n",
      "Iteration: 249. Loss: 0.0002085113519569859. Accuracy: 18\n",
      "Iteration: 250. Loss: 0.0005034637288190424. Accuracy: 20\n",
      "Iteration: 251. Loss: 0.0002452850458212197. Accuracy: 22\n",
      "Iteration: 252. Loss: 0.0037631988525390625. Accuracy: 22\n",
      "Iteration: 253. Loss: 0.00043853759416379035. Accuracy: 24\n",
      "Iteration: 254. Loss: 0.001040496863424778. Accuracy: 20\n",
      "Iteration: 255. Loss: 0.0005013275076635182. Accuracy: 22\n",
      "Iteration: 256. Loss: 0.0005706787342205644. Accuracy: 20\n",
      "Iteration: 257. Loss: 0.0006571197300218046. Accuracy: 18\n",
      "Iteration: 258. Loss: 0.0003363800060469657. Accuracy: 22\n",
      "Iteration: 259. Loss: 0.0005984497256577015. Accuracy: 22\n",
      "Iteration: 260. Loss: 0.00044921875814907253. Accuracy: 20\n",
      "Iteration: 261. Loss: 0.0006674194592051208. Accuracy: 24\n",
      "Iteration: 262. Loss: 0.0006157684256322682. Accuracy: 20\n",
      "Iteration: 263. Loss: 0.0001922607480082661. Accuracy: 22\n",
      "Iteration: 264. Loss: 0.0002464294375386089. Accuracy: 20\n",
      "Iteration: 265. Loss: 0.0010907745454460382. Accuracy: 22\n",
      "Iteration: 266. Loss: 0.001062698313035071. Accuracy: 20\n",
      "Iteration: 267. Loss: 0.00010948181443382055. Accuracy: 16\n",
      "Iteration: 268. Loss: 0.0005753326695412397. Accuracy: 20\n",
      "Iteration: 269. Loss: 0.0003146362432744354. Accuracy: 24\n",
      "Iteration: 270. Loss: 0.00031455993303097785. Accuracy: 22\n",
      "Iteration: 271. Loss: 0.0004634857177734375. Accuracy: 20\n",
      "Iteration: 272. Loss: 0.0004750060907099396. Accuracy: 24\n",
      "Iteration: 273. Loss: 0.0002758026239462197. Accuracy: 22\n",
      "Iteration: 274. Loss: 0.0011897278018295765. Accuracy: 20\n",
      "Iteration: 275. Loss: 0.0001758575381245464. Accuracy: 24\n",
      "Iteration: 276. Loss: 0.0005811309674754739. Accuracy: 22\n",
      "Iteration: 277. Loss: 0.001024551340378821. Accuracy: 20\n",
      "Iteration: 278. Loss: 0.0004595947393681854. Accuracy: 20\n",
      "Iteration: 279. Loss: 0.00032096862560138106. Accuracy: 22\n",
      "Iteration: 280. Loss: 0.0002085113519569859. Accuracy: 24\n",
      "Iteration: 281. Loss: 0.0007706451578997076. Accuracy: 16\n",
      "Iteration: 282. Loss: 0.00030532837263308465. Accuracy: 20\n",
      "Iteration: 283. Loss: 0.0005605316255241632. Accuracy: 20\n",
      "Iteration: 284. Loss: 0.0001464843808207661. Accuracy: 18\n",
      "Iteration: 285. Loss: 0.0004493713495321572. Accuracy: 24\n",
      "Iteration: 286. Loss: 0.0004009246767964214. Accuracy: 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 287. Loss: 0.0004665374872274697. Accuracy: 22\n",
      "Iteration: 288. Loss: 0.00013473510625772178. Accuracy: 24\n",
      "Iteration: 289. Loss: 0.00035072327591478825. Accuracy: 20\n",
      "Iteration: 290. Loss: 0.0006639098864980042. Accuracy: 20\n",
      "Iteration: 291. Loss: 0.0003100585890933871. Accuracy: 20\n",
      "Iteration: 292. Loss: 0.0004226684686727822. Accuracy: 22\n",
      "Iteration: 293. Loss: 0.0019891357515007257. Accuracy: 20\n",
      "Iteration: 294. Loss: 0.00010559082147665322. Accuracy: 20\n",
      "Iteration: 295. Loss: 0.0009846496395766735. Accuracy: 22\n",
      "Iteration: 296. Loss: 0.00029800416086800396. Accuracy: 20\n",
      "Iteration: 297. Loss: 0.0006199646159075201. Accuracy: 20\n",
      "Iteration: 298. Loss: 0.0002539825509302318. Accuracy: 20\n",
      "Iteration: 299. Loss: 0.0005614471156150103. Accuracy: 26\n",
      "Iteration: 300. Loss: 0.00030799864907748997. Accuracy: 18\n",
      "Iteration: 301. Loss: 0.0002988433698192239. Accuracy: 22\n",
      "Iteration: 302. Loss: 0.00028900147299282253. Accuracy: 24\n",
      "Iteration: 303. Loss: 0.00029853821615688503. Accuracy: 22\n",
      "Iteration: 304. Loss: 0.00044960022205486894. Accuracy: 24\n",
      "Iteration: 305. Loss: 0.00030151367536745965. Accuracy: 22\n",
      "Iteration: 306. Loss: 0.00018356322834733874. Accuracy: 20\n",
      "Iteration: 307. Loss: 0.0003813934454228729. Accuracy: 22\n",
      "Iteration: 308. Loss: 0.0006809997721575201. Accuracy: 22\n",
      "Iteration: 309. Loss: 0.0003801727434620261. Accuracy: 22\n",
      "Iteration: 310. Loss: 0.0011885070707648993. Accuracy: 24\n",
      "Iteration: 311. Loss: 0.0003952026308979839. Accuracy: 22\n",
      "Iteration: 312. Loss: 0.0003978729364462197. Accuracy: 20\n",
      "Iteration: 313. Loss: 0.00021186828962527215. Accuracy: 24\n",
      "Iteration: 314. Loss: 0.000932235736399889. Accuracy: 24\n",
      "Iteration: 315. Loss: 0.0002222442562924698. Accuracy: 20\n",
      "Iteration: 316. Loss: 0.00015815734514035285. Accuracy: 20\n",
      "Iteration: 317. Loss: 0.0007356262067332864. Accuracy: 22\n",
      "Iteration: 318. Loss: 0.0005169677897356451. Accuracy: 20\n",
      "Iteration: 319. Loss: 0.00041793822310864925. Accuracy: 22\n",
      "Iteration: 320. Loss: 0.0009089660597965121. Accuracy: 20\n",
      "Iteration: 321. Loss: 0.0006496429559774697. Accuracy: 18\n",
      "Iteration: 322. Loss: 0.0003646850527729839. Accuracy: 20\n",
      "Iteration: 323. Loss: 0.0002475738583598286. Accuracy: 20\n",
      "Iteration: 324. Loss: 0.0005212402320466936. Accuracy: 22\n",
      "Iteration: 325. Loss: 0.00038436890463344753. Accuracy: 22\n",
      "Iteration: 326. Loss: 0.008412475697696209. Accuracy: 20\n",
      "Iteration: 327. Loss: 0.00011146545148221776. Accuracy: 20\n",
      "Iteration: 328. Loss: 0.0003117370652034879. Accuracy: 18\n",
      "Iteration: 329. Loss: 0.0014565277379006147. Accuracy: 20\n",
      "Iteration: 330. Loss: 0.00046630858560092747. Accuracy: 22\n",
      "Iteration: 331. Loss: 0.00023345947556663305. Accuracy: 24\n",
      "Iteration: 332. Loss: 0.00010032653517555445. Accuracy: 22\n",
      "Iteration: 333. Loss: 0.00025321959401480854. Accuracy: 24\n",
      "Iteration: 334. Loss: 0.0007172393961809576. Accuracy: 22\n",
      "Iteration: 335. Loss: 0.001112441997975111. Accuracy: 16\n",
      "Iteration: 336. Loss: 0.00038429259438998997. Accuracy: 24\n",
      "Iteration: 337. Loss: 0.0001503753592260182. Accuracy: 26\n",
      "Iteration: 338. Loss: 0.0003033447137568146. Accuracy: 20\n",
      "Iteration: 339. Loss: 0.00032852173899300396. Accuracy: 20\n",
      "Iteration: 340. Loss: 0.00018875121895689517. Accuracy: 24\n",
      "Iteration: 341. Loss: 0.00026870728470385075. Accuracy: 20\n",
      "Iteration: 342. Loss: 0.0002193450927734375. Accuracy: 20\n",
      "Iteration: 343. Loss: 0.0005561828729696572. Accuracy: 22\n",
      "Iteration: 344. Loss: 0.0005382538074627519. Accuracy: 20\n",
      "Iteration: 345. Loss: 0.0007881164783611894. Accuracy: 22\n",
      "Iteration: 346. Loss: 0.00018524170445743948. Accuracy: 22\n",
      "Iteration: 347. Loss: 0.0005184936453588307. Accuracy: 18\n",
      "Iteration: 348. Loss: 0.0002662658807821572. Accuracy: 24\n",
      "Iteration: 349. Loss: 0.0016021728515625. Accuracy: 22\n",
      "Iteration: 350. Loss: 0.0014182281447574496. Accuracy: 18\n",
      "Iteration: 351. Loss: 0.0006735229399055243. Accuracy: 20\n",
      "Iteration: 352. Loss: 0.0004968261928297579. Accuracy: 24\n",
      "Iteration: 353. Loss: 0.0002520752022974193. Accuracy: 22\n",
      "Iteration: 354. Loss: 0.0005109405610710382. Accuracy: 18\n",
      "Iteration: 355. Loss: 0.00020645141194108874. Accuracy: 24\n",
      "Iteration: 356. Loss: 0.0004485321114771068. Accuracy: 20\n",
      "Iteration: 357. Loss: 0.0004093933093827218. Accuracy: 20\n",
      "Iteration: 358. Loss: 0.0004547119024209678. Accuracy: 16\n",
      "Iteration: 359. Loss: 0.00016578673967160285. Accuracy: 22\n",
      "Iteration: 360. Loss: 0.00044036866165697575. Accuracy: 20\n",
      "Iteration: 361. Loss: 0.0003777313104365021. Accuracy: 24\n",
      "Iteration: 362. Loss: 0.0024922944139689207. Accuracy: 22\n",
      "Iteration: 363. Loss: 0.0006233215099200606. Accuracy: 22\n",
      "Iteration: 364. Loss: 0.0004313659737817943. Accuracy: 24\n",
      "Iteration: 365. Loss: 0.0007487487746402621. Accuracy: 22\n",
      "Iteration: 366. Loss: 0.0003649902355391532. Accuracy: 24\n",
      "Iteration: 367. Loss: 0.0006717682117596269. Accuracy: 24\n",
      "Iteration: 368. Loss: 0.00040344239096157253. Accuracy: 26\n",
      "Iteration: 369. Loss: 0.0007982635288499296. Accuracy: 22\n",
      "Iteration: 370. Loss: 0.0009253692696802318. Accuracy: 20\n",
      "Iteration: 371. Loss: 0.0007611846667714417. Accuracy: 14\n",
      "Iteration: 372. Loss: 0.0004787445068359375. Accuracy: 20\n",
      "Iteration: 373. Loss: 0.0006378173711709678. Accuracy: 22\n",
      "Iteration: 374. Loss: 0.0006487274076789618. Accuracy: 20\n",
      "Iteration: 375. Loss: 0.00020133971702307463. Accuracy: 16\n",
      "Iteration: 376. Loss: 0.00017044067499227822. Accuracy: 20\n",
      "Iteration: 377. Loss: 0.00023315429280046374. Accuracy: 22\n",
      "Iteration: 378. Loss: 0.00023101807164493948. Accuracy: 22\n",
      "Iteration: 379. Loss: 0.00010971069423248991. Accuracy: 22\n",
      "Iteration: 380. Loss: 0.0004431152483448386. Accuracy: 22\n",
      "Iteration: 381. Loss: 0.0003732299664989114. Accuracy: 22\n",
      "Iteration: 382. Loss: 0.0003792571951635182. Accuracy: 24\n",
      "Iteration: 383. Loss: 0.0002571868826635182. Accuracy: 18\n",
      "Iteration: 384. Loss: 0.0003205108514521271. Accuracy: 22\n",
      "Iteration: 385. Loss: 0.0006357574602589011. Accuracy: 22\n",
      "Iteration: 386. Loss: 0.0007036590832285583. Accuracy: 24\n",
      "Iteration: 387. Loss: 0.0002368164132349193. Accuracy: 20\n",
      "Iteration: 388. Loss: 0.0003467559872660786. Accuracy: 22\n",
      "Iteration: 389. Loss: 0.0006352996570058167. Accuracy: 18\n",
      "Iteration: 390. Loss: 0.0005435180501081049. Accuracy: 22\n",
      "Iteration: 391. Loss: 0.0006911468226462603. Accuracy: 22\n",
      "Iteration: 392. Loss: 0.0007939910865388811. Accuracy: 24\n",
      "Iteration: 393. Loss: 0.00024276733165606856. Accuracy: 22\n",
      "Iteration: 394. Loss: 0.0002549743512645364. Accuracy: 18\n",
      "Iteration: 395. Loss: 0.00043731689220294356. Accuracy: 22\n",
      "Iteration: 396. Loss: 0.0004859924374613911. Accuracy: 20\n",
      "Iteration: 397. Loss: 0.0005540466518141329. Accuracy: 22\n",
      "Iteration: 398. Loss: 0.0002017974911723286. Accuracy: 22\n",
      "Iteration: 399. Loss: 0.0001483917294535786. Accuracy: 24\n",
      "Iteration: 400. Loss: 0.00023155212693382055. Accuracy: 24\n",
      "Iteration: 401. Loss: 0.0002974701055791229. Accuracy: 22\n",
      "Iteration: 402. Loss: 0.0005479431129060686. Accuracy: 22\n",
      "Iteration: 403. Loss: 0.0005863189580850303. Accuracy: 20\n",
      "Iteration: 404. Loss: 8.178711141226813e-05. Accuracy: 18\n",
      "Iteration: 405. Loss: 0.0003296661307103932. Accuracy: 24\n",
      "Iteration: 406. Loss: 0.0003078460576944053. Accuracy: 20\n",
      "Iteration: 407. Loss: 0.001536407507956028. Accuracy: 24\n",
      "Iteration: 408. Loss: 0.0003028869687113911. Accuracy: 24\n",
      "Iteration: 409. Loss: 0.00018936156993731856. Accuracy: 20\n",
      "Iteration: 410. Loss: 0.0001913452142616734. Accuracy: 20\n",
      "Iteration: 411. Loss: 0.00023094177595339715. Accuracy: 20\n",
      "Iteration: 412. Loss: 0.0004437255847733468. Accuracy: 24\n",
      "Iteration: 413. Loss: 0.0007509613060392439. Accuracy: 22\n",
      "Iteration: 414. Loss: 0.0003040313604287803. Accuracy: 22\n",
      "Iteration: 415. Loss: 0.0029219817370176315. Accuracy: 22\n",
      "Iteration: 416. Loss: 0.0013650512555614114. Accuracy: 20\n",
      "Iteration: 417. Loss: 0.0003818511904682964. Accuracy: 20\n",
      "Iteration: 418. Loss: 0.0001179504397441633. Accuracy: 24\n",
      "Iteration: 419. Loss: 0.00034309388138353825. Accuracy: 24\n",
      "Iteration: 420. Loss: 0.0003825378371402621. Accuracy: 18\n",
      "Iteration: 421. Loss: 0.0001611328189028427. Accuracy: 22\n",
      "Iteration: 422. Loss: 0.00010810852108988911. Accuracy: 24\n",
      "Iteration: 423. Loss: 0.0002651214599609375. Accuracy: 20\n",
      "Iteration: 424. Loss: 0.0003010559012182057. Accuracy: 24\n",
      "Iteration: 425. Loss: 0.00027061463333666325. Accuracy: 22\n",
      "Iteration: 426. Loss: 0.00011550903582246974. Accuracy: 20\n",
      "Iteration: 427. Loss: 0.0007724761962890625. Accuracy: 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 428. Loss: 0.0002606963971629739. Accuracy: 22\n",
      "Iteration: 429. Loss: 0.00013648986350744963. Accuracy: 20\n",
      "Iteration: 430. Loss: 0.0005401611560955644. Accuracy: 22\n",
      "Iteration: 431. Loss: 0.0029719544108957052. Accuracy: 20\n",
      "Iteration: 432. Loss: 0.00042297362233512104. Accuracy: 22\n",
      "Iteration: 433. Loss: 0.0028584289830178022. Accuracy: 22\n",
      "Iteration: 434. Loss: 0.0006199646159075201. Accuracy: 22\n",
      "Iteration: 435. Loss: 0.0003814697265625. Accuracy: 20\n",
      "Iteration: 436. Loss: 0.00015136718866415322. Accuracy: 22\n",
      "Iteration: 437. Loss: 0.0008784484816715121. Accuracy: 22\n",
      "Iteration: 438. Loss: 0.0009053039830178022. Accuracy: 24\n",
      "Iteration: 439. Loss: 0.00030517578125. Accuracy: 22\n",
      "Iteration: 440. Loss: 0.00041366578079760075. Accuracy: 24\n",
      "Iteration: 441. Loss: 0.0002011871401919052. Accuracy: 24\n",
      "Iteration: 442. Loss: 0.0001979827939067036. Accuracy: 24\n",
      "Iteration: 443. Loss: 0.0005323028308339417. Accuracy: 24\n",
      "Iteration: 444. Loss: 0.00032661439036019146. Accuracy: 22\n",
      "Iteration: 445. Loss: 0.00018989562522619963. Accuracy: 24\n",
      "Iteration: 446. Loss: 0.0002811431768350303. Accuracy: 22\n",
      "Iteration: 447. Loss: 0.0002510833728592843. Accuracy: 18\n",
      "Iteration: 448. Loss: 0.0001493835443397984. Accuracy: 22\n",
      "Iteration: 449. Loss: 0.0004467010439839214. Accuracy: 24\n",
      "Iteration: 450. Loss: 0.00553409568965435. Accuracy: 22\n",
      "Iteration: 451. Loss: 0.00010604858107399195. Accuracy: 22\n",
      "Iteration: 452. Loss: 0.00014755249139852822. Accuracy: 22\n",
      "Iteration: 453. Loss: 0.0006594085716642439. Accuracy: 18\n",
      "Iteration: 454. Loss: 0.0003070068487431854. Accuracy: 16\n",
      "Iteration: 455. Loss: 0.0005101013230159879. Accuracy: 18\n",
      "Iteration: 456. Loss: 0.0001636505185160786. Accuracy: 24\n",
      "Iteration: 457. Loss: 0.0001776886056177318. Accuracy: 24\n",
      "Iteration: 458. Loss: 0.00027030945057049394. Accuracy: 20\n",
      "Iteration: 459. Loss: 0.00023155212693382055. Accuracy: 22\n",
      "Iteration: 460. Loss: 0.0004912567092105746. Accuracy: 22\n",
      "Iteration: 461. Loss: 0.00024940489674918354. Accuracy: 18\n",
      "Iteration: 462. Loss: 0.0002012634213315323. Accuracy: 24\n",
      "Iteration: 463. Loss: 0.00027030945057049394. Accuracy: 20\n",
      "Iteration: 464. Loss: 0.0002387237618677318. Accuracy: 24\n",
      "Iteration: 465. Loss: 0.00016067504475358874. Accuracy: 16\n",
      "Iteration: 466. Loss: 0.00047477721818722785. Accuracy: 24\n",
      "Iteration: 467. Loss: 0.000133514404296875. Accuracy: 24\n",
      "Iteration: 468. Loss: 0.0002872467157430947. Accuracy: 24\n",
      "Iteration: 469. Loss: 0.00024551391834393144. Accuracy: 24\n",
      "Iteration: 470. Loss: 0.00026298523880541325. Accuracy: 22\n",
      "Iteration: 471. Loss: 0.0003245544503442943. Accuracy: 20\n",
      "Iteration: 472. Loss: 0.0004900360363535583. Accuracy: 22\n",
      "Iteration: 473. Loss: 0.0004605102585628629. Accuracy: 20\n",
      "Iteration: 474. Loss: 0.0002349090646021068. Accuracy: 22\n",
      "Iteration: 475. Loss: 0.0008580780122429132. Accuracy: 24\n",
      "Iteration: 476. Loss: 0.0001894378656288609. Accuracy: 24\n",
      "Iteration: 477. Loss: 0.0010102844098582864. Accuracy: 24\n",
      "Iteration: 478. Loss: 0.0003236389020457864. Accuracy: 20\n",
      "Iteration: 479. Loss: 0.00040863038157112896. Accuracy: 20\n",
      "Iteration: 480. Loss: 0.0004884338122792542. Accuracy: 20\n",
      "Iteration: 481. Loss: 0.00014823913807049394. Accuracy: 28\n",
      "Iteration: 482. Loss: 0.0006391143542714417. Accuracy: 18\n",
      "Iteration: 483. Loss: 0.00015754699415992945. Accuracy: 22\n",
      "Iteration: 484. Loss: 0.00020988464530091733. Accuracy: 26\n",
      "Iteration: 485. Loss: 0.00019531250291038305. Accuracy: 22\n",
      "Iteration: 486. Loss: 0.00010726929031079635. Accuracy: 24\n",
      "Iteration: 487. Loss: 0.0002422332763671875. Accuracy: 24\n",
      "Iteration: 488. Loss: 0.00027900695567950606. Accuracy: 22\n",
      "Iteration: 489. Loss: 0.00013275146193336695. Accuracy: 22\n",
      "Iteration: 490. Loss: 0.00047317505232058465. Accuracy: 24\n",
      "Iteration: 491. Loss: 0.0008638763683848083. Accuracy: 26\n",
      "Iteration: 492. Loss: 0.0002731323183979839. Accuracy: 22\n",
      "Iteration: 493. Loss: 0.0012828826438635588. Accuracy: 22\n",
      "Iteration: 494. Loss: 0.000612716656178236. Accuracy: 22\n",
      "Iteration: 495. Loss: 0.00019973755115643144. Accuracy: 20\n",
      "Iteration: 496. Loss: 0.0003039550792891532. Accuracy: 22\n",
      "Iteration: 497. Loss: 8.888244337867945e-05. Accuracy: 22\n",
      "Iteration: 498. Loss: 0.0007191467448137701. Accuracy: 24\n",
      "Iteration: 499. Loss: 0.0009079742594622076. Accuracy: 22\n",
      "Iteration: 500. Loss: 0.00029708861256949604. Accuracy: 16\n",
      "Iteration: 501. Loss: 0.00034622193197719753. Accuracy: 22\n",
      "Iteration: 502. Loss: 0.0001795959542505443. Accuracy: 22\n",
      "Iteration: 503. Loss: 0.00024650574778206646. Accuracy: 22\n",
      "Iteration: 504. Loss: 0.00021354675118345767. Accuracy: 24\n",
      "Iteration: 505. Loss: 0.00022270203044172376. Accuracy: 22\n",
      "Iteration: 506. Loss: 0.00025154114700853825. Accuracy: 24\n",
      "Iteration: 507. Loss: 0.00022773742966819555. Accuracy: 22\n",
      "Iteration: 508. Loss: 0.0003033447137568146. Accuracy: 18\n",
      "Iteration: 509. Loss: 0.0003958892775699496. Accuracy: 22\n",
      "Iteration: 510. Loss: 0.0014189147623255849. Accuracy: 20\n",
      "Iteration: 511. Loss: 0.00013473510625772178. Accuracy: 20\n",
      "Iteration: 512. Loss: 0.00046333312639035285. Accuracy: 24\n",
      "Iteration: 513. Loss: 0.0004264068556949496. Accuracy: 22\n",
      "Iteration: 514. Loss: 0.0005194854456931353. Accuracy: 24\n",
      "Iteration: 515. Loss: 0.00017532348283566535. Accuracy: 22\n",
      "Iteration: 516. Loss: 0.00013214111095294356. Accuracy: 20\n",
      "Iteration: 517. Loss: 0.00017250061500817537. Accuracy: 20\n",
      "Iteration: 518. Loss: 0.0002707672247197479. Accuracy: 24\n",
      "Iteration: 519. Loss: 0.0003215789911337197. Accuracy: 16\n",
      "Iteration: 520. Loss: 0.00048065185546875. Accuracy: 26\n",
      "Iteration: 521. Loss: 0.0003235626209061593. Accuracy: 24\n",
      "Iteration: 522. Loss: 0.00021865844610147178. Accuracy: 20\n",
      "Iteration: 523. Loss: 0.00032089234446175396. Accuracy: 20\n",
      "Iteration: 524. Loss: 0.00030281065846793354. Accuracy: 22\n",
      "Iteration: 525. Loss: 0.0003446197370067239. Accuracy: 24\n",
      "Iteration: 526. Loss: 0.00023513793712481856. Accuracy: 20\n",
      "Iteration: 527. Loss: 0.00020927429432049394. Accuracy: 24\n",
      "Iteration: 528. Loss: 0.0002282714849570766. Accuracy: 22\n",
      "Iteration: 529. Loss: 0.001676254323683679. Accuracy: 20\n",
      "Iteration: 530. Loss: 0.00015075683768372983. Accuracy: 24\n",
      "Iteration: 531. Loss: 0.00012138366582803428. Accuracy: 20\n",
      "Iteration: 532. Loss: 0.0001865387021098286. Accuracy: 24\n",
      "Iteration: 533. Loss: 0.0005715942243114114. Accuracy: 22\n",
      "Iteration: 534. Loss: 0.0005568694905377924. Accuracy: 22\n",
      "Iteration: 535. Loss: 0.00019096375035587698. Accuracy: 24\n",
      "Iteration: 536. Loss: 0.0004836273146793246. Accuracy: 20\n",
      "Iteration: 537. Loss: 0.0007015991141088307. Accuracy: 18\n",
      "Iteration: 538. Loss: 0.0014334869338199496. Accuracy: 24\n",
      "Iteration: 539. Loss: 0.00019271850760560483. Accuracy: 22\n",
      "Iteration: 540. Loss: 0.0002241516049252823. Accuracy: 20\n",
      "Iteration: 541. Loss: 0.00029731751419603825. Accuracy: 20\n",
      "Iteration: 542. Loss: 0.00019500732014421374. Accuracy: 24\n",
      "Iteration: 543. Loss: 0.00034667967702262104. Accuracy: 20\n",
      "Iteration: 544. Loss: 0.0012664794921875. Accuracy: 20\n",
      "Iteration: 545. Loss: 0.0012502288445830345. Accuracy: 24\n",
      "Iteration: 546. Loss: 0.0006286621210165322. Accuracy: 22\n",
      "Iteration: 547. Loss: 0.00042778014903888106. Accuracy: 22\n",
      "Iteration: 548. Loss: 0.0002883911074604839. Accuracy: 20\n",
      "Iteration: 549. Loss: 0.0001789093075785786. Accuracy: 24\n",
      "Iteration: 550. Loss: 0.0003188324044458568. Accuracy: 22\n",
      "Iteration: 551. Loss: 0.0002666473446879536. Accuracy: 18\n",
      "Iteration: 552. Loss: 0.0001894378656288609. Accuracy: 24\n",
      "Iteration: 553. Loss: 0.0002524566662032157. Accuracy: 24\n",
      "Iteration: 554. Loss: 0.0008233642438426614. Accuracy: 20\n",
      "Iteration: 555. Loss: 0.00019821166642941535. Accuracy: 20\n",
      "Iteration: 556. Loss: 0.00014282226038631052. Accuracy: 24\n",
      "Iteration: 557. Loss: 0.0002359008794883266. Accuracy: 24\n",
      "Iteration: 558. Loss: 0.0009025573963299394. Accuracy: 20\n",
      "Iteration: 559. Loss: 0.00025764465681277215. Accuracy: 22\n",
      "Iteration: 560. Loss: 0.0002333068841835484. Accuracy: 24\n",
      "Iteration: 561. Loss: 0.00011260986502747983. Accuracy: 18\n",
      "Iteration: 562. Loss: 0.00024200438929256052. Accuracy: 20\n",
      "Iteration: 563. Loss: 0.0003337860107421875. Accuracy: 24\n",
      "Iteration: 564. Loss: 0.00029327391530387104. Accuracy: 18\n",
      "Iteration: 565. Loss: 0.0005776977632194757. Accuracy: 26\n",
      "Iteration: 566. Loss: 0.0002523803850635886. Accuracy: 20\n",
      "Iteration: 567. Loss: 0.0001338958682026714. Accuracy: 24\n",
      "Iteration: 568. Loss: 0.0001917266781674698. Accuracy: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 569. Loss: 0.0004822540213353932. Accuracy: 22\n",
      "Iteration: 570. Loss: 0.0003722381661646068. Accuracy: 20\n",
      "Iteration: 571. Loss: 8.96453857421875e-05. Accuracy: 24\n",
      "Iteration: 572. Loss: 0.00027603149646893144. Accuracy: 22\n",
      "Iteration: 573. Loss: 0.00012825011799577624. Accuracy: 22\n",
      "Iteration: 574. Loss: 0.00013847350783180445. Accuracy: 22\n",
      "Iteration: 575. Loss: 0.00035873413435183465. Accuracy: 16\n",
      "Iteration: 576. Loss: 0.0001090240475605242. Accuracy: 24\n",
      "Iteration: 577. Loss: 0.00015747069846838713. Accuracy: 24\n",
      "Iteration: 578. Loss: 0.00014564514276571572. Accuracy: 24\n",
      "Iteration: 579. Loss: 0.0008174896356649697. Accuracy: 22\n",
      "Iteration: 580. Loss: 0.00010383605695096776. Accuracy: 22\n",
      "Iteration: 581. Loss: 0.0004273223748896271. Accuracy: 20\n",
      "Iteration: 582. Loss: 0.00026908874860964715. Accuracy: 22\n",
      "Iteration: 583. Loss: 0.0003147888055536896. Accuracy: 22\n",
      "Iteration: 584. Loss: 0.00030082702869549394. Accuracy: 22\n",
      "Iteration: 585. Loss: 0.00017333983851131052. Accuracy: 22\n",
      "Iteration: 586. Loss: 0.0002152252127416432. Accuracy: 22\n",
      "Iteration: 587. Loss: 0.00012855530076194555. Accuracy: 22\n",
      "Iteration: 588. Loss: 0.0001426696835551411. Accuracy: 22\n",
      "Iteration: 589. Loss: 0.00019599914958234876. Accuracy: 22\n",
      "Iteration: 590. Loss: 0.0031127166002988815. Accuracy: 22\n",
      "Iteration: 591. Loss: 0.0006524657947011292. Accuracy: 22\n",
      "Iteration: 592. Loss: 0.0001303100580116734. Accuracy: 22\n",
      "Iteration: 593. Loss: 0.0018803406273946166. Accuracy: 26\n",
      "Iteration: 594. Loss: 0.0004800414899364114. Accuracy: 22\n",
      "Iteration: 595. Loss: 0.00030494690872728825. Accuracy: 22\n",
      "Iteration: 596. Loss: 0.0004912567092105746. Accuracy: 22\n",
      "Iteration: 597. Loss: 0.0006118774181231856. Accuracy: 24\n",
      "Iteration: 598. Loss: 0.00011276244913460687. Accuracy: 20\n",
      "Iteration: 599. Loss: 0.00023124694416765124. Accuracy: 24\n",
      "Iteration: 600. Loss: 0.0001389312674291432. Accuracy: 22\n",
      "Iteration: 601. Loss: 0.00022331238142214715. Accuracy: 22\n",
      "Iteration: 602. Loss: 0.00012260436778888106. Accuracy: 22\n",
      "Iteration: 603. Loss: 0.0001617431698832661. Accuracy: 24\n",
      "Iteration: 604. Loss: 9.902953752316535e-05. Accuracy: 24\n",
      "Iteration: 605. Loss: 0.0002829742443282157. Accuracy: 24\n",
      "Iteration: 606. Loss: 0.0003635406610555947. Accuracy: 22\n",
      "Iteration: 607. Loss: 0.0005001830868422985. Accuracy: 22\n",
      "Iteration: 608. Loss: 0.0002902221749536693. Accuracy: 22\n",
      "Iteration: 609. Loss: 0.0007221222040243447. Accuracy: 22\n",
      "Iteration: 610. Loss: 0.00022705078299622983. Accuracy: 20\n",
      "Iteration: 611. Loss: 9.544372733216733e-05. Accuracy: 22\n",
      "Iteration: 612. Loss: 0.0005767822149209678. Accuracy: 24\n",
      "Iteration: 613. Loss: 0.0002449035528115928. Accuracy: 24\n",
      "Iteration: 614. Loss: 0.0002501678536646068. Accuracy: 22\n",
      "Iteration: 615. Loss: 0.00029922486282885075. Accuracy: 22\n",
      "Iteration: 616. Loss: 0.00023765563673805445. Accuracy: 22\n",
      "Iteration: 617. Loss: 0.00031295776716433465. Accuracy: 24\n",
      "Iteration: 618. Loss: 0.0003299713134765625. Accuracy: 22\n",
      "Iteration: 619. Loss: 0.00022674560023006052. Accuracy: 22\n",
      "Iteration: 620. Loss: 0.0021804047282785177. Accuracy: 22\n",
      "Iteration: 621. Loss: 0.00024368286540266126. Accuracy: 24\n",
      "Iteration: 622. Loss: 0.00024559019948355854. Accuracy: 26\n",
      "Iteration: 623. Loss: 0.00019462585623841733. Accuracy: 24\n",
      "Iteration: 624. Loss: 0.005688476376235485. Accuracy: 24\n",
      "Iteration: 625. Loss: 0.00035263062454760075. Accuracy: 22\n",
      "Iteration: 626. Loss: 0.00011169433855684474. Accuracy: 24\n",
      "Iteration: 627. Loss: 0.00018707275739870965. Accuracy: 24\n",
      "Iteration: 628. Loss: 0.0001341247552772984. Accuracy: 24\n",
      "Iteration: 629. Loss: 0.0003506469656713307. Accuracy: 24\n",
      "Iteration: 630. Loss: 0.00015174865256994963. Accuracy: 26\n",
      "Iteration: 631. Loss: 0.00024429321638308465. Accuracy: 20\n",
      "Iteration: 632. Loss: 0.0002256774896522984. Accuracy: 22\n",
      "Iteration: 633. Loss: 0.00017860412481240928. Accuracy: 24\n",
      "Iteration: 634. Loss: 0.0001557922369102016. Accuracy: 18\n",
      "Iteration: 635. Loss: 0.0002826690615620464. Accuracy: 22\n",
      "Iteration: 636. Loss: 0.00019470215192995965. Accuracy: 22\n",
      "Iteration: 637. Loss: 0.00034729004255495965. Accuracy: 24\n",
      "Iteration: 638. Loss: 0.0001720428408589214. Accuracy: 22\n",
      "Iteration: 639. Loss: 0.00023925781715661287. Accuracy: 26\n",
      "Iteration: 640. Loss: 0.00035881041549146175. Accuracy: 26\n",
      "Iteration: 641. Loss: 0.00021797179942950606. Accuracy: 22\n",
      "Iteration: 642. Loss: 0.00017021178791765124. Accuracy: 24\n",
      "Iteration: 643. Loss: 0.00033424378489144146. Accuracy: 22\n",
      "Iteration: 644. Loss: 0.00026184081798419356. Accuracy: 22\n",
      "Iteration: 645. Loss: 0.00010383605695096776. Accuracy: 24\n",
      "Iteration: 646. Loss: 4.318237188272178e-05. Accuracy: 22\n",
      "Iteration: 647. Loss: 0.0003102874616160989. Accuracy: 26\n",
      "Iteration: 648. Loss: 0.0002165222103940323. Accuracy: 22\n",
      "Iteration: 649. Loss: 0.00020812988805118948. Accuracy: 28\n",
      "Iteration: 650. Loss: 0.00038856506580486894. Accuracy: 28\n",
      "Iteration: 651. Loss: 0.0001682281435932964. Accuracy: 22\n",
      "Iteration: 652. Loss: 0.0002464294375386089. Accuracy: 24\n",
      "Iteration: 653. Loss: 0.0001398468011757359. Accuracy: 24\n",
      "Iteration: 654. Loss: 0.00040649413131177425. Accuracy: 26\n",
      "Iteration: 655. Loss: 0.000547866802662611. Accuracy: 26\n",
      "Iteration: 656. Loss: 6.530762038892135e-05. Accuracy: 22\n",
      "Iteration: 657. Loss: 0.0002753448497969657. Accuracy: 20\n",
      "Iteration: 658. Loss: 0.00018394470680505037. Accuracy: 24\n",
      "Iteration: 659. Loss: 0.0002478790411259979. Accuracy: 26\n",
      "Iteration: 660. Loss: 0.00011276244913460687. Accuracy: 22\n",
      "Iteration: 661. Loss: 0.00012184143270133063. Accuracy: 22\n",
      "Iteration: 662. Loss: 0.00024330138694494963. Accuracy: 22\n",
      "Iteration: 663. Loss: 0.00036216736771166325. Accuracy: 22\n",
      "Iteration: 664. Loss: 0.00039123534224927425. Accuracy: 22\n",
      "Iteration: 665. Loss: 0.00012199401680845767. Accuracy: 26\n",
      "Iteration: 666. Loss: 0.00011680603347485885. Accuracy: 26\n",
      "Iteration: 667. Loss: 0.00011863708641612902. Accuracy: 22\n",
      "Iteration: 668. Loss: 8.430481102550402e-05. Accuracy: 20\n",
      "Iteration: 669. Loss: 0.00032897948403842747. Accuracy: 24\n",
      "Iteration: 670. Loss: 9.902953752316535e-05. Accuracy: 24\n",
      "Iteration: 671. Loss: 0.00022323608573060483. Accuracy: 20\n",
      "Iteration: 672. Loss: 0.00020484924607444555. Accuracy: 20\n",
      "Iteration: 673. Loss: 0.000710983294993639. Accuracy: 24\n",
      "Iteration: 674. Loss: 0.00018226623069494963. Accuracy: 24\n",
      "Iteration: 675. Loss: 0.00028015137650072575. Accuracy: 22\n",
      "Iteration: 676. Loss: 0.00011741638445528224. Accuracy: 24\n",
      "Iteration: 677. Loss: 0.0001974487240659073. Accuracy: 24\n",
      "Iteration: 678. Loss: 0.0008152007940225303. Accuracy: 26\n",
      "Iteration: 679. Loss: 0.0001478576596127823. Accuracy: 24\n",
      "Iteration: 680. Loss: 0.00014328003453556448. Accuracy: 20\n",
      "Iteration: 681. Loss: 5.5541993788210675e-05. Accuracy: 24\n",
      "Iteration: 682. Loss: 0.00022438049199990928. Accuracy: 22\n",
      "Iteration: 683. Loss: 0.00017402648518327624. Accuracy: 18\n",
      "Iteration: 684. Loss: 0.0013474273728206754. Accuracy: 26\n",
      "Iteration: 685. Loss: 0.0001586914004292339. Accuracy: 22\n",
      "Iteration: 686. Loss: 0.0004538726934697479. Accuracy: 20\n",
      "Iteration: 687. Loss: 0.0003719329833984375. Accuracy: 26\n",
      "Iteration: 688. Loss: 0.00022743224690202624. Accuracy: 22\n",
      "Iteration: 689. Loss: 0.0001672363287070766. Accuracy: 22\n",
      "Iteration: 690. Loss: 0.00013244629371911287. Accuracy: 24\n",
      "Iteration: 691. Loss: 0.0004841613699682057. Accuracy: 26\n",
      "Iteration: 692. Loss: 0.00018333435582462698. Accuracy: 22\n",
      "Iteration: 693. Loss: 0.0010387420188635588. Accuracy: 22\n",
      "Iteration: 694. Loss: 0.0004183197161182761. Accuracy: 24\n",
      "Iteration: 695. Loss: 0.0001738739083521068. Accuracy: 26\n",
      "Iteration: 696. Loss: 0.00021720885706599802. Accuracy: 26\n",
      "Iteration: 697. Loss: 0.00030082702869549394. Accuracy: 24\n",
      "Iteration: 698. Loss: 0.00030731200240552425. Accuracy: 18\n",
      "Iteration: 699. Loss: 0.0004970550653524697. Accuracy: 24\n",
      "Iteration: 700. Loss: 0.00020912170293740928. Accuracy: 24\n",
      "Iteration: 701. Loss: 0.00023666382185183465. Accuracy: 24\n",
      "Iteration: 702. Loss: 0.0002467346203047782. Accuracy: 28\n",
      "Iteration: 703. Loss: 0.0008961486746557057. Accuracy: 24\n",
      "Iteration: 704. Loss: 0.00022308349434752017. Accuracy: 24\n",
      "Iteration: 705. Loss: 0.0007559204241260886. Accuracy: 22\n",
      "Iteration: 706. Loss: 0.0001865387021098286. Accuracy: 24\n",
      "Iteration: 707. Loss: 0.0002639770391397178. Accuracy: 22\n",
      "Iteration: 708. Loss: 0.00048393249744549394. Accuracy: 26\n",
      "Iteration: 709. Loss: 0.00017982482677325606. Accuracy: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 710. Loss: 0.0002031707699643448. Accuracy: 24\n",
      "Iteration: 711. Loss: 0.00018867492326535285. Accuracy: 24\n",
      "Iteration: 712. Loss: 8.972168143372983e-05. Accuracy: 20\n",
      "Iteration: 713. Loss: 0.00019065856758970767. Accuracy: 24\n",
      "Iteration: 714. Loss: 0.0001446533133275807. Accuracy: 22\n",
      "Iteration: 715. Loss: 0.00038574219797737896. Accuracy: 24\n",
      "Iteration: 716. Loss: 0.00011703491327352822. Accuracy: 24\n",
      "Iteration: 717. Loss: 0.00016967773262877017. Accuracy: 26\n",
      "Iteration: 718. Loss: 0.0001634216314414516. Accuracy: 24\n",
      "Iteration: 719. Loss: 0.00020019531075377017. Accuracy: 24\n",
      "Iteration: 720. Loss: 0.0002689361572265625. Accuracy: 20\n",
      "Iteration: 721. Loss: 0.00023597717517986894. Accuracy: 24\n",
      "Iteration: 722. Loss: 0.0002254486025776714. Accuracy: 22\n",
      "Iteration: 723. Loss: 0.0002800750662572682. Accuracy: 22\n",
      "Iteration: 724. Loss: 0.003084030235186219. Accuracy: 24\n",
      "Iteration: 725. Loss: 0.00030952453380450606. Accuracy: 22\n",
      "Iteration: 726. Loss: 0.00030303956009447575. Accuracy: 22\n",
      "Iteration: 727. Loss: 0.0006817626999691129. Accuracy: 24\n",
      "Iteration: 728. Loss: 0.00044685363536700606. Accuracy: 22\n",
      "Iteration: 729. Loss: 0.00017883301188703626. Accuracy: 18\n",
      "Iteration: 730. Loss: 0.00017753601423464715. Accuracy: 22\n",
      "Iteration: 731. Loss: 0.0009310913155786693. Accuracy: 24\n",
      "Iteration: 732. Loss: 0.0001879119809018448. Accuracy: 24\n",
      "Iteration: 733. Loss: 0.00021034240489825606. Accuracy: 26\n",
      "Iteration: 734. Loss: 0.00023719787714071572. Accuracy: 26\n",
      "Iteration: 735. Loss: 0.0001417541498085484. Accuracy: 20\n",
      "Iteration: 736. Loss: 0.00010215759539278224. Accuracy: 24\n",
      "Iteration: 737. Loss: 0.00039817809010855854. Accuracy: 22\n",
      "Iteration: 738. Loss: 7.82012939453125e-05. Accuracy: 22\n",
      "Iteration: 739. Loss: 0.00015426635218318552. Accuracy: 24\n",
      "Iteration: 740. Loss: 0.0007544708205386996. Accuracy: 24\n",
      "Iteration: 741. Loss: 0.00019477844762150198. Accuracy: 22\n",
      "Iteration: 742. Loss: 0.0002120208810083568. Accuracy: 24\n",
      "Iteration: 743. Loss: 0.00013526916154660285. Accuracy: 22\n",
      "Iteration: 744. Loss: 0.00017349242989439517. Accuracy: 24\n",
      "Iteration: 745. Loss: 0.0004016113234683871. Accuracy: 24\n",
      "Iteration: 746. Loss: 0.0005812072777189314. Accuracy: 24\n",
      "Iteration: 747. Loss: 0.00019218445231672376. Accuracy: 24\n",
      "Iteration: 748. Loss: 0.00010978698992403224. Accuracy: 24\n",
      "Iteration: 749. Loss: 0.0001631164486752823. Accuracy: 18\n",
      "Iteration: 750. Loss: 0.00017974853108171374. Accuracy: 24\n",
      "Iteration: 751. Loss: 0.0006325531285256147. Accuracy: 20\n",
      "Iteration: 752. Loss: 0.00014610290236305445. Accuracy: 22\n",
      "Iteration: 753. Loss: 0.0007675933884456754. Accuracy: 20\n",
      "Iteration: 754. Loss: 0.0001506805419921875. Accuracy: 22\n",
      "Iteration: 755. Loss: 0.0003746795700863004. Accuracy: 26\n",
      "Iteration: 756. Loss: 0.0001824951177695766. Accuracy: 24\n",
      "Iteration: 757. Loss: 0.00022270203044172376. Accuracy: 26\n",
      "Iteration: 758. Loss: 0.0004218292306177318. Accuracy: 22\n",
      "Iteration: 759. Loss: 0.0001637267996557057. Accuracy: 20\n",
      "Iteration: 760. Loss: 0.00012290955055505037. Accuracy: 20\n",
      "Iteration: 761. Loss: 0.0004866790841333568. Accuracy: 22\n",
      "Iteration: 762. Loss: 0.0001678466796875. Accuracy: 24\n",
      "Iteration: 763. Loss: 0.0010715484386309981. Accuracy: 24\n",
      "Iteration: 764. Loss: 0.00020149230840615928. Accuracy: 22\n",
      "Iteration: 765. Loss: 0.00032859802013263106. Accuracy: 24\n",
      "Iteration: 766. Loss: 0.00021064758766442537. Accuracy: 22\n",
      "Iteration: 767. Loss: 0.00017112732166424394. Accuracy: 26\n",
      "Iteration: 768. Loss: 0.00029922486282885075. Accuracy: 22\n",
      "Iteration: 769. Loss: 0.0004008483956567943. Accuracy: 24\n",
      "Iteration: 770. Loss: 8.857726788846776e-05. Accuracy: 22\n",
      "Iteration: 771. Loss: 0.0005174255347810686. Accuracy: 22\n",
      "Iteration: 772. Loss: 0.00020774840959347785. Accuracy: 24\n",
      "Iteration: 773. Loss: 0.00025650023599155247. Accuracy: 24\n",
      "Iteration: 774. Loss: 0.00021865844610147178. Accuracy: 22\n",
      "Iteration: 775. Loss: 0.00025001526228152215. Accuracy: 24\n",
      "Iteration: 776. Loss: 0.00024620056501589715. Accuracy: 22\n",
      "Iteration: 777. Loss: 9.880065772449598e-05. Accuracy: 22\n",
      "Iteration: 778. Loss: 0.0002890777541324496. Accuracy: 24\n",
      "Iteration: 779. Loss: 0.00038963317638263106. Accuracy: 22\n",
      "Iteration: 780. Loss: 0.0003548431268427521. Accuracy: 24\n",
      "Iteration: 781. Loss: 0.0001682281435932964. Accuracy: 22\n",
      "Iteration: 782. Loss: 0.00017631531227380037. Accuracy: 24\n",
      "Iteration: 783. Loss: 0.0003002166631631553. Accuracy: 24\n",
      "Iteration: 784. Loss: 0.00012275695917196572. Accuracy: 24\n",
      "Iteration: 785. Loss: 0.0003720092645380646. Accuracy: 24\n",
      "Iteration: 786. Loss: 0.0004151153552811593. Accuracy: 24\n",
      "Iteration: 787. Loss: 0.0001528930733911693. Accuracy: 24\n",
      "Iteration: 788. Loss: 0.0002769470156636089. Accuracy: 22\n",
      "Iteration: 789. Loss: 0.0001319122384302318. Accuracy: 22\n",
      "Iteration: 790. Loss: 0.0006322479457594454. Accuracy: 22\n",
      "Iteration: 791. Loss: 0.0001497650082455948. Accuracy: 24\n",
      "Iteration: 792. Loss: 0.0001795959542505443. Accuracy: 24\n",
      "Iteration: 793. Loss: 0.0003079223679378629. Accuracy: 26\n",
      "Iteration: 794. Loss: 0.0001732635428197682. Accuracy: 18\n",
      "Iteration: 795. Loss: 0.00027137756114825606. Accuracy: 24\n",
      "Iteration: 796. Loss: 0.00031646728166379035. Accuracy: 20\n",
      "Iteration: 797. Loss: 0.00011482238915050402. Accuracy: 26\n",
      "Iteration: 798. Loss: 0.00035003662924282253. Accuracy: 22\n",
      "Iteration: 799. Loss: 0.0001669311459409073. Accuracy: 24\n",
      "Iteration: 800. Loss: 0.00010063171066576615. Accuracy: 20\n",
      "Iteration: 801. Loss: 0.0006973266717977822. Accuracy: 24\n",
      "Iteration: 802. Loss: 0.0001744842593325302. Accuracy: 22\n",
      "Iteration: 803. Loss: 0.00046058653970248997. Accuracy: 22\n",
      "Iteration: 804. Loss: 0.0005810547154396772. Accuracy: 22\n",
      "Iteration: 805. Loss: 0.0001373291015625. Accuracy: 24\n",
      "Iteration: 806. Loss: 0.00016601562674622983. Accuracy: 22\n",
      "Iteration: 807. Loss: 0.0005129242199473083. Accuracy: 24\n",
      "Iteration: 808. Loss: 0.00024703980307094753. Accuracy: 22\n",
      "Iteration: 809. Loss: 0.000738601665943861. Accuracy: 24\n",
      "Iteration: 810. Loss: 0.0001236724783666432. Accuracy: 22\n",
      "Iteration: 811. Loss: 0.00021369934256654233. Accuracy: 22\n",
      "Iteration: 812. Loss: 0.0005920410039834678. Accuracy: 22\n",
      "Iteration: 813. Loss: 0.0002037811209447682. Accuracy: 18\n",
      "Iteration: 814. Loss: 0.0002771759172901511. Accuracy: 20\n",
      "Iteration: 815. Loss: 0.0003598785260692239. Accuracy: 22\n",
      "Iteration: 816. Loss: 0.0005716705345548689. Accuracy: 26\n",
      "Iteration: 817. Loss: 0.0005420685047283769. Accuracy: 20\n",
      "Iteration: 818. Loss: 0.001486816443502903. Accuracy: 24\n",
      "Iteration: 819. Loss: 0.00019813537073787302. Accuracy: 24\n",
      "Iteration: 820. Loss: 7.08770749042742e-05. Accuracy: 18\n",
      "Iteration: 821. Loss: 0.00045494080404751003. Accuracy: 20\n",
      "Iteration: 822. Loss: 0.0002265930233988911. Accuracy: 24\n",
      "Iteration: 823. Loss: 0.0005158233689144254. Accuracy: 22\n",
      "Iteration: 824. Loss: 0.00026145935407839715. Accuracy: 22\n",
      "Iteration: 825. Loss: 0.0002473449567332864. Accuracy: 20\n",
      "Iteration: 826. Loss: 0.00013473510625772178. Accuracy: 24\n",
      "Iteration: 827. Loss: 0.00015441894356627017. Accuracy: 22\n",
      "Iteration: 828. Loss: 0.0001551055902382359. Accuracy: 24\n",
      "Iteration: 829. Loss: 0.0002542877336964011. Accuracy: 22\n",
      "Iteration: 830. Loss: 0.00028976440080441535. Accuracy: 22\n",
      "Iteration: 831. Loss: 0.00040000915760174394. Accuracy: 22\n",
      "Iteration: 832. Loss: 0.00013854980352334678. Accuracy: 20\n",
      "Iteration: 833. Loss: 0.0002453613269608468. Accuracy: 22\n",
      "Iteration: 834. Loss: 0.00016761779261287302. Accuracy: 24\n",
      "Iteration: 835. Loss: 0.0001303100580116734. Accuracy: 26\n",
      "Iteration: 836. Loss: 0.0002323150692973286. Accuracy: 22\n",
      "Iteration: 837. Loss: 0.00018302917305845767. Accuracy: 22\n",
      "Iteration: 838. Loss: 0.00021095275587867945. Accuracy: 20\n",
      "Iteration: 839. Loss: 0.00029998779064044356. Accuracy: 26\n",
      "Iteration: 840. Loss: 0.0004199981631245464. Accuracy: 24\n",
      "Iteration: 841. Loss: 0.0001468658447265625. Accuracy: 22\n",
      "Iteration: 842. Loss: 0.0003243255487177521. Accuracy: 22\n",
      "Iteration: 843. Loss: 0.00015632629219908267. Accuracy: 22\n",
      "Iteration: 844. Loss: 0.0002177429269067943. Accuracy: 24\n",
      "Iteration: 845. Loss: 0.0001821136538637802. Accuracy: 24\n",
      "Iteration: 846. Loss: 0.0001345062191830948. Accuracy: 24\n",
      "Iteration: 847. Loss: 0.00011428832658566535. Accuracy: 24\n",
      "Iteration: 848. Loss: 0.00015182494826149195. Accuracy: 20\n",
      "Iteration: 849. Loss: 0.00019180297385901213. Accuracy: 26\n",
      "Iteration: 850. Loss: 0.00026153563521802425. Accuracy: 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 851. Loss: 9.361267439089715e-05. Accuracy: 22\n",
      "Iteration: 852. Loss: 8.384704415220767e-05. Accuracy: 22\n",
      "Iteration: 853. Loss: 0.00015853882359806448. Accuracy: 22\n",
      "Iteration: 854. Loss: 0.0001471710274927318. Accuracy: 22\n",
      "Iteration: 855. Loss: 9.834289812715724e-05. Accuracy: 22\n",
      "Iteration: 856. Loss: 0.00010917663894360885. Accuracy: 18\n",
      "Iteration: 857. Loss: 0.00013336181291379035. Accuracy: 24\n",
      "Iteration: 858. Loss: 0.0002873229968827218. Accuracy: 22\n",
      "Iteration: 859. Loss: 0.00022552489826921374. Accuracy: 24\n",
      "Iteration: 860. Loss: 0.00014068603923078626. Accuracy: 24\n",
      "Iteration: 861. Loss: 0.00015617370081599802. Accuracy: 24\n",
      "Iteration: 862. Loss: 0.00010589599696686491. Accuracy: 24\n",
      "Iteration: 863. Loss: 0.0001439666812075302. Accuracy: 24\n",
      "Iteration: 864. Loss: 0.0001741790765663609. Accuracy: 22\n",
      "Iteration: 865. Loss: 0.00014617919805459678. Accuracy: 24\n",
      "Iteration: 866. Loss: 0.00019058227189816535. Accuracy: 24\n",
      "Iteration: 867. Loss: 0.0001846313534770161. Accuracy: 24\n",
      "Iteration: 868. Loss: 0.00042327880510129035. Accuracy: 22\n",
      "Iteration: 869. Loss: 0.00021690368885174394. Accuracy: 22\n",
      "Iteration: 870. Loss: 0.00020668029901571572. Accuracy: 24\n",
      "Iteration: 871. Loss: 0.0003940582391805947. Accuracy: 18\n",
      "Iteration: 872. Loss: 0.00016136169142555445. Accuracy: 22\n",
      "Iteration: 873. Loss: 7.759094296488911e-05. Accuracy: 24\n",
      "Iteration: 874. Loss: 0.00015518188592977822. Accuracy: 24\n",
      "Iteration: 875. Loss: 0.00045417784713208675. Accuracy: 24\n",
      "Iteration: 876. Loss: 0.0001472473086323589. Accuracy: 24\n",
      "Iteration: 877. Loss: 0.0001443481451133266. Accuracy: 22\n",
      "Iteration: 878. Loss: 0.00020149230840615928. Accuracy: 26\n",
      "Iteration: 879. Loss: 0.00023185729514807463. Accuracy: 22\n",
      "Iteration: 880. Loss: 6.355285586323589e-05. Accuracy: 26\n",
      "Iteration: 881. Loss: 0.0002466583391651511. Accuracy: 26\n",
      "Iteration: 882. Loss: 0.0003952026308979839. Accuracy: 24\n",
      "Iteration: 883. Loss: 0.00024345397832803428. Accuracy: 24\n",
      "Iteration: 884. Loss: 0.00013740539725404233. Accuracy: 24\n",
      "Iteration: 885. Loss: 7.46917721698992e-05. Accuracy: 22\n",
      "Iteration: 886. Loss: 0.0002632903924677521. Accuracy: 24\n",
      "Iteration: 887. Loss: 0.00012390136544127017. Accuracy: 20\n",
      "Iteration: 888. Loss: 0.00013206481526140124. Accuracy: 24\n",
      "Iteration: 889. Loss: 0.0002418518124613911. Accuracy: 24\n",
      "Iteration: 890. Loss: 8.872985927155241e-05. Accuracy: 20\n",
      "Iteration: 891. Loss: 0.0004994201590307057. Accuracy: 20\n",
      "Iteration: 892. Loss: 0.00012123108172090724. Accuracy: 24\n",
      "Iteration: 893. Loss: 9.651183790992945e-05. Accuracy: 22\n",
      "Iteration: 894. Loss: 0.0001325225894106552. Accuracy: 18\n",
      "Iteration: 895. Loss: 8.979797712527215e-05. Accuracy: 22\n",
      "Iteration: 896. Loss: 0.0001691436773398891. Accuracy: 24\n",
      "Iteration: 897. Loss: 0.0001624298165552318. Accuracy: 24\n",
      "Iteration: 898. Loss: 0.0002709197869990021. Accuracy: 22\n",
      "Iteration: 899. Loss: 0.0001964569091796875. Accuracy: 20\n",
      "Iteration: 900. Loss: 0.0003344726574141532. Accuracy: 24\n",
      "Iteration: 901. Loss: 0.0002734375011641532. Accuracy: 24\n",
      "Iteration: 902. Loss: 0.0002619171282276511. Accuracy: 18\n",
      "Iteration: 903. Loss: 0.00014221190940588713. Accuracy: 20\n",
      "Iteration: 904. Loss: 0.00022499084298033267. Accuracy: 24\n",
      "Iteration: 905. Loss: 0.00020301819313317537. Accuracy: 24\n",
      "Iteration: 906. Loss: 0.00023391723516397178. Accuracy: 24\n",
      "Iteration: 907. Loss: 0.0001255798269994557. Accuracy: 22\n",
      "Iteration: 908. Loss: 0.0005916595691815019. Accuracy: 20\n",
      "Iteration: 909. Loss: 0.0003024291945621371. Accuracy: 22\n",
      "Iteration: 910. Loss: 0.00012481689918786287. Accuracy: 24\n",
      "Iteration: 911. Loss: 0.0006527709774672985. Accuracy: 22\n",
      "Iteration: 912. Loss: 9.010315261548385e-05. Accuracy: 18\n",
      "Iteration: 913. Loss: 0.00024078368733171374. Accuracy: 22\n",
      "Iteration: 914. Loss: 0.00020019531075377017. Accuracy: 24\n",
      "Iteration: 915. Loss: 7.05718994140625e-05. Accuracy: 22\n",
      "Iteration: 916. Loss: 0.00020713805861305445. Accuracy: 22\n",
      "Iteration: 917. Loss: 0.0001531982416054234. Accuracy: 26\n",
      "Iteration: 918. Loss: 0.00016540527576580644. Accuracy: 24\n",
      "Iteration: 919. Loss: 0.00027030945057049394. Accuracy: 16\n",
      "Iteration: 920. Loss: 7.804870256222785e-05. Accuracy: 26\n",
      "Iteration: 921. Loss: 0.00010643005225574598. Accuracy: 22\n",
      "Iteration: 922. Loss: 0.00010665893205441535. Accuracy: 22\n",
      "Iteration: 923. Loss: 0.00020706176292151213. Accuracy: 22\n",
      "Iteration: 924. Loss: 0.0001261901925317943. Accuracy: 22\n",
      "Iteration: 925. Loss: 0.00019752501975744963. Accuracy: 24\n",
      "Iteration: 926. Loss: 0.00018745422130450606. Accuracy: 22\n",
      "Iteration: 927. Loss: 0.00010345459304517135. Accuracy: 26\n",
      "Iteration: 928. Loss: 6.225585821084678e-05. Accuracy: 24\n",
      "Iteration: 929. Loss: 0.00019599914958234876. Accuracy: 24\n",
      "Iteration: 930. Loss: 0.0002330779971089214. Accuracy: 24\n",
      "Iteration: 931. Loss: 6.70623776386492e-05. Accuracy: 20\n",
      "Iteration: 932. Loss: 0.0002566528273746371. Accuracy: 24\n",
      "Iteration: 933. Loss: 0.0003147125244140625. Accuracy: 26\n",
      "Iteration: 934. Loss: 8.445739513263106e-05. Accuracy: 24\n",
      "Iteration: 935. Loss: 0.00020927429432049394. Accuracy: 24\n",
      "Iteration: 936. Loss: 0.00018241882207803428. Accuracy: 20\n",
      "Iteration: 937. Loss: 0.0003055572451557964. Accuracy: 26\n",
      "Iteration: 938. Loss: 0.0005787658737972379. Accuracy: 24\n",
      "Iteration: 939. Loss: 0.00036064148298464715. Accuracy: 24\n",
      "Iteration: 940. Loss: 0.00020286560175009072. Accuracy: 22\n",
      "Iteration: 941. Loss: 0.00025657654623501003. Accuracy: 24\n",
      "Iteration: 942. Loss: 0.00020591735665220767. Accuracy: 24\n",
      "Iteration: 943. Loss: 6.629943527514115e-05. Accuracy: 22\n",
      "Iteration: 944. Loss: 0.00019638061348814517. Accuracy: 24\n",
      "Iteration: 945. Loss: 0.0004752349923364818. Accuracy: 24\n",
      "Iteration: 946. Loss: 7.904052472440526e-05. Accuracy: 20\n",
      "Iteration: 947. Loss: 0.0001884460507426411. Accuracy: 22\n",
      "Iteration: 948. Loss: 0.00022430419630836695. Accuracy: 26\n",
      "Iteration: 949. Loss: 0.00012062073074048385. Accuracy: 22\n",
      "Iteration: 950. Loss: 0.00019401550525799394. Accuracy: 22\n",
      "Iteration: 951. Loss: 0.00010337829735362902. Accuracy: 22\n",
      "Iteration: 952. Loss: 0.00013473510625772178. Accuracy: 24\n",
      "Iteration: 953. Loss: 0.0009469604701735079. Accuracy: 18\n",
      "Iteration: 954. Loss: 0.00017936706717591733. Accuracy: 22\n",
      "Iteration: 955. Loss: 0.0006875610561110079. Accuracy: 26\n",
      "Iteration: 956. Loss: 0.00012184143270133063. Accuracy: 24\n",
      "Iteration: 957. Loss: 0.00019195556524209678. Accuracy: 24\n",
      "Iteration: 958. Loss: 0.00015762328985147178. Accuracy: 22\n",
      "Iteration: 959. Loss: 0.0002826690615620464. Accuracy: 22\n",
      "Iteration: 960. Loss: 0.00016700744163244963. Accuracy: 24\n",
      "Iteration: 961. Loss: 0.0003593444707803428. Accuracy: 24\n",
      "Iteration: 962. Loss: 0.00026947021251544356. Accuracy: 22\n",
      "Iteration: 963. Loss: 0.00010620117245707661. Accuracy: 24\n",
      "Iteration: 964. Loss: 0.00014572143845725805. Accuracy: 24\n",
      "Iteration: 965. Loss: 0.00017349242989439517. Accuracy: 22\n",
      "Iteration: 966. Loss: 0.00013908385881222785. Accuracy: 22\n",
      "Iteration: 967. Loss: 0.00023124694416765124. Accuracy: 26\n",
      "Iteration: 968. Loss: 0.00015090942906681448. Accuracy: 22\n",
      "Iteration: 969. Loss: 0.00015144348435569555. Accuracy: 20\n",
      "Iteration: 970. Loss: 0.00011047362932004035. Accuracy: 26\n",
      "Iteration: 971. Loss: 9.826660243561491e-05. Accuracy: 22\n",
      "Iteration: 972. Loss: 0.00048240661271847785. Accuracy: 24\n",
      "Iteration: 973. Loss: 0.00010932922305073589. Accuracy: 20\n",
      "Iteration: 974. Loss: 0.00021339415980037302. Accuracy: 24\n",
      "Iteration: 975. Loss: 0.0001707458432065323. Accuracy: 26\n",
      "Iteration: 976. Loss: 0.0001954650942934677. Accuracy: 26\n",
      "Iteration: 977. Loss: 0.00019142150995321572. Accuracy: 26\n",
      "Iteration: 978. Loss: 0.00012382506974972785. Accuracy: 24\n",
      "Iteration: 979. Loss: 0.00015556334983557463. Accuracy: 26\n",
      "Iteration: 980. Loss: 0.00012184143270133063. Accuracy: 24\n",
      "Iteration: 981. Loss: 0.0001853942812886089. Accuracy: 22\n",
      "Iteration: 982. Loss: 0.00010833740088855848. Accuracy: 22\n",
      "Iteration: 983. Loss: 0.0001617431698832661. Accuracy: 22\n",
      "Iteration: 984. Loss: 7.575989002361894e-05. Accuracy: 22\n",
      "Iteration: 985. Loss: 0.00016487122047692537. Accuracy: 18\n",
      "Iteration: 986. Loss: 0.00014945984003134072. Accuracy: 20\n",
      "Iteration: 987. Loss: 0.0002227783261332661. Accuracy: 24\n",
      "Iteration: 988. Loss: 0.0001223754952661693. Accuracy: 26\n",
      "Iteration: 989. Loss: 0.0001261901925317943. Accuracy: 22\n",
      "Iteration: 990. Loss: 0.00015647888358216733. Accuracy: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 991. Loss: 0.00011314392031636089. Accuracy: 22\n",
      "Iteration: 992. Loss: 0.00014167785411700606. Accuracy: 26\n",
      "Iteration: 993. Loss: 9.254455653717741e-05. Accuracy: 18\n",
      "Iteration: 994. Loss: 0.00024436949752271175. Accuracy: 24\n",
      "Iteration: 995. Loss: 0.0005015563801862299. Accuracy: 24\n",
      "Iteration: 996. Loss: 0.0001805877691367641. Accuracy: 24\n",
      "Iteration: 997. Loss: 0.00027565003256313503. Accuracy: 22\n",
      "Iteration: 998. Loss: 0.0006776428199373186. Accuracy: 22\n",
      "Iteration: 999. Loss: 7.019042823230848e-05. Accuracy: 20\n",
      "Iteration: 1000. Loss: 0.00018356322834733874. Accuracy: 18\n",
      "Iteration: 1001. Loss: 0.00016128539573401213. Accuracy: 24\n",
      "Iteration: 1002. Loss: 0.0004965210100635886. Accuracy: 20\n",
      "Iteration: 1003. Loss: 0.00020423889509402215. Accuracy: 22\n",
      "Iteration: 1004. Loss: 0.00010070800635730848. Accuracy: 22\n",
      "Iteration: 1005. Loss: 9.368896280648187e-05. Accuracy: 24\n",
      "Iteration: 1006. Loss: 0.00018829345935955644. Accuracy: 24\n",
      "Iteration: 1007. Loss: 0.00019844055350404233. Accuracy: 24\n",
      "Iteration: 1008. Loss: 0.00012229919957462698. Accuracy: 20\n",
      "Iteration: 1009. Loss: 0.00013214111095294356. Accuracy: 24\n",
      "Iteration: 1010. Loss: 0.00026908874860964715. Accuracy: 26\n",
      "Iteration: 1011. Loss: 0.00017051697068382055. Accuracy: 20\n",
      "Iteration: 1012. Loss: 0.00017494201892986894. Accuracy: 28\n",
      "Iteration: 1013. Loss: 7.179260137490928e-05. Accuracy: 24\n",
      "Iteration: 1014. Loss: 0.0003036498965229839. Accuracy: 22\n",
      "Iteration: 1015. Loss: 0.0004538726934697479. Accuracy: 24\n",
      "Iteration: 1016. Loss: 5.2566527301678434e-05. Accuracy: 24\n",
      "Iteration: 1017. Loss: 8.918761886889115e-05. Accuracy: 22\n",
      "Iteration: 1018. Loss: 0.00016525268438272178. Accuracy: 24\n",
      "Iteration: 1019. Loss: 7.415771688101813e-05. Accuracy: 26\n",
      "Iteration: 1020. Loss: 0.0004580688546411693. Accuracy: 22\n",
      "Iteration: 1021. Loss: 0.0001147460934589617. Accuracy: 20\n",
      "Iteration: 1022. Loss: 0.0001815795840229839. Accuracy: 20\n",
      "Iteration: 1023. Loss: 0.0005870056338608265. Accuracy: 24\n",
      "Iteration: 1024. Loss: 0.0001281738223042339. Accuracy: 26\n",
      "Iteration: 1025. Loss: 0.00027221679920330644. Accuracy: 20\n",
      "Iteration: 1026. Loss: 0.00010810852108988911. Accuracy: 22\n",
      "Iteration: 1027. Loss: 0.00020591735665220767. Accuracy: 20\n",
      "Iteration: 1028. Loss: 0.00010887146345339715. Accuracy: 26\n",
      "Iteration: 1029. Loss: 8.7738037109375e-05. Accuracy: 24\n",
      "Iteration: 1030. Loss: 0.00010047912655863911. Accuracy: 22\n",
      "Iteration: 1031. Loss: 0.0002915191580541432. Accuracy: 24\n",
      "Iteration: 1032. Loss: 0.00019859314488712698. Accuracy: 22\n",
      "Iteration: 1033. Loss: 0.0001464843808207661. Accuracy: 22\n",
      "Iteration: 1034. Loss: 0.00020706176292151213. Accuracy: 24\n",
      "Iteration: 1035. Loss: 0.00010925292735919356. Accuracy: 26\n",
      "Iteration: 1036. Loss: 0.0001554870541440323. Accuracy: 24\n",
      "Iteration: 1037. Loss: 0.0001274108944926411. Accuracy: 22\n",
      "Iteration: 1038. Loss: 0.00011619567521847785. Accuracy: 22\n",
      "Iteration: 1039. Loss: 0.00033050536876544356. Accuracy: 22\n",
      "Iteration: 1040. Loss: 0.0006253814790397882. Accuracy: 22\n",
      "Iteration: 1041. Loss: 0.00013664245489053428. Accuracy: 24\n",
      "Iteration: 1042. Loss: 9.94873043964617e-05. Accuracy: 22\n",
      "Iteration: 1043. Loss: 0.00016098022751975805. Accuracy: 24\n",
      "Iteration: 1044. Loss: 0.00020484924607444555. Accuracy: 22\n",
      "Iteration: 1045. Loss: 0.0002552795340307057. Accuracy: 22\n",
      "Iteration: 1046. Loss: 0.0003071594110224396. Accuracy: 22\n",
      "Iteration: 1047. Loss: 0.0008966827299445868. Accuracy: 22\n",
      "Iteration: 1048. Loss: 0.00010391235264251009. Accuracy: 22\n",
      "Iteration: 1049. Loss: 0.00015075683768372983. Accuracy: 22\n",
      "Iteration: 1050. Loss: 0.00013305664469953626. Accuracy: 22\n",
      "Iteration: 1051. Loss: 0.0001879119809018448. Accuracy: 18\n",
      "Iteration: 1052. Loss: 0.0005512237548828125. Accuracy: 24\n",
      "Iteration: 1053. Loss: 0.00011520386033225805. Accuracy: 24\n",
      "Iteration: 1054. Loss: 0.0001201629638671875. Accuracy: 24\n",
      "Iteration: 1055. Loss: 0.00029510498279705644. Accuracy: 22\n",
      "Iteration: 1056. Loss: 8.682251063873991e-05. Accuracy: 24\n",
      "Iteration: 1057. Loss: 0.00015243529924191535. Accuracy: 26\n",
      "Iteration: 1058. Loss: 0.00012886046897619963. Accuracy: 24\n",
      "Iteration: 1059. Loss: 0.00020301819313317537. Accuracy: 24\n",
      "Iteration: 1060. Loss: 0.0002688598760869354. Accuracy: 22\n",
      "Iteration: 1061. Loss: 0.0005618286086246371. Accuracy: 22\n",
      "Iteration: 1062. Loss: 0.00020462035899981856. Accuracy: 22\n",
      "Iteration: 1063. Loss: 0.00026687621721066535. Accuracy: 24\n",
      "Iteration: 1064. Loss: 4.760742012877017e-05. Accuracy: 26\n",
      "Iteration: 1065. Loss: 0.0003064727643504739. Accuracy: 24\n",
      "Iteration: 1066. Loss: 0.0001665496820351109. Accuracy: 18\n",
      "Iteration: 1067. Loss: 0.00020332336134742945. Accuracy: 26\n",
      "Iteration: 1068. Loss: 0.00035957337240688503. Accuracy: 22\n",
      "Iteration: 1069. Loss: 0.00013168335135560483. Accuracy: 26\n",
      "Iteration: 1070. Loss: 0.00020324706565588713. Accuracy: 20\n",
      "Iteration: 1071. Loss: 0.00011199951404705644. Accuracy: 24\n",
      "Iteration: 1072. Loss: 0.0002933502255473286. Accuracy: 22\n",
      "Iteration: 1073. Loss: 0.0001916503970278427. Accuracy: 20\n",
      "Iteration: 1074. Loss: 0.00012390136544127017. Accuracy: 26\n",
      "Iteration: 1075. Loss: 0.00017044067499227822. Accuracy: 22\n",
      "Iteration: 1076. Loss: 0.0002629089285619557. Accuracy: 24\n",
      "Iteration: 1077. Loss: 0.00016487122047692537. Accuracy: 22\n",
      "Iteration: 1078. Loss: 0.0012634277809411287. Accuracy: 20\n",
      "Iteration: 1079. Loss: 0.00010086059774039313. Accuracy: 22\n",
      "Iteration: 1080. Loss: 0.0001373291015625. Accuracy: 22\n",
      "Iteration: 1081. Loss: 0.00015953063848428428. Accuracy: 22\n",
      "Iteration: 1082. Loss: 0.00010864257637877017. Accuracy: 22\n",
      "Iteration: 1083. Loss: 0.00021881103748455644. Accuracy: 24\n",
      "Iteration: 1084. Loss: 0.0001223754952661693. Accuracy: 20\n",
      "Iteration: 1085. Loss: 3.7841797166038305e-05. Accuracy: 22\n",
      "Iteration: 1086. Loss: 0.0003974151622969657. Accuracy: 22\n",
      "Iteration: 1087. Loss: 0.00046134949661791325. Accuracy: 20\n",
      "Iteration: 1088. Loss: 0.00015472412633243948. Accuracy: 24\n",
      "Iteration: 1089. Loss: 0.00018356322834733874. Accuracy: 22\n",
      "Iteration: 1090. Loss: 0.0001662445138208568. Accuracy: 24\n",
      "Iteration: 1091. Loss: 0.000734786968678236. Accuracy: 20\n",
      "Iteration: 1092. Loss: 0.0015813446370884776. Accuracy: 26\n",
      "Iteration: 1093. Loss: 9.117126319324598e-05. Accuracy: 26\n",
      "Iteration: 1094. Loss: 9.071350359590724e-05. Accuracy: 26\n",
      "Iteration: 1095. Loss: 0.00011466979776741937. Accuracy: 20\n",
      "Iteration: 1096. Loss: 0.0007892608409747481. Accuracy: 20\n",
      "Iteration: 1097. Loss: 0.0003138732863590121. Accuracy: 26\n",
      "Iteration: 1098. Loss: 0.0001471710274927318. Accuracy: 26\n",
      "Iteration: 1099. Loss: 0.0001748657232383266. Accuracy: 22\n",
      "Iteration: 1100. Loss: 0.0002371215814491734. Accuracy: 20\n",
      "Iteration: 1101. Loss: 7.575989002361894e-05. Accuracy: 24\n",
      "Iteration: 1102. Loss: 0.0003691863967105746. Accuracy: 22\n",
      "Iteration: 1103. Loss: 0.00024360656971111894. Accuracy: 24\n",
      "Iteration: 1104. Loss: 0.00014770508278161287. Accuracy: 22\n",
      "Iteration: 1105. Loss: 0.00026870728470385075. Accuracy: 20\n",
      "Iteration: 1106. Loss: 0.00019508361583575606. Accuracy: 24\n",
      "Iteration: 1107. Loss: 0.0003745269787032157. Accuracy: 26\n",
      "Iteration: 1108. Loss: 0.00019142150995321572. Accuracy: 20\n",
      "Iteration: 1109. Loss: 0.0005538177210837603. Accuracy: 22\n",
      "Iteration: 1110. Loss: 8.392333984375e-05. Accuracy: 22\n",
      "Iteration: 1111. Loss: 8.445739513263106e-05. Accuracy: 20\n",
      "Iteration: 1112. Loss: 0.0002069854672299698. Accuracy: 22\n",
      "Iteration: 1113. Loss: 0.00010536194167798385. Accuracy: 20\n",
      "Iteration: 1114. Loss: 0.00014671325334347785. Accuracy: 24\n",
      "Iteration: 1115. Loss: 0.00018890381033997983. Accuracy: 24\n",
      "Iteration: 1116. Loss: 0.00021484374883584678. Accuracy: 24\n",
      "Iteration: 1117. Loss: 0.00011512756464071572. Accuracy: 24\n",
      "Iteration: 1118. Loss: 0.00014961243141442537. Accuracy: 22\n",
      "Iteration: 1119. Loss: 0.00011863708641612902. Accuracy: 22\n",
      "Iteration: 1120. Loss: 0.0004119110235478729. Accuracy: 16\n",
      "Iteration: 1121. Loss: 0.0002113342343363911. Accuracy: 24\n",
      "Iteration: 1122. Loss: 7.308959902729839e-05. Accuracy: 22\n",
      "Iteration: 1123. Loss: 0.0002444458077661693. Accuracy: 24\n",
      "Iteration: 1124. Loss: 0.00019721985154319555. Accuracy: 22\n",
      "Iteration: 1125. Loss: 0.0002972412039525807. Accuracy: 22\n",
      "Iteration: 1126. Loss: 0.0001185607907245867. Accuracy: 22\n",
      "Iteration: 1127. Loss: 9.773254714673385e-05. Accuracy: 22\n",
      "Iteration: 1128. Loss: 4.158019874012098e-05. Accuracy: 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1129. Loss: 0.0002024841378442943. Accuracy: 24\n",
      "Iteration: 1130. Loss: 0.0002111053472617641. Accuracy: 18\n",
      "Iteration: 1131. Loss: 0.0005460357642732561. Accuracy: 22\n",
      "Iteration: 1132. Loss: 0.00026039124350063503. Accuracy: 24\n",
      "Iteration: 1133. Loss: 9.742737165652215e-05. Accuracy: 22\n",
      "Iteration: 1134. Loss: 9.643554949434474e-05. Accuracy: 26\n",
      "Iteration: 1135. Loss: 0.0001535797055112198. Accuracy: 26\n",
      "Iteration: 1136. Loss: 0.0001519775396445766. Accuracy: 22\n",
      "Iteration: 1137. Loss: 0.0003128814569208771. Accuracy: 20\n",
      "Iteration: 1138. Loss: 0.00019592285389080644. Accuracy: 20\n",
      "Iteration: 1139. Loss: 0.0001920318609336391. Accuracy: 22\n",
      "Iteration: 1140. Loss: 0.0045977020636200905. Accuracy: 20\n",
      "Iteration: 1141. Loss: 0.00020988464530091733. Accuracy: 22\n",
      "Iteration: 1142. Loss: 0.00031837463029660285. Accuracy: 26\n",
      "Iteration: 1143. Loss: 0.0001345062191830948. Accuracy: 22\n",
      "Iteration: 1144. Loss: 0.0001459503109799698. Accuracy: 24\n",
      "Iteration: 1145. Loss: 7.827758963685483e-05. Accuracy: 24\n",
      "Iteration: 1146. Loss: 0.0001974487240659073. Accuracy: 20\n",
      "Iteration: 1147. Loss: 0.00016769408830441535. Accuracy: 24\n",
      "Iteration: 1148. Loss: 9.307861182605848e-05. Accuracy: 20\n",
      "Iteration: 1149. Loss: 0.00015724182594567537. Accuracy: 24\n",
      "Iteration: 1150. Loss: 0.0002456665097270161. Accuracy: 20\n",
      "Iteration: 1151. Loss: 0.0003688049328047782. Accuracy: 24\n",
      "Iteration: 1152. Loss: 0.00010276794637320563. Accuracy: 26\n",
      "Iteration: 1153. Loss: 0.00019279480329714715. Accuracy: 26\n",
      "Iteration: 1154. Loss: 0.0006489562802016735. Accuracy: 24\n",
      "Iteration: 1155. Loss: 0.0022589873988181353. Accuracy: 24\n",
      "Iteration: 1156. Loss: 0.00012023925955872983. Accuracy: 24\n",
      "Iteration: 1157. Loss: 6.927490176167339e-05. Accuracy: 22\n",
      "Iteration: 1158. Loss: 0.00010971069423248991. Accuracy: 22\n",
      "Iteration: 1159. Loss: 0.00017669677617959678. Accuracy: 24\n",
      "Iteration: 1160. Loss: 0.00016807556676212698. Accuracy: 22\n",
      "Iteration: 1161. Loss: 0.0002008819574257359. Accuracy: 22\n",
      "Iteration: 1162. Loss: 0.0001818084710976109. Accuracy: 22\n",
      "Iteration: 1163. Loss: 7.232665666379035e-05. Accuracy: 24\n",
      "Iteration: 1164. Loss: 0.00011390686267986894. Accuracy: 22\n",
      "Iteration: 1165. Loss: 0.0002442169061396271. Accuracy: 22\n",
      "Iteration: 1166. Loss: 0.00012107849033782259. Accuracy: 20\n",
      "Iteration: 1167. Loss: 0.00010261535499012098. Accuracy: 26\n",
      "Iteration: 1168. Loss: 0.00011863708641612902. Accuracy: 24\n",
      "Iteration: 1169. Loss: 0.0002246856747660786. Accuracy: 24\n",
      "Iteration: 1170. Loss: 0.00023956298537086695. Accuracy: 22\n",
      "Iteration: 1171. Loss: 0.0002237701410194859. Accuracy: 22\n",
      "Iteration: 1172. Loss: 0.0005268859677016735. Accuracy: 22\n",
      "Iteration: 1173. Loss: 0.00019096375035587698. Accuracy: 24\n",
      "Iteration: 1174. Loss: 0.00015312194591388106. Accuracy: 24\n",
      "Iteration: 1175. Loss: 7.682800060138106e-05. Accuracy: 22\n",
      "Iteration: 1176. Loss: 0.0006185913225635886. Accuracy: 22\n",
      "Iteration: 1177. Loss: 0.00021316528727766126. Accuracy: 24\n",
      "Iteration: 1178. Loss: 0.00018775940407067537. Accuracy: 20\n",
      "Iteration: 1179. Loss: 0.00040573120350018144. Accuracy: 22\n",
      "Iteration: 1180. Loss: 9.925842459779233e-05. Accuracy: 22\n",
      "Iteration: 1181. Loss: 0.0002532959042582661. Accuracy: 22\n",
      "Iteration: 1182. Loss: 0.00023185729514807463. Accuracy: 24\n",
      "Iteration: 1183. Loss: 8.323669317178428e-05. Accuracy: 26\n",
      "Iteration: 1184. Loss: 0.0005561065627261996. Accuracy: 22\n",
      "Iteration: 1185. Loss: 7.919311610748991e-05. Accuracy: 22\n",
      "Iteration: 1186. Loss: 7.072449079714715e-05. Accuracy: 26\n",
      "Iteration: 1187. Loss: 8.323669317178428e-05. Accuracy: 22\n",
      "Iteration: 1188. Loss: 0.00022308349434752017. Accuracy: 22\n",
      "Iteration: 1189. Loss: 4.7454832383664325e-05. Accuracy: 20\n",
      "Iteration: 1190. Loss: 0.00025466919760219753. Accuracy: 16\n",
      "Iteration: 1191. Loss: 0.00018646240641828626. Accuracy: 24\n",
      "Iteration: 1192. Loss: 8.598327985964715e-05. Accuracy: 22\n",
      "Iteration: 1193. Loss: 0.0002655029238667339. Accuracy: 22\n",
      "Iteration: 1194. Loss: 0.0001147460934589617. Accuracy: 22\n",
      "Iteration: 1195. Loss: 0.0006917571881785989. Accuracy: 22\n",
      "Iteration: 1196. Loss: 0.00017631531227380037. Accuracy: 24\n",
      "Iteration: 1197. Loss: 5.4321288189385086e-05. Accuracy: 28\n",
      "Iteration: 1198. Loss: 6.362915155477822e-05. Accuracy: 22\n",
      "Iteration: 1199. Loss: 0.00037330627674236894. Accuracy: 24\n",
      "Iteration: 1200. Loss: 0.00023132323985919356. Accuracy: 22\n",
      "Iteration: 1201. Loss: 0.00019477844762150198. Accuracy: 22\n",
      "Iteration: 1202. Loss: 0.00014511108747683465. Accuracy: 20\n",
      "Iteration: 1203. Loss: 0.0003990173281636089. Accuracy: 22\n",
      "Iteration: 1204. Loss: 0.0001370239187963307. Accuracy: 20\n",
      "Iteration: 1205. Loss: 7.720947178313509e-05. Accuracy: 26\n",
      "Iteration: 1206. Loss: 8.094787335721776e-05. Accuracy: 22\n",
      "Iteration: 1207. Loss: 0.000532608013600111. Accuracy: 22\n",
      "Iteration: 1208. Loss: 0.00012229919957462698. Accuracy: 24\n",
      "Iteration: 1209. Loss: 0.0003305816790089011. Accuracy: 24\n",
      "Iteration: 1210. Loss: 0.0002241516049252823. Accuracy: 22\n",
      "Iteration: 1211. Loss: 0.00021560669119935483. Accuracy: 24\n",
      "Iteration: 1212. Loss: 9.689330909168348e-05. Accuracy: 22\n",
      "Iteration: 1213. Loss: 0.00015876769612077624. Accuracy: 22\n",
      "Iteration: 1214. Loss: 0.00019439696916379035. Accuracy: 24\n",
      "Iteration: 1215. Loss: 0.0001802063052309677. Accuracy: 22\n",
      "Iteration: 1216. Loss: 0.0001402282650815323. Accuracy: 20\n",
      "Iteration: 1217. Loss: 8.888244337867945e-05. Accuracy: 24\n",
      "Iteration: 1218. Loss: 9.529113594908267e-05. Accuracy: 24\n",
      "Iteration: 1219. Loss: 0.0005046081496402621. Accuracy: 20\n",
      "Iteration: 1220. Loss: 0.00027137756114825606. Accuracy: 20\n",
      "Iteration: 1221. Loss: 8.224487100960687e-05. Accuracy: 22\n",
      "Iteration: 1222. Loss: 0.00011688232189044356. Accuracy: 22\n",
      "Iteration: 1223. Loss: 0.00010437011951580644. Accuracy: 24\n",
      "Iteration: 1224. Loss: 6.797790410928428e-05. Accuracy: 22\n",
      "Iteration: 1225. Loss: 0.00043006896157748997. Accuracy: 20\n",
      "Iteration: 1226. Loss: 0.0015106201171875. Accuracy: 24\n",
      "Iteration: 1227. Loss: 0.0001815033028833568. Accuracy: 22\n",
      "Iteration: 1228. Loss: 0.00010276794637320563. Accuracy: 26\n",
      "Iteration: 1229. Loss: 0.00034957885509356856. Accuracy: 22\n",
      "Iteration: 1230. Loss: 0.0001348114019492641. Accuracy: 26\n",
      "Iteration: 1231. Loss: 0.0001882171636680141. Accuracy: 20\n",
      "Iteration: 1232. Loss: 0.00018966675270348787. Accuracy: 24\n",
      "Iteration: 1233. Loss: 0.0001252746587852016. Accuracy: 18\n",
      "Iteration: 1234. Loss: 0.0004122924874536693. Accuracy: 20\n",
      "Iteration: 1235. Loss: 3.997802559752017e-05. Accuracy: 22\n",
      "Iteration: 1236. Loss: 8.460998651571572e-05. Accuracy: 20\n",
      "Iteration: 1237. Loss: 9.628295811126009e-05. Accuracy: 26\n",
      "Iteration: 1238. Loss: 8.209228690247983e-05. Accuracy: 22\n",
      "Iteration: 1239. Loss: 7.431030098814517e-05. Accuracy: 24\n",
      "Iteration: 1240. Loss: 0.00033706665271893144. Accuracy: 22\n",
      "Iteration: 1241. Loss: 0.00010383605695096776. Accuracy: 22\n",
      "Iteration: 1242. Loss: 0.00011215209815418348. Accuracy: 24\n",
      "Iteration: 1243. Loss: 9.384155418956652e-05. Accuracy: 24\n",
      "Iteration: 1244. Loss: 8.621215965831652e-05. Accuracy: 22\n",
      "Iteration: 1245. Loss: 0.0009697723435238004. Accuracy: 20\n",
      "Iteration: 1246. Loss: 0.00011123657168354839. Accuracy: 26\n",
      "Iteration: 1247. Loss: 0.00014442444080486894. Accuracy: 22\n",
      "Iteration: 1248. Loss: 0.00023437499476131052. Accuracy: 24\n",
      "Iteration: 1249. Loss: 0.00012573241838254035. Accuracy: 20\n",
      "Iteration: 1250. Loss: 0.00013397216389421374. Accuracy: 24\n",
      "Iteration: 1251. Loss: 0.00010070800635730848. Accuracy: 22\n",
      "Iteration: 1252. Loss: 5.3100586228538305e-05. Accuracy: 20\n",
      "Iteration: 1253. Loss: 0.0002747344842646271. Accuracy: 22\n",
      "Iteration: 1254. Loss: 4.9285888962913305e-05. Accuracy: 22\n",
      "Iteration: 1255. Loss: 0.0001065063479472883. Accuracy: 22\n",
      "Iteration: 1256. Loss: 0.00022644043201580644. Accuracy: 20\n",
      "Iteration: 1257. Loss: 3.2653810194460675e-05. Accuracy: 22\n",
      "Iteration: 1258. Loss: 0.0001665496820351109. Accuracy: 24\n",
      "Iteration: 1259. Loss: 9.88769534160383e-05. Accuracy: 22\n",
      "Iteration: 1260. Loss: 6.721496902173385e-05. Accuracy: 22\n",
      "Iteration: 1261. Loss: 8.93402102519758e-05. Accuracy: 26\n",
      "Iteration: 1262. Loss: 0.0005125427269376814. Accuracy: 24\n",
      "Iteration: 1263. Loss: 7.301330333575606e-05. Accuracy: 26\n",
      "Iteration: 1264. Loss: 9.140014299191535e-05. Accuracy: 18\n",
      "Iteration: 1265. Loss: 0.0002449035528115928. Accuracy: 22\n",
      "Iteration: 1266. Loss: 0.00011734008876373991. Accuracy: 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1267. Loss: 0.00019920349586755037. Accuracy: 26\n",
      "Iteration: 1268. Loss: 0.00047584532876498997. Accuracy: 26\n",
      "Iteration: 1269. Loss: 0.00012153625721111894. Accuracy: 24\n",
      "Iteration: 1270. Loss: 8.834838808979839e-05. Accuracy: 22\n",
      "Iteration: 1271. Loss: 4.394531424622983e-05. Accuracy: 24\n",
      "Iteration: 1272. Loss: 0.00015380859258584678. Accuracy: 22\n",
      "Iteration: 1273. Loss: 7.873534923419356e-05. Accuracy: 22\n",
      "Iteration: 1274. Loss: 8.926391456043348e-05. Accuracy: 24\n",
      "Iteration: 1275. Loss: 0.00044387817615643144. Accuracy: 24\n",
      "Iteration: 1276. Loss: 7.324219041038305e-05. Accuracy: 26\n",
      "Iteration: 1277. Loss: 6.67572021484375e-05. Accuracy: 26\n",
      "Iteration: 1278. Loss: 0.0002317809994565323. Accuracy: 24\n",
      "Iteration: 1279. Loss: 8.850097947288305e-05. Accuracy: 20\n",
      "Iteration: 1280. Loss: 0.0001338195870630443. Accuracy: 24\n",
      "Iteration: 1281. Loss: 0.0002483367861714214. Accuracy: 22\n",
      "Iteration: 1282. Loss: 0.0002575683465693146. Accuracy: 24\n",
      "Iteration: 1283. Loss: 0.004138412419706583. Accuracy: 22\n",
      "Iteration: 1284. Loss: 0.00016662597772665322. Accuracy: 24\n",
      "Iteration: 1285. Loss: 0.0002050781185971573. Accuracy: 22\n",
      "Iteration: 1286. Loss: 0.00010360717715229839. Accuracy: 22\n",
      "Iteration: 1287. Loss: 4.318237188272178e-05. Accuracy: 20\n",
      "Iteration: 1288. Loss: 0.0003464508044999093. Accuracy: 26\n",
      "Iteration: 1289. Loss: 0.00018142700719181448. Accuracy: 22\n",
      "Iteration: 1290. Loss: 0.0002600860607344657. Accuracy: 22\n",
      "Iteration: 1291. Loss: 0.00010368347284384072. Accuracy: 16\n",
      "Iteration: 1292. Loss: 0.00013435364235192537. Accuracy: 22\n",
      "Iteration: 1293. Loss: 0.0001141357424785383. Accuracy: 22\n",
      "Iteration: 1294. Loss: 9.50622561504133e-05. Accuracy: 26\n",
      "Iteration: 1295. Loss: 6.828307959949598e-05. Accuracy: 22\n",
      "Iteration: 1296. Loss: 0.00010757446580100805. Accuracy: 26\n",
      "Iteration: 1297. Loss: 0.00022689819161314517. Accuracy: 20\n",
      "Iteration: 1298. Loss: 0.0003551483096089214. Accuracy: 24\n",
      "Iteration: 1299. Loss: 5.607604907709174e-05. Accuracy: 22\n",
      "Iteration: 1300. Loss: 0.00015373229689430445. Accuracy: 22\n",
      "Iteration: 1301. Loss: 8.308410906465724e-05. Accuracy: 18\n",
      "Iteration: 1302. Loss: 0.0004302978632040322. Accuracy: 20\n",
      "Iteration: 1303. Loss: 0.00025421142345294356. Accuracy: 22\n",
      "Iteration: 1304. Loss: 0.001889724750071764. Accuracy: 24\n",
      "Iteration: 1305. Loss: 0.00024154662969522178. Accuracy: 24\n",
      "Iteration: 1306. Loss: 7.164001726778224e-05. Accuracy: 24\n",
      "Iteration: 1307. Loss: 0.0003032684326171875. Accuracy: 24\n",
      "Iteration: 1308. Loss: 0.0001325225894106552. Accuracy: 18\n",
      "Iteration: 1309. Loss: 0.00015083313337527215. Accuracy: 20\n",
      "Iteration: 1310. Loss: 0.00014961243141442537. Accuracy: 24\n",
      "Iteration: 1311. Loss: 0.00011009216541424394. Accuracy: 22\n",
      "Iteration: 1312. Loss: 7.972717139637098e-05. Accuracy: 22\n",
      "Iteration: 1313. Loss: 6.423950253520161e-05. Accuracy: 20\n",
      "Iteration: 1314. Loss: 0.0001739501894917339. Accuracy: 24\n",
      "Iteration: 1315. Loss: 9.010315261548385e-05. Accuracy: 24\n",
      "Iteration: 1316. Loss: 8.590698416810483e-05. Accuracy: 22\n",
      "Iteration: 1317. Loss: 0.0001325988705502823. Accuracy: 20\n",
      "Iteration: 1318. Loss: 0.00011833191092591733. Accuracy: 22\n",
      "Iteration: 1319. Loss: 0.0030324554536491632. Accuracy: 24\n",
      "Iteration: 1320. Loss: 0.00014602660667151213. Accuracy: 24\n",
      "Iteration: 1321. Loss: 7.843017374398187e-05. Accuracy: 24\n",
      "Iteration: 1322. Loss: 0.0001261901925317943. Accuracy: 20\n",
      "Iteration: 1323. Loss: 0.00011962890857830644. Accuracy: 24\n",
      "Iteration: 1324. Loss: 0.00010391235264251009. Accuracy: 24\n",
      "Iteration: 1325. Loss: 0.00011703491327352822. Accuracy: 24\n",
      "Iteration: 1326. Loss: 0.00022102355433162302. Accuracy: 22\n",
      "Iteration: 1327. Loss: 0.00014236450078897178. Accuracy: 22\n",
      "Iteration: 1328. Loss: 8.58306884765625e-05. Accuracy: 18\n",
      "Iteration: 1329. Loss: 6.591797136934474e-05. Accuracy: 26\n",
      "Iteration: 1330. Loss: 0.00010414123244117945. Accuracy: 22\n",
      "Iteration: 1331. Loss: 0.0001430511474609375. Accuracy: 16\n",
      "Iteration: 1332. Loss: 0.0005718230968341231. Accuracy: 24\n",
      "Iteration: 1333. Loss: 6.286620919127017e-05. Accuracy: 22\n",
      "Iteration: 1334. Loss: 0.00014137268590275198. Accuracy: 20\n",
      "Iteration: 1335. Loss: 0.0004096984921488911. Accuracy: 22\n",
      "Iteration: 1336. Loss: 0.00019187926955055445. Accuracy: 22\n",
      "Iteration: 1337. Loss: 0.0001434326113667339. Accuracy: 24\n",
      "Iteration: 1338. Loss: 0.00026535033248364925. Accuracy: 22\n",
      "Iteration: 1339. Loss: 0.0001445770321879536. Accuracy: 24\n",
      "Iteration: 1340. Loss: 0.00010864257637877017. Accuracy: 22\n",
      "Iteration: 1341. Loss: 0.00021316528727766126. Accuracy: 20\n",
      "Iteration: 1342. Loss: 0.00026412963052280247. Accuracy: 22\n",
      "Iteration: 1343. Loss: 0.00015151978004723787. Accuracy: 24\n",
      "Iteration: 1344. Loss: 0.00011917114170501009. Accuracy: 24\n",
      "Iteration: 1345. Loss: 6.629943527514115e-05. Accuracy: 24\n",
      "Iteration: 1346. Loss: 0.0002886962902266532. Accuracy: 18\n",
      "Iteration: 1347. Loss: 7.766723865643144e-05. Accuracy: 24\n",
      "Iteration: 1348. Loss: 0.00017791747814044356. Accuracy: 22\n",
      "Iteration: 1349. Loss: 0.000822525005787611. Accuracy: 24\n",
      "Iteration: 1350. Loss: 0.00010986327833961695. Accuracy: 20\n",
      "Iteration: 1351. Loss: 0.00010437011951580644. Accuracy: 22\n",
      "Iteration: 1352. Loss: 0.00024940489674918354. Accuracy: 24\n",
      "Iteration: 1353. Loss: 0.0002898407110478729. Accuracy: 20\n",
      "Iteration: 1354. Loss: 9.971618419513106e-05. Accuracy: 20\n",
      "Iteration: 1355. Loss: 0.000505905132740736. Accuracy: 22\n",
      "Iteration: 1356. Loss: 9.353637869935483e-05. Accuracy: 22\n",
      "Iteration: 1357. Loss: 0.00012351990153547376. Accuracy: 24\n",
      "Iteration: 1358. Loss: 0.00011039734090445563. Accuracy: 22\n",
      "Iteration: 1359. Loss: 0.00026412963052280247. Accuracy: 22\n",
      "Iteration: 1360. Loss: 9.010315261548385e-05. Accuracy: 16\n",
      "Iteration: 1361. Loss: 0.0003578948962967843. Accuracy: 18\n",
      "Iteration: 1362. Loss: 0.00014869689766783267. Accuracy: 24\n",
      "Iteration: 1363. Loss: 0.0001697540283203125. Accuracy: 26\n",
      "Iteration: 1364. Loss: 0.00010826110519701615. Accuracy: 22\n",
      "Iteration: 1365. Loss: 0.00010070800635730848. Accuracy: 26\n",
      "Iteration: 1366. Loss: 8.003234688658267e-05. Accuracy: 26\n",
      "Iteration: 1367. Loss: 0.0001252746587852016. Accuracy: 24\n",
      "Iteration: 1368. Loss: 8.514404180459678e-05. Accuracy: 22\n",
      "Iteration: 1369. Loss: 0.00022262573475018144. Accuracy: 24\n",
      "Iteration: 1370. Loss: 0.000308990478515625. Accuracy: 24\n",
      "Iteration: 1371. Loss: 0.00010513305460335687. Accuracy: 24\n",
      "Iteration: 1372. Loss: 0.00029617309337481856. Accuracy: 22\n",
      "Iteration: 1373. Loss: 7.873534923419356e-05. Accuracy: 22\n",
      "Iteration: 1374. Loss: 8.834838808979839e-05. Accuracy: 24\n",
      "Iteration: 1375. Loss: 4.074096796102822e-05. Accuracy: 22\n",
      "Iteration: 1376. Loss: 0.0004871368291787803. Accuracy: 22\n",
      "Iteration: 1377. Loss: 0.00030494690872728825. Accuracy: 24\n",
      "Iteration: 1378. Loss: 0.0001910400460474193. Accuracy: 20\n",
      "Iteration: 1379. Loss: 0.00011390686267986894. Accuracy: 22\n",
      "Iteration: 1380. Loss: 0.0001799011224647984. Accuracy: 22\n",
      "Iteration: 1381. Loss: 0.00022010803513694555. Accuracy: 24\n",
      "Iteration: 1382. Loss: 8.399963553529233e-05. Accuracy: 18\n",
      "Iteration: 1383. Loss: 0.00016639709065202624. Accuracy: 24\n",
      "Iteration: 1384. Loss: 5.790710565634072e-05. Accuracy: 22\n",
      "Iteration: 1385. Loss: 0.00012657165643759072. Accuracy: 24\n",
      "Iteration: 1386. Loss: 0.0002574920654296875. Accuracy: 22\n",
      "Iteration: 1387. Loss: 0.0003451538213994354. Accuracy: 26\n",
      "Iteration: 1388. Loss: 6.576537998626009e-05. Accuracy: 22\n",
      "Iteration: 1389. Loss: 0.00011306762462481856. Accuracy: 26\n",
      "Iteration: 1390. Loss: 0.00027175902505405247. Accuracy: 20\n",
      "Iteration: 1391. Loss: 0.00010742187441792339. Accuracy: 24\n",
      "Iteration: 1392. Loss: 9.063720790436491e-05. Accuracy: 24\n",
      "Iteration: 1393. Loss: 0.00025962828658521175. Accuracy: 24\n",
      "Iteration: 1394. Loss: 9.170532575808465e-05. Accuracy: 20\n",
      "Iteration: 1395. Loss: 0.0002979278506245464. Accuracy: 24\n",
      "Iteration: 1396. Loss: 0.0002095031668432057. Accuracy: 24\n",
      "Iteration: 1397. Loss: 0.00020523070998024195. Accuracy: 24\n",
      "Iteration: 1398. Loss: 0.00012535095447674394. Accuracy: 26\n",
      "Iteration: 1399. Loss: 6.408691115211695e-05. Accuracy: 22\n",
      "Iteration: 1400. Loss: 0.00010261535499012098. Accuracy: 24\n",
      "Iteration: 1401. Loss: 0.0003654480096884072. Accuracy: 24\n",
      "Iteration: 1402. Loss: 0.0001659393310546875. Accuracy: 22\n",
      "Iteration: 1403. Loss: 3.570556509657763e-05. Accuracy: 24\n",
      "Iteration: 1404. Loss: 0.00017539977852720767. Accuracy: 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1405. Loss: 0.00014930724864825606. Accuracy: 20\n",
      "Iteration: 1406. Loss: 0.0003436279366724193. Accuracy: 24\n",
      "Iteration: 1407. Loss: 0.0001776886056177318. Accuracy: 24\n",
      "Iteration: 1408. Loss: 0.0001882171636680141. Accuracy: 22\n",
      "Iteration: 1409. Loss: 0.00027328490978106856. Accuracy: 22\n",
      "Iteration: 1410. Loss: 0.0001261901925317943. Accuracy: 20\n",
      "Iteration: 1411. Loss: 0.00013496399333234876. Accuracy: 22\n",
      "Iteration: 1412. Loss: 0.00033241271739825606. Accuracy: 22\n",
      "Iteration: 1413. Loss: 0.0002062988351099193. Accuracy: 18\n",
      "Iteration: 1414. Loss: 0.0002781677176244557. Accuracy: 24\n",
      "Iteration: 1415. Loss: 9.544372733216733e-05. Accuracy: 24\n",
      "Iteration: 1416. Loss: 8.537292160326615e-05. Accuracy: 24\n",
      "Iteration: 1417. Loss: 0.00034698485978879035. Accuracy: 22\n",
      "Iteration: 1418. Loss: 0.00011329651169944555. Accuracy: 26\n",
      "Iteration: 1419. Loss: 0.00017845153342932463. Accuracy: 22\n",
      "Iteration: 1420. Loss: 4.074096796102822e-05. Accuracy: 20\n",
      "Iteration: 1421. Loss: 0.0002711486886255443. Accuracy: 24\n",
      "Iteration: 1422. Loss: 0.0003178405750077218. Accuracy: 22\n",
      "Iteration: 1423. Loss: 7.560729864053428e-05. Accuracy: 22\n",
      "Iteration: 1424. Loss: 0.0001567840517964214. Accuracy: 22\n",
      "Iteration: 1425. Loss: 0.00023246764612849802. Accuracy: 24\n",
      "Iteration: 1426. Loss: 7.308959902729839e-05. Accuracy: 22\n",
      "Iteration: 1427. Loss: 0.00010131835733773187. Accuracy: 22\n",
      "Iteration: 1428. Loss: 0.0001309967046836391. Accuracy: 24\n",
      "Iteration: 1429. Loss: 0.00012123108172090724. Accuracy: 22\n",
      "Iteration: 1430. Loss: 0.0001700592110864818. Accuracy: 20\n",
      "Iteration: 1431. Loss: 0.00012725830310955644. Accuracy: 24\n",
      "Iteration: 1432. Loss: 0.00014442444080486894. Accuracy: 24\n",
      "Iteration: 1433. Loss: 6.591797136934474e-05. Accuracy: 22\n",
      "Iteration: 1434. Loss: 0.0003198242047801614. Accuracy: 24\n",
      "Iteration: 1435. Loss: 0.000148773193359375. Accuracy: 28\n",
      "Iteration: 1436. Loss: 0.00013359069998841733. Accuracy: 22\n",
      "Iteration: 1437. Loss: 0.0001496887271059677. Accuracy: 22\n",
      "Iteration: 1438. Loss: 0.00012031555525027215. Accuracy: 24\n",
      "Iteration: 1439. Loss: 0.0003195953322574496. Accuracy: 24\n",
      "Iteration: 1440. Loss: 0.00015083313337527215. Accuracy: 24\n",
      "Iteration: 1441. Loss: 0.00010040283086709678. Accuracy: 24\n",
      "Iteration: 1442. Loss: 0.00010940551874227822. Accuracy: 24\n",
      "Iteration: 1443. Loss: 0.0001388549862895161. Accuracy: 22\n",
      "Iteration: 1444. Loss: 0.0001773834228515625. Accuracy: 24\n",
      "Iteration: 1445. Loss: 6.607055547647178e-05. Accuracy: 22\n",
      "Iteration: 1446. Loss: 0.0001796722353901714. Accuracy: 26\n",
      "Iteration: 1447. Loss: 5.355834946385585e-05. Accuracy: 22\n",
      "Iteration: 1448. Loss: 0.0001735687255859375. Accuracy: 24\n",
      "Iteration: 1449. Loss: 7.202148117357865e-05. Accuracy: 24\n",
      "Iteration: 1450. Loss: 4.0893555706134066e-05. Accuracy: 24\n",
      "Iteration: 1451. Loss: 0.00011558532423805445. Accuracy: 22\n",
      "Iteration: 1452. Loss: 0.00011024474952137098. Accuracy: 22\n",
      "Iteration: 1453. Loss: 0.0001462554937461391. Accuracy: 26\n",
      "Iteration: 1454. Loss: 0.00014991759962867945. Accuracy: 24\n",
      "Iteration: 1455. Loss: 0.00012107849033782259. Accuracy: 26\n",
      "Iteration: 1456. Loss: 0.00010147094872081652. Accuracy: 22\n",
      "Iteration: 1457. Loss: 4.402160629979335e-05. Accuracy: 22\n",
      "Iteration: 1458. Loss: 0.00019432067347224802. Accuracy: 22\n",
      "Iteration: 1459. Loss: 5.981445428915322e-05. Accuracy: 20\n",
      "Iteration: 1460. Loss: 0.00012390136544127017. Accuracy: 24\n",
      "Iteration: 1461. Loss: 0.00017654418479651213. Accuracy: 22\n",
      "Iteration: 1462. Loss: 0.0012499999720603228. Accuracy: 20\n",
      "Iteration: 1463. Loss: 0.0001141357424785383. Accuracy: 20\n",
      "Iteration: 1464. Loss: 0.0001770782400853932. Accuracy: 22\n",
      "Iteration: 1465. Loss: 0.0001358032168354839. Accuracy: 24\n",
      "Iteration: 1466. Loss: 0.0003157806349918246. Accuracy: 18\n",
      "Iteration: 1467. Loss: 0.0011484527494758368. Accuracy: 24\n",
      "Iteration: 1468. Loss: 0.0002419280936010182. Accuracy: 20\n",
      "Iteration: 1469. Loss: 7.324219041038305e-05. Accuracy: 24\n",
      "Iteration: 1470. Loss: 0.0002552032528910786. Accuracy: 24\n",
      "Iteration: 1471. Loss: 0.00026466368581168354. Accuracy: 24\n",
      "Iteration: 1472. Loss: 0.0001910400460474193. Accuracy: 24\n",
      "Iteration: 1473. Loss: 0.00013267516624182463. Accuracy: 18\n",
      "Iteration: 1474. Loss: 0.00014602660667151213. Accuracy: 22\n",
      "Iteration: 1475. Loss: 0.00017547607421875. Accuracy: 24\n",
      "Iteration: 1476. Loss: 0.000531692523509264. Accuracy: 24\n",
      "Iteration: 1477. Loss: 8.026123396120965e-05. Accuracy: 24\n",
      "Iteration: 1478. Loss: 7.759094296488911e-05. Accuracy: 22\n",
      "Iteration: 1479. Loss: 6.851195939816535e-05. Accuracy: 20\n",
      "Iteration: 1480. Loss: 5.294799848343246e-05. Accuracy: 22\n",
      "Iteration: 1481. Loss: 0.00014862060197629035. Accuracy: 24\n",
      "Iteration: 1482. Loss: 0.00010795592970680445. Accuracy: 24\n",
      "Iteration: 1483. Loss: 0.0008065795991569757. Accuracy: 20\n",
      "Iteration: 1484. Loss: 6.752014451194555e-05. Accuracy: 26\n",
      "Iteration: 1485. Loss: 0.0002867126604542136. Accuracy: 22\n",
      "Iteration: 1486. Loss: 9.5367431640625e-05. Accuracy: 24\n",
      "Iteration: 1487. Loss: 8.99505612323992e-05. Accuracy: 20\n",
      "Iteration: 1488. Loss: 9.338378731627017e-05. Accuracy: 22\n",
      "Iteration: 1489. Loss: 0.0001672363287070766. Accuracy: 22\n",
      "Iteration: 1490. Loss: 0.00017021178791765124. Accuracy: 22\n",
      "Iteration: 1491. Loss: 7.751464727334678e-05. Accuracy: 22\n",
      "Iteration: 1492. Loss: 0.0001468658447265625. Accuracy: 24\n",
      "Iteration: 1493. Loss: 5.8059693401446566e-05. Accuracy: 20\n",
      "Iteration: 1494. Loss: 0.00017364502127747983. Accuracy: 22\n",
      "Iteration: 1495. Loss: 0.00011459350935183465. Accuracy: 20\n",
      "Iteration: 1496. Loss: 0.0001160430911113508. Accuracy: 24\n",
      "Iteration: 1497. Loss: 0.0001892089785542339. Accuracy: 20\n",
      "Iteration: 1498. Loss: 7.156372157623991e-05. Accuracy: 24\n",
      "Iteration: 1499. Loss: 0.0003348541213199496. Accuracy: 22\n",
      "Iteration: 1500. Loss: 5.91278076171875e-05. Accuracy: 22\n",
      "Iteration: 1501. Loss: 0.00017494201892986894. Accuracy: 26\n",
      "Iteration: 1502. Loss: 0.0002483367861714214. Accuracy: 24\n",
      "Iteration: 1503. Loss: 0.0002400207449682057. Accuracy: 22\n",
      "Iteration: 1504. Loss: 0.0001185607907245867. Accuracy: 24\n",
      "Iteration: 1505. Loss: 0.0002714538713917136. Accuracy: 24\n",
      "Iteration: 1506. Loss: 0.00013229370233602822. Accuracy: 22\n",
      "Iteration: 1507. Loss: 4.432678179000504e-05. Accuracy: 22\n",
      "Iteration: 1508. Loss: 0.00024024963204283267. Accuracy: 24\n",
      "Iteration: 1509. Loss: 8.506774611305445e-05. Accuracy: 22\n",
      "Iteration: 1510. Loss: 6.0043334087822586e-05. Accuracy: 24\n",
      "Iteration: 1511. Loss: 0.00012702941603492945. Accuracy: 18\n",
      "Iteration: 1512. Loss: 8.102416904876009e-05. Accuracy: 24\n",
      "Iteration: 1513. Loss: 0.00420799246057868. Accuracy: 20\n",
      "Iteration: 1514. Loss: 0.0001306152407778427. Accuracy: 24\n",
      "Iteration: 1515. Loss: 0.00015060424630064517. Accuracy: 22\n",
      "Iteration: 1516. Loss: 0.0002593994140625. Accuracy: 22\n",
      "Iteration: 1517. Loss: 4.1046143451239914e-05. Accuracy: 24\n",
      "Iteration: 1518. Loss: 0.0002244567876914516. Accuracy: 18\n",
      "Iteration: 1519. Loss: 0.00029090882162563503. Accuracy: 20\n",
      "Iteration: 1520. Loss: 9.010315261548385e-05. Accuracy: 24\n",
      "Iteration: 1521. Loss: 7.118225039448589e-05. Accuracy: 26\n",
      "Iteration: 1522. Loss: 0.00021766663121525198. Accuracy: 22\n",
      "Iteration: 1523. Loss: 0.00011039734090445563. Accuracy: 22\n",
      "Iteration: 1524. Loss: 0.00017173767264466733. Accuracy: 24\n",
      "Iteration: 1525. Loss: 0.00011848449503304437. Accuracy: 24\n",
      "Iteration: 1526. Loss: 5.516052260645665e-05. Accuracy: 18\n",
      "Iteration: 1527. Loss: 0.00013916015450377017. Accuracy: 26\n",
      "Iteration: 1528. Loss: 0.000527877826243639. Accuracy: 20\n",
      "Iteration: 1529. Loss: 0.0001382446353090927. Accuracy: 20\n",
      "Iteration: 1530. Loss: 0.0003604126104619354. Accuracy: 18\n",
      "Iteration: 1531. Loss: 0.00029800416086800396. Accuracy: 24\n",
      "Iteration: 1532. Loss: 5.874633643543348e-05. Accuracy: 24\n",
      "Iteration: 1533. Loss: 0.0001687622134340927. Accuracy: 20\n",
      "Iteration: 1534. Loss: 7.614135392941535e-05. Accuracy: 26\n",
      "Iteration: 1535. Loss: 8.720398182049394e-05. Accuracy: 22\n",
      "Iteration: 1536. Loss: 9.429931378690526e-05. Accuracy: 24\n",
      "Iteration: 1537. Loss: 8.23211667011492e-05. Accuracy: 22\n",
      "Iteration: 1538. Loss: 7.759094296488911e-05. Accuracy: 22\n",
      "Iteration: 1539. Loss: 0.0005761718493886292. Accuracy: 22\n",
      "Iteration: 1540. Loss: 6.0195921832928434e-05. Accuracy: 22\n",
      "Iteration: 1541. Loss: 0.0005194091936573386. Accuracy: 22\n",
      "Iteration: 1542. Loss: 0.00023826598771847785. Accuracy: 24\n",
      "Iteration: 1543. Loss: 7.804870256222785e-05. Accuracy: 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1544. Loss: 0.0004774475237354636. Accuracy: 22\n",
      "Iteration: 1545. Loss: 0.0001405334478477016. Accuracy: 24\n",
      "Iteration: 1546. Loss: 0.0002161407464882359. Accuracy: 22\n",
      "Iteration: 1547. Loss: 5.6381224567303434e-05. Accuracy: 24\n",
      "Iteration: 1548. Loss: 7.423400529660285e-05. Accuracy: 22\n",
      "Iteration: 1549. Loss: 0.00018585205543786287. Accuracy: 26\n",
      "Iteration: 1550. Loss: 0.00024391173792537302. Accuracy: 22\n",
      "Iteration: 1551. Loss: 9.078979201149195e-05. Accuracy: 22\n",
      "Iteration: 1552. Loss: 5.798339770990424e-05. Accuracy: 22\n",
      "Iteration: 1553. Loss: 5.645752025884576e-05. Accuracy: 24\n",
      "Iteration: 1554. Loss: 0.00020599365234375. Accuracy: 24\n",
      "Iteration: 1555. Loss: 0.0001230621273862198. Accuracy: 22\n",
      "Iteration: 1556. Loss: 0.0002963256847579032. Accuracy: 24\n",
      "Iteration: 1557. Loss: 0.00018051147344522178. Accuracy: 24\n",
      "Iteration: 1558. Loss: 0.00016998291539493948. Accuracy: 26\n",
      "Iteration: 1559. Loss: 0.00012290955055505037. Accuracy: 22\n",
      "Iteration: 1560. Loss: 0.0001738739083521068. Accuracy: 22\n",
      "Iteration: 1561. Loss: 0.00037780762067995965. Accuracy: 24\n",
      "Iteration: 1562. Loss: 7.560729864053428e-05. Accuracy: 24\n",
      "Iteration: 1563. Loss: 0.00021087646018713713. Accuracy: 24\n",
      "Iteration: 1564. Loss: 0.0003093719424214214. Accuracy: 26\n",
      "Iteration: 1565. Loss: 0.00012756347132381052. Accuracy: 22\n",
      "Iteration: 1566. Loss: 0.00010673522774595767. Accuracy: 22\n",
      "Iteration: 1567. Loss: 0.0002317809994565323. Accuracy: 24\n",
      "Iteration: 1568. Loss: 0.00028137207846157253. Accuracy: 22\n",
      "Iteration: 1569. Loss: 0.00030418395181186497. Accuracy: 22\n",
      "Iteration: 1570. Loss: 0.00029235839610919356. Accuracy: 24\n",
      "Iteration: 1571. Loss: 9.1552734375e-05. Accuracy: 20\n",
      "Iteration: 1572. Loss: 0.00011726379307219759. Accuracy: 20\n",
      "Iteration: 1573. Loss: 6.767272861907259e-05. Accuracy: 22\n",
      "Iteration: 1574. Loss: 0.0003659057547338307. Accuracy: 20\n",
      "Iteration: 1575. Loss: 0.0003305816790089011. Accuracy: 20\n",
      "Iteration: 1576. Loss: 5.538940604310483e-05. Accuracy: 22\n",
      "Iteration: 1577. Loss: 7.148742588469759e-05. Accuracy: 26\n",
      "Iteration: 1578. Loss: 0.0003296661307103932. Accuracy: 22\n",
      "Iteration: 1579. Loss: 0.00019569396681617945. Accuracy: 24\n",
      "Iteration: 1580. Loss: 2.9830933272023685e-05. Accuracy: 26\n",
      "Iteration: 1581. Loss: 7.995605119504035e-05. Accuracy: 24\n",
      "Iteration: 1582. Loss: 0.00031448365189135075. Accuracy: 20\n",
      "Iteration: 1583. Loss: 0.00015899658319540322. Accuracy: 20\n",
      "Iteration: 1584. Loss: 0.00020523070998024195. Accuracy: 26\n",
      "Iteration: 1585. Loss: 0.00013282775762490928. Accuracy: 24\n",
      "Iteration: 1586. Loss: 0.0004688262997660786. Accuracy: 18\n",
      "Iteration: 1587. Loss: 8.659363084007055e-05. Accuracy: 22\n",
      "Iteration: 1588. Loss: 0.00017120361735578626. Accuracy: 24\n",
      "Iteration: 1589. Loss: 0.0001605224679224193. Accuracy: 24\n",
      "Iteration: 1590. Loss: 0.0001725769106997177. Accuracy: 18\n",
      "Iteration: 1591. Loss: 0.0001316070556640625. Accuracy: 20\n",
      "Iteration: 1592. Loss: 0.0001961517264135182. Accuracy: 22\n",
      "Iteration: 1593. Loss: 0.00013534545723814517. Accuracy: 24\n",
      "Iteration: 1594. Loss: 0.00019477844762150198. Accuracy: 20\n",
      "Iteration: 1595. Loss: 0.00016738892009016126. Accuracy: 22\n",
      "Iteration: 1596. Loss: 9.803772263694555e-05. Accuracy: 22\n",
      "Iteration: 1597. Loss: 7.919311610748991e-05. Accuracy: 24\n",
      "Iteration: 1598. Loss: 0.0002127075131284073. Accuracy: 22\n",
      "Iteration: 1599. Loss: 0.00010597229265840724. Accuracy: 22\n",
      "Iteration: 1600. Loss: 0.00014602660667151213. Accuracy: 20\n",
      "Iteration: 1601. Loss: 8.819580398267135e-05. Accuracy: 20\n",
      "Iteration: 1602. Loss: 5.8212281146552414e-05. Accuracy: 22\n",
      "Iteration: 1603. Loss: 0.00017150878557004035. Accuracy: 24\n",
      "Iteration: 1604. Loss: 0.00010147094872081652. Accuracy: 24\n",
      "Iteration: 1605. Loss: 0.00017623901658225805. Accuracy: 24\n",
      "Iteration: 1606. Loss: 7.583617843920365e-05. Accuracy: 20\n",
      "Iteration: 1607. Loss: 8.277893357444555e-05. Accuracy: 24\n",
      "Iteration: 1608. Loss: 0.00017791747814044356. Accuracy: 24\n",
      "Iteration: 1609. Loss: 6.851195939816535e-05. Accuracy: 22\n",
      "Iteration: 1610. Loss: 7.621764962095767e-05. Accuracy: 24\n",
      "Iteration: 1611. Loss: 0.001344223041087389. Accuracy: 24\n",
      "Iteration: 1612. Loss: 0.00012023925955872983. Accuracy: 22\n",
      "Iteration: 1613. Loss: 0.00020591735665220767. Accuracy: 22\n",
      "Iteration: 1614. Loss: 4.2800904338946566e-05. Accuracy: 18\n",
      "Iteration: 1615. Loss: 0.0002101898135151714. Accuracy: 24\n",
      "Iteration: 1616. Loss: 0.00012077331484761089. Accuracy: 20\n",
      "Iteration: 1617. Loss: 6.683349783997983e-05. Accuracy: 20\n",
      "Iteration: 1618. Loss: 0.00011100769188487902. Accuracy: 24\n",
      "Iteration: 1619. Loss: 7.392882980639115e-05. Accuracy: 20\n",
      "Iteration: 1620. Loss: 0.00010398864833405241. Accuracy: 20\n",
      "Iteration: 1621. Loss: 5.9967042034259066e-05. Accuracy: 20\n",
      "Iteration: 1622. Loss: 4.318237188272178e-05. Accuracy: 20\n",
      "Iteration: 1623. Loss: 5.790710565634072e-05. Accuracy: 22\n",
      "Iteration: 1624. Loss: 0.00017692566325422376. Accuracy: 24\n",
      "Iteration: 1625. Loss: 0.0002206420904258266. Accuracy: 22\n",
      "Iteration: 1626. Loss: 8.193969551939517e-05. Accuracy: 24\n",
      "Iteration: 1627. Loss: 9.18579098652117e-05. Accuracy: 24\n",
      "Iteration: 1628. Loss: 0.00011726379307219759. Accuracy: 20\n",
      "Iteration: 1629. Loss: 5.683898780262098e-05. Accuracy: 22\n",
      "Iteration: 1630. Loss: 0.00023468017752747983. Accuracy: 24\n",
      "Iteration: 1631. Loss: 0.00037780762067995965. Accuracy: 22\n",
      "Iteration: 1632. Loss: 9.010315261548385e-05. Accuracy: 20\n",
      "Iteration: 1633. Loss: 0.00027290344587527215. Accuracy: 24\n",
      "Iteration: 1634. Loss: 6.668090645689517e-05. Accuracy: 22\n",
      "Iteration: 1635. Loss: 0.002121505793184042. Accuracy: 24\n",
      "Iteration: 1636. Loss: 0.00016456603771075606. Accuracy: 22\n",
      "Iteration: 1637. Loss: 0.00011650085798464715. Accuracy: 22\n",
      "Iteration: 1638. Loss: 0.0001277923583984375. Accuracy: 22\n",
      "Iteration: 1639. Loss: 0.00011077881208620965. Accuracy: 22\n",
      "Iteration: 1640. Loss: 7.85064694355242e-05. Accuracy: 20\n",
      "Iteration: 1641. Loss: 0.00015838623221497983. Accuracy: 24\n",
      "Iteration: 1642. Loss: 8.415221964241937e-05. Accuracy: 22\n",
      "Iteration: 1643. Loss: 9.292602771893144e-05. Accuracy: 22\n",
      "Iteration: 1644. Loss: 5.729675467591733e-05. Accuracy: 20\n",
      "Iteration: 1645. Loss: 8.300781337311491e-05. Accuracy: 20\n",
      "Iteration: 1646. Loss: 5.5007934861350805e-05. Accuracy: 24\n",
      "Iteration: 1647. Loss: 0.0009796905796974897. Accuracy: 22\n",
      "Iteration: 1648. Loss: 8.865356358001009e-05. Accuracy: 24\n",
      "Iteration: 1649. Loss: 7.225036824820563e-05. Accuracy: 22\n",
      "Iteration: 1650. Loss: 4.241943315719254e-05. Accuracy: 24\n",
      "Iteration: 1651. Loss: 0.00016448974201921374. Accuracy: 24\n",
      "Iteration: 1652. Loss: 0.00017478942754678428. Accuracy: 24\n",
      "Iteration: 1653. Loss: 0.0001376342843286693. Accuracy: 20\n",
      "Iteration: 1654. Loss: 6.378173566190526e-05. Accuracy: 18\n",
      "Iteration: 1655. Loss: 0.00021667480177711695. Accuracy: 22\n",
      "Iteration: 1656. Loss: 8.071899355854839e-05. Accuracy: 22\n",
      "Iteration: 1657. Loss: 0.00016296387184411287. Accuracy: 22\n",
      "Iteration: 1658. Loss: 0.00014831543376203626. Accuracy: 22\n",
      "Iteration: 1659. Loss: 8.811950829112902e-05. Accuracy: 22\n",
      "Iteration: 1660. Loss: 0.0001309967046836391. Accuracy: 22\n",
      "Iteration: 1661. Loss: 5.470275937113911e-05. Accuracy: 20\n",
      "Iteration: 1662. Loss: 0.0001249694760190323. Accuracy: 24\n",
      "Iteration: 1663. Loss: 0.00011070251639466733. Accuracy: 22\n",
      "Iteration: 1664. Loss: 5.3100586228538305e-05. Accuracy: 20\n",
      "Iteration: 1665. Loss: 0.00036766051198355854. Accuracy: 22\n",
      "Iteration: 1666. Loss: 0.0003558349562808871. Accuracy: 22\n",
      "Iteration: 1667. Loss: 0.00034889220842160285. Accuracy: 22\n",
      "Iteration: 1668. Loss: 0.0009642791701480746. Accuracy: 20\n",
      "Iteration: 1669. Loss: 0.00023101807164493948. Accuracy: 24\n",
      "Iteration: 1670. Loss: 8.483886631438509e-05. Accuracy: 20\n",
      "Iteration: 1671. Loss: 0.0002263641363242641. Accuracy: 22\n",
      "Iteration: 1672. Loss: 0.00021278380881994963. Accuracy: 24\n",
      "Iteration: 1673. Loss: 0.0006272125174291432. Accuracy: 26\n",
      "Iteration: 1674. Loss: 0.0003082275507040322. Accuracy: 22\n",
      "Iteration: 1675. Loss: 6.393432704498991e-05. Accuracy: 20\n",
      "Iteration: 1676. Loss: 0.00037528990651480854. Accuracy: 22\n",
      "Iteration: 1677. Loss: 0.0001281738223042339. Accuracy: 26\n",
      "Iteration: 1678. Loss: 0.0001932525628944859. Accuracy: 22\n",
      "Iteration: 1679. Loss: 0.0001348114019492641. Accuracy: 22\n",
      "Iteration: 1680. Loss: 0.0001128387448261492. Accuracy: 22\n",
      "Iteration: 1681. Loss: 0.0001786804205039516. Accuracy: 24\n",
      "Iteration: 1682. Loss: 0.00016281128046102822. Accuracy: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1683. Loss: 0.00019309997151140124. Accuracy: 22\n",
      "Iteration: 1684. Loss: 0.0001497650082455948. Accuracy: 24\n",
      "Iteration: 1685. Loss: 4.806518700206652e-05. Accuracy: 22\n",
      "Iteration: 1686. Loss: 3.5400389606365934e-05. Accuracy: 18\n",
      "Iteration: 1687. Loss: 7.339477451751009e-05. Accuracy: 20\n",
      "Iteration: 1688. Loss: 0.0002830505254678428. Accuracy: 22\n",
      "Iteration: 1689. Loss: 9.811401105253026e-05. Accuracy: 26\n",
      "Iteration: 1690. Loss: 0.00014564514276571572. Accuracy: 20\n",
      "Iteration: 1691. Loss: 0.0004666137683670968. Accuracy: 24\n",
      "Iteration: 1692. Loss: 0.000179290771484375. Accuracy: 24\n",
      "Iteration: 1693. Loss: 0.00011047362932004035. Accuracy: 24\n",
      "Iteration: 1694. Loss: 0.0001277923583984375. Accuracy: 22\n",
      "Iteration: 1695. Loss: 0.0001942443777807057. Accuracy: 22\n",
      "Iteration: 1696. Loss: 0.00021759033552370965. Accuracy: 20\n",
      "Iteration: 1697. Loss: 0.0001459503109799698. Accuracy: 22\n",
      "Iteration: 1698. Loss: 3.7841797166038305e-05. Accuracy: 22\n",
      "Iteration: 1699. Loss: 0.0001468658447265625. Accuracy: 22\n",
      "Iteration: 1700. Loss: 0.00011848449503304437. Accuracy: 24\n",
      "Iteration: 1701. Loss: 0.0006412506336346269. Accuracy: 24\n",
      "Iteration: 1702. Loss: 5.53131103515625e-05. Accuracy: 26\n",
      "Iteration: 1703. Loss: 0.0003020477306563407. Accuracy: 20\n",
      "Iteration: 1704. Loss: 7.339477451751009e-05. Accuracy: 26\n",
      "Iteration: 1705. Loss: 9.368896280648187e-05. Accuracy: 22\n",
      "Iteration: 1706. Loss: 7.62939453125e-05. Accuracy: 18\n",
      "Iteration: 1707. Loss: 9.170532575808465e-05. Accuracy: 22\n",
      "Iteration: 1708. Loss: 0.00010452270362293348. Accuracy: 26\n",
      "Iteration: 1709. Loss: 0.00012199401680845767. Accuracy: 22\n",
      "Iteration: 1710. Loss: 0.0002778625348582864. Accuracy: 20\n",
      "Iteration: 1711. Loss: 0.00010292053048033267. Accuracy: 22\n",
      "Iteration: 1712. Loss: 8.033752237679437e-05. Accuracy: 24\n",
      "Iteration: 1713. Loss: 6.439208664232865e-05. Accuracy: 22\n",
      "Iteration: 1714. Loss: 0.0001789855887182057. Accuracy: 18\n",
      "Iteration: 1715. Loss: 9.780883556231856e-05. Accuracy: 24\n",
      "Iteration: 1716. Loss: 5.3634645155398175e-05. Accuracy: 24\n",
      "Iteration: 1717. Loss: 5.874633643543348e-05. Accuracy: 24\n",
      "Iteration: 1718. Loss: 8.781433280091733e-05. Accuracy: 24\n",
      "Iteration: 1719. Loss: 0.00019561767112463713. Accuracy: 22\n",
      "Iteration: 1720. Loss: 6.408691115211695e-05. Accuracy: 24\n",
      "Iteration: 1721. Loss: 0.00014808654668740928. Accuracy: 22\n",
      "Iteration: 1722. Loss: 0.0002945709275081754. Accuracy: 24\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e32991c7fa44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;31m# Forward pass only to get logits/output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# Get predictions from the maximum value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "losses = []\n",
    "accuracies = []\n",
    "for epoch in range(100):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "#         images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        labels = torch.max(labels,1)[1]\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 1 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in validation_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                # images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                labels = torch.max(labels,1)[1]\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy =  100 * correct / total\n",
    "            \n",
    "            accuracies.append(accuracy)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'resnet18proto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
