{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST\n",
    "Use this notebook as a skeleton for developing your own network to solve this classification problem!\n",
    "Feel free to experiment (as a matter of fact, its encouraged) with what you've learned so far here. Don't be afraid to ask questions and use different architectures.\n",
    "Be conscious of what you don't know so that you know what to ask/look for.\n",
    "\n",
    "No GPU required!\n",
    "\n",
    "The basic 7 steps for building models in general are listed so:\n",
    " 1. Load Dataset\n",
    " 2. Make Dataset Iterable\n",
    " 3. Create Model Class\n",
    " 4. Instantiate Model Class\n",
    " 5. Instantiate Loss Class\n",
    " 6. Instantiate Optimizer Class\n",
    " 7. Train Model\n",
    "\n",
    "I have handled steps 1 and 2 for you. Please handle the rest!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below cells until 'stop' to get your data processed and loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 1: LOAD DATASET\n",
    "'''\n",
    "test_df = pd.read_csv('fashionmnist/fashion-mnist_test.csv')\n",
    "test_df_labels = test_df['label']\n",
    "test_pixels_df = test_df.drop('label', axis=1)\n",
    "\n",
    "'''\n",
    "If you're curious about how I did this see the below cells. If not just skip to STEP 1.5\n",
    "\n",
    "Pandas is a library for dataprocessing. You might run into dask.DataFrame at some point if you continue with ML.\n",
    "dask.DataFrame is built ontop of Pandas with the purpose of concurrency and parallelized computing...basically when\n",
    "working with datasets so large that you require multiple machines to handle it. This is part of the data pipeline!\n",
    "'''\n",
    "\n",
    "# This reads the csv file into a pandas dataframe\n",
    "train_df = pd.read_csv('fashionmnist/fashion-mnist_train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We create a new dataframe without the 'label' column here so we only get the pixel data\n",
    "# The original dataframe train_df is unmodified\n",
    "train_pixels_df = train_df.drop('label', axis=1)\n",
    "train_pixels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we grab only the labels. Keep in mind that we do not change the order of either the pixel values nor the labels\n",
    "# so that they stay consistent\n",
    "train_df_labels = train_df['label']\n",
    "train_df_labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 1.5: defining and instantiating Dataset subclass \n",
    "'''\n",
    "\n",
    "'''\n",
    "This is our custom Dataset class. Remember from 1st meeting that we need this to pipeline our data into training our model.\n",
    "\n",
    "The pipeline is important!!! At larger scale, machine learning can get bottlenecked at disk reads (in image classification for example)\n",
    "so understanding the various stages is important. We don't have to worry about that kind of stuff now since we're just creating small\n",
    "project models as opposed to complex production models.\n",
    "\n",
    "NOTE: this is not the only way to create a dataset. An alternative is to simply pass in a dataframe that contains both pixel and label data.\n",
    "Then we can index the label and pixel data inside of __getitem__ as opposed to separating labels and pixel data before hand like I did.\n",
    "'''\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, dataframe, labels):\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.df = dataframe\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # I'm using .loc to access the row of the dataframe by index\n",
    "        # HINT You don't need to do this but try normalizing your image vector before making it a torch Tensor.\n",
    "        # BONUS train your model with and without normalization and see what happens\n",
    "        img = torch.Tensor(self.df.loc[index])\n",
    "        label = self.labels[index]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "train_dataset = FashionDataset(train_pixels_df, train_df_labels.values)\n",
    "test_dataset = FashionDataset(test_pixels_df, test_df_labels.values)\n",
    "\n",
    "'''\n",
    "Batch_size will determine how many data samples to go through before \n",
    "updating the weights of our model with SGD (stochastic gradient descent)\n",
    "\n",
    "Currently at 100 but feel free to change this to whatever you want. You can consider\n",
    "batch size a hyper parameter!\n",
    "'''\n",
    "batch_size = 100\n",
    "\n",
    "# shuffle is true so that we train our data on all labels simultaneously. The data is already shuffled in \n",
    "# this case(You can verify this by looking through the training labels by running train_labels in its own cell)\n",
    "# If this wasn't the case, and we had shuffle=False, we might end up training the model on label = 0 and \n",
    "# then ending with label = 9. This would cause the model to 'forget' what label = 0 looked like\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,\n",
    "                                           num_workers=8)\n",
    "\n",
    "# shuffle=False because theres no reason to do so with testing\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False,\n",
    "                                          num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stop\n",
    "\n",
    "Below this block is your responsibility! Best of luck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class YourModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Define you model here. Make sure to update the constructor and forward methods!\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(YourModel, self).__init__()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Currently this model simply returns a random label.\n",
    "        \"\"\"\n",
    "        out = np.random.choice(10)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "model = YourModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "\"\"\"\n",
    "Most of the time I use SGD. Feel free to use another optimizer if you wish.\n",
    "What hyperparameters would you use/set here?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "# This iteration variable keeps track of what iteration you're on so you may print out your progress during the training loop\n",
    "iteration = 0\n",
    "\n",
    "'''\n",
    "Write your training loop here\n",
    "\n",
    "HINT: You'll need two for loops. 1 for every epoch you wish to train and 1 to iterate over your train_loader\n",
    "HIN2: see the bottom of this doc if you want more hints on how to write your training loop\n",
    "'''\n",
    "#LOOP HERE\n",
    "\n",
    "\n",
    "        \n",
    "        # I've left this for your use\n",
    "\"\"\"\n",
    "        The below code block prints out your iteration number, loss, and accuracy\n",
    "        \n",
    "        This may need to be modified depending on how you implemented steps 3-7\n",
    "        \n",
    "        If it doesn't work and you have no clue what is wrong send me your code so I may help debug!\n",
    "\"\"\"\n",
    "        \n",
    "'''\n",
    "        if iteration % YOUR_NUMBER == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                images = images.view(-1, 28*28).requires_grad_()\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HINT 2: for your inner for loop you need to do these steps:\n",
    "    # Load images with gradient accumulation capabilities\n",
    "    # Clear gradients w.r.t. parameters\n",
    "    # Forward pass to get output/logits\n",
    "    # Calculate Loss: softmax --> cross entropy loss\n",
    "    # Getting gradients w.r.t. parameters\n",
    "    # Updating parameters\n",
    "\n",
    "HINT 3: You may look at FF NN MNIST.ipynb if you're stuck or have no clue where to start. Yes it is difficult but you're all very capable <3\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
